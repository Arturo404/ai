{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0147c79-f5d0-4f65-8348-ad00c102f7f2",
   "metadata": {},
   "source": [
    "# Training a neural network on the Fashion MNIST dataset\n",
    "\n",
    "The **Fashion MNIST** dataset is a popular benchmark dataset used for evaluating machine learning and deep learning models, especially for classification tasks involving image data. It's intended as a more challenging and realistic replacement for the original MNIST dataset of handwritten digits.\n",
    "\n",
    "### Key Facts:\n",
    "\n",
    "* **Published by**: Zalando Research\n",
    "* **Number of classes**: 10\n",
    "* **Total samples**: 70,000 grayscale images (60,000 for training, 10,000 for testing)\n",
    "* **Image size**: 28x28 pixels\n",
    "* **Image type**: Grayscale (1 channel)\n",
    "* **File format**: Same structure and format as original MNIST\n",
    "\n",
    "### The 10 Fashion Categories:\n",
    "\n",
    "Each image corresponds to one of the following classes:\n",
    "\n",
    "1. T-shirt/top\n",
    "2. Trouser\n",
    "3. Pullover\n",
    "4. Dress\n",
    "5. Coat\n",
    "6. Sandal\n",
    "7. Shirt\n",
    "8. Sneaker\n",
    "9. Bag\n",
    "10. Ankle boot\n",
    "\n",
    "Each label is a number from 0 to 9 corresponding to the above categories.\n",
    "\n",
    "### Why use Fashion MNIST?\n",
    "\n",
    "* It has the **same dimensions and format** as MNIST, so itâ€™s easy to switch with minimal code changes.\n",
    "* It is **more complex** than digit recognition, making it a better test of model performance.\n",
    "* It is widely used in **deep learning tutorials and research papers** as a benchmark dataset.\n",
    "\n",
    "### Model setup\n",
    "\n",
    "We are facing here a multi class classification model.\n",
    "\n",
    "Dataset samples are 28x28 pixels grayscale images. So each we treat each pixel as an input feature, we have 784 input features that are values between 0 and 255.\n",
    "\n",
    "Each sample is associated a class label between 0 and 9 (10 classes).\n",
    "\n",
    "With this setup, we will likely use a deep neural network for multi class classification using the softmax function as our output function (and to compute the cross entropy loss with multiple classes).\n",
    "\n",
    "Let's start by visualizing our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79d755-6240-4a62-9030-b826e780c9e5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1fba5c-e532-4f1f-9aee-2e2bc9fda439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch==2.7.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch==2.7.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arthur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install torch\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd174e4-112f-4991-868b-23beca2849ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26fe23-b7f5-4aa4-acdc-8b10db9efb55",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086c0221-f0c5-4c53-b1b9-43be55a1b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bc77a-34c8-4f4d-b91e-298a5dc3025a",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6cbd685-2a62-43c8-b48b-ea056e201c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceddf160-1ef4-4650-a944-2ab8b6e86ea0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
      "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0039, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
      "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
      "          0.0157, 0.0000, 0.0000, 0.0118],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
      "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0471, 0.0392, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
      "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
      "          0.3020, 0.5098, 0.2824, 0.0588],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
      "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
      "          0.5529, 0.3451, 0.6745, 0.2588],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
      "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
      "          0.4824, 0.7686, 0.8980, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
      "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
      "          0.8745, 0.9608, 0.6784, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
      "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
      "          0.8627, 0.9529, 0.7922, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
      "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
      "          0.8863, 0.7725, 0.8196, 0.2039],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
      "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
      "          0.9608, 0.4667, 0.6549, 0.2196],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
      "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
      "          0.8510, 0.8196, 0.3608, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
      "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
      "          0.8549, 1.0000, 0.3020, 0.0000],\n",
      "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
      "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
      "          0.8784, 0.9569, 0.6235, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
      "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
      "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
      "          0.9137, 0.9333, 0.8431, 0.0000],\n",
      "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
      "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
      "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
      "          0.8627, 0.9098, 0.9647, 0.0000],\n",
      "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
      "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
      "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
      "          0.8706, 0.8941, 0.8824, 0.0000],\n",
      "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
      "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
      "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
      "          0.8745, 0.8784, 0.8980, 0.1137],\n",
      "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
      "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
      "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
      "          0.8627, 0.8667, 0.9020, 0.2627],\n",
      "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
      "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
      "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
      "          0.7098, 0.8039, 0.8078, 0.4510],\n",
      "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
      "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
      "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
      "          0.6549, 0.6941, 0.8235, 0.3608],\n",
      "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
      "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
      "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
      "          0.7529, 0.8471, 0.6667, 0.0000],\n",
      "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
      "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
      "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
      "          0.3882, 0.2275, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
      "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "image_tensor, label = training_data[0]\n",
    "\n",
    "print(label)\n",
    "\n",
    "print(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e40995f2-014a-4230-b7b5-763ac5beb7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = ['T_shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "for i in range(0,30):\n",
    "  \n",
    "    plt.subplot(6,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid=False\n",
    "\n",
    "    image_tensor, class_index = training_data[i]\n",
    "    array = image_tensor.numpy()\n",
    "    sample_img = array.reshape((28,28))\n",
    "    \n",
    "    plt.imshow(sample_img)\n",
    "    \n",
    "    plt.title(class_names[class_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c70baf-4ec3-4aed-a653-0ec66314532c",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We will perform Z-score normalization on the dataset. In our case it helps:\n",
    "1. **Stabilizes training**: Prevents the gradients from becoming too small or too large (which could lead to vanishing or exploding gradients).\n",
    "2. **Faster convergence**: Well-scaled inputs allow the optimization algorithm to make more consistent updates.\n",
    "3. **Improves generalization**: Reduces overfitting and ensures that the model can handle unseen data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625f3732-8f0b-4c76-b26a-59a852ba5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_arrays(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for img, label in dataset:\n",
    "        img_array = np.array(img)  # Converts PIL Image to NumPy array (28x28)\n",
    "        images.append(img_array.flatten())  # Flatten to 1D\n",
    "        labels.append(label)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "images_train, labels_train = dataset_to_arrays(training_data)\n",
    "images_test, labels_test = dataset_to_arrays(test_data)\n",
    "X_train = np.stack(images_train)  # Shape: (N, 784)\n",
    "y_train = np.array(labels_train)\n",
    "\n",
    "X_test = np.stack(images_test)  # Shape: (N, 784)\n",
    "y_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7d23e2-3833-42ed-af8a-986edc56eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02a34bb-a276-4890-b3a2-d3feebc1bec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b23930-fcf2-4715-96f7-c540698c465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor_scaled = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor_scaled = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "training_set = TensorDataset(X_train_tensor_scaled, y_train_tensor_scaled)\n",
    "test_set = TensorDataset(X_test_tensor_scaled, y_test_tensor_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a521d5-6e38-44a8-ba6d-4f623fb9d4e9",
   "metadata": {},
   "source": [
    "## Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "831ba55c-74eb-4383-a6f7-2eb0a8d2a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor, label = training_set[0]\n",
    "input_dim = image_tensor.shape[0]\n",
    "output_dim = 10\n",
    "\n",
    "dev_set_split = 0.5\n",
    "\n",
    "layers_dims = [input_dim, 64, output_dim]\n",
    "layers_activations = [nn.ReLU(), None]\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "learning_rate=1e-3\n",
    "momentum=0.9\n",
    "\n",
    "end_training_threshold = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548a6ce-b9c1-4d87-bce1-ec372bb36a43",
   "metadata": {},
   "source": [
    "## Splitting dev/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "291a91f4-b121-42d6-9a11-c79a49c8dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(test_set)\n",
    "dev_size = int(dev_set_split * N)\n",
    "test_size = N - dev_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "dev_set, test_set = random_split(test_set, [dev_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e3674-2b72-48e9-98f7-7baf7f12f5c5",
   "metadata": {},
   "source": [
    "## Creating DataLoaders\n",
    "\n",
    "In order to pass our data to our neural network, we have to put it in the right format.\n",
    "\n",
    "A `DataLoader` is a PyTorch utility that **automatically manages how your dataset is fed to the model** during training/testing.\n",
    "\n",
    "It **wraps** your dataset and:\n",
    "- Divides it into **mini-batches**\n",
    "- **Shuffles** the data (if you want)\n",
    "- **Loads** it efficiently (can even load in **parallel** with multiple worker threads)\n",
    "- **Handles** going through the whole dataset multiple times (epochs)\n",
    "\n",
    "We use them for:\n",
    "\n",
    "1. **Mini-batch training**  \n",
    "   - Training with the full dataset at once is inefficient or impossible (memory!).  \n",
    "   - Training sample-by-sample is too slow.  \n",
    "   â†’ Solution: **batch** samples.\n",
    "\n",
    "2. **Shuffling**  \n",
    "   - Shuffling helps **break correlations** in the dataset, improving generalization.\n",
    "\n",
    "3. **Efficiency**  \n",
    "   - Can load data **in parallel** while the GPU is busy training on a previous batch (`num_workers > 0`).\n",
    "\n",
    "4. **Code simplicity**  \n",
    "   - Instead of manually slicing arrays, you just write:\n",
    "     ```python\n",
    "     for batch in dataloader:\n",
    "         ...\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de6b796-400d-4670-b430-4bb759d79b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([64, 784])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_set, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_set, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51b526-da02-4da4-ab2a-29493146bf98",
   "metadata": {},
   "source": [
    "## Build our neural network\n",
    "\n",
    "We will start with the following simple network architecture:\n",
    "\n",
    "- Input layer: 784 input units for the 784 input pixels of each image.\n",
    "- Hidden layers: we will start with one hidden layer with 64 units.\n",
    "- Output layer: 10 units for the 10 digit classes output probability.\n",
    "\n",
    "We define our loss function (Cross Entropy Loss) as well as an optimizer to perform gradient descent (SGD with momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9972234c-15c0-48bd-97af-4f0cba962f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f987321-3153-4004-bd16-744b52fc251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, activations):\n",
    "        \"\"\"\n",
    "        layer_sizes: list of integers, e.g. [784, 256, 128, 10]\n",
    "        activations: list of activation functions (nn.Module), len = len(layer_sizes) - 1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_features = layer_sizes[i]\n",
    "            out_features = layer_sizes[i + 1]\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            if activations[i] is not None:\n",
    "                layers.append(activations[i])\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ef1adea-07ab-4a24-8a58-1c7a1901e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(layers_dims, layers_activations).to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e38c9d4-1c31-477e-8077-09bba651bf7f",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "Now let's implement our training function that will iterate through the mini-batches of our DataLoader and each time perform the following steps:\n",
    "- Compute the predicted values.\n",
    "- Compute the loss for our predicted values.\n",
    "- Perform backpropagation using the loss.backward() function that will compute the gradient for each parameter of the model.\n",
    "- Run one optimization step using the optimizer we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be30b69e-24b8-465b-865a-0af318b2a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_dataloader, model, loss_fn, optimizer):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    average_loss = total_loss/len(train_dataloader)\n",
    "    print(f\"Average training loss: {average_loss}\")\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c5b77-8abd-4ef1-aabf-4c61d119ad90",
   "metadata": {},
   "source": [
    "Let's define a test function that will run the model on our test data and measure how good it does (computing average loss and accuracy on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea6f7cab-aed7-4b34-96ad-9a1791df96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fada8a-bc13-4ea9-828f-2395ee8c97fb",
   "metadata": {},
   "source": [
    "Now we're ready to run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62ade626-0496-4099-8f03-c7bccd798d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, train_dataloader, dev_dataloader, end_training_threshold):\n",
    "    epoch = 1\n",
    "    previous_average_loss = None\n",
    "    while True:\n",
    "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "        epoch_average_training_loss = train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "        print(f\"Training performance for current epoch\")\n",
    "        test(train_dataloader, model, loss_fn)\n",
    "\n",
    "        print(f\"Dev performance for current epoch\")\n",
    "        test(dev_dataloader, model, loss_fn)\n",
    "        \n",
    "        change_in_average_loss = np.inf if not previous_average_loss else previous_average_loss - epoch_average_training_loss\n",
    "        print(f\"Change in average loss: {change_in_average_loss}\")\n",
    "        if(change_in_average_loss and change_in_average_loss > 0 and change_in_average_loss < end_training_threshold): break\n",
    "        previous_average_loss = epoch_average_training_loss\n",
    "        epoch+=1\n",
    "        \n",
    "    print(\"TRAINING DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8eb3dbc-a64b-4924-b83b-a6a9917de996",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.267593  [   64/60000]\n",
      "loss: 1.267373  [ 6464/60000]\n",
      "loss: 0.719086  [12864/60000]\n",
      "loss: 0.899726  [19264/60000]\n",
      "loss: 0.676065  [25664/60000]\n",
      "loss: 0.654656  [32064/60000]\n",
      "loss: 0.596075  [38464/60000]\n",
      "loss: 0.631905  [44864/60000]\n",
      "loss: 0.667214  [51264/60000]\n",
      "loss: 0.555026  [57664/60000]\n",
      "Average training loss: 0.7578288701170289\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.500888 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.522420 \n",
      "\n",
      "Change in average loss: inf\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.413704  [   64/60000]\n",
      "loss: 0.530964  [ 6464/60000]\n",
      "loss: 0.339655  [12864/60000]\n",
      "loss: 0.615319  [19264/60000]\n",
      "loss: 0.483520  [25664/60000]\n",
      "loss: 0.479051  [32064/60000]\n",
      "loss: 0.450016  [38464/60000]\n",
      "loss: 0.584514  [44864/60000]\n",
      "loss: 0.625548  [51264/60000]\n",
      "loss: 0.462038  [57664/60000]\n",
      "Average training loss: 0.46306607818234957\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.430335 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.460588 \n",
      "\n",
      "Change in average loss: 0.29476279193467936\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.320882  [   64/60000]\n",
      "loss: 0.454911  [ 6464/60000]\n",
      "loss: 0.275614  [12864/60000]\n",
      "loss: 0.551887  [19264/60000]\n",
      "loss: 0.415852  [25664/60000]\n",
      "loss: 0.422309  [32064/60000]\n",
      "loss: 0.405918  [38464/60000]\n",
      "loss: 0.553950  [44864/60000]\n",
      "loss: 0.588603  [51264/60000]\n",
      "loss: 0.431877  [57664/60000]\n",
      "Average training loss: 0.41715799535769643\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.398893 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.435250 \n",
      "\n",
      "Change in average loss: 0.04590808282465314\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.281121  [   64/60000]\n",
      "loss: 0.417650  [ 6464/60000]\n",
      "loss: 0.241974  [12864/60000]\n",
      "loss: 0.506579  [19264/60000]\n",
      "loss: 0.381128  [25664/60000]\n",
      "loss: 0.397114  [32064/60000]\n",
      "loss: 0.382390  [38464/60000]\n",
      "loss: 0.530679  [44864/60000]\n",
      "loss: 0.560903  [51264/60000]\n",
      "loss: 0.417347  [57664/60000]\n",
      "Average training loss: 0.39197723237054943\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.378463 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.419169 \n",
      "\n",
      "Change in average loss: 0.025180762987147\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.258387  [   64/60000]\n",
      "loss: 0.391466  [ 6464/60000]\n",
      "loss: 0.220818  [12864/60000]\n",
      "loss: 0.467850  [19264/60000]\n",
      "loss: 0.359863  [25664/60000]\n",
      "loss: 0.382470  [32064/60000]\n",
      "loss: 0.367221  [38464/60000]\n",
      "loss: 0.512523  [44864/60000]\n",
      "loss: 0.540922  [51264/60000]\n",
      "loss: 0.405074  [57664/60000]\n",
      "Average training loss: 0.37433056364943984\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.363265 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.407539 \n",
      "\n",
      "Change in average loss: 0.017646668721109593\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.243767  [   64/60000]\n",
      "loss: 0.371588  [ 6464/60000]\n",
      "loss: 0.206716  [12864/60000]\n",
      "loss: 0.435474  [19264/60000]\n",
      "loss: 0.346371  [25664/60000]\n",
      "loss: 0.371309  [32064/60000]\n",
      "loss: 0.354425  [38464/60000]\n",
      "loss: 0.493179  [44864/60000]\n",
      "loss: 0.526094  [51264/60000]\n",
      "loss: 0.395525  [57664/60000]\n",
      "Average training loss: 0.36071745165661456\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.351200 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.398828 \n",
      "\n",
      "Change in average loss: 0.013613111992825278\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.233338  [   64/60000]\n",
      "loss: 0.356233  [ 6464/60000]\n",
      "loss: 0.199162  [12864/60000]\n",
      "loss: 0.410125  [19264/60000]\n",
      "loss: 0.335537  [25664/60000]\n",
      "loss: 0.361705  [32064/60000]\n",
      "loss: 0.345522  [38464/60000]\n",
      "loss: 0.478017  [44864/60000]\n",
      "loss: 0.512453  [51264/60000]\n",
      "loss: 0.385567  [57664/60000]\n",
      "Average training loss: 0.3495852878607158\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.341111 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.391639 \n",
      "\n",
      "Change in average loss: 0.011132163795898764\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.225882  [   64/60000]\n",
      "loss: 0.345429  [ 6464/60000]\n",
      "loss: 0.193815  [12864/60000]\n",
      "loss: 0.388814  [19264/60000]\n",
      "loss: 0.327473  [25664/60000]\n",
      "loss: 0.352576  [32064/60000]\n",
      "loss: 0.337728  [38464/60000]\n",
      "loss: 0.463637  [44864/60000]\n",
      "loss: 0.498000  [51264/60000]\n",
      "loss: 0.379320  [57664/60000]\n",
      "Average training loss: 0.3400947983140376\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.332300 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.385458 \n",
      "\n",
      "Change in average loss: 0.009490489546678194\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.221576  [   64/60000]\n",
      "loss: 0.336664  [ 6464/60000]\n",
      "loss: 0.188473  [12864/60000]\n",
      "loss: 0.371943  [19264/60000]\n",
      "loss: 0.319324  [25664/60000]\n",
      "loss: 0.344007  [32064/60000]\n",
      "loss: 0.331585  [38464/60000]\n",
      "loss: 0.451873  [44864/60000]\n",
      "loss: 0.485923  [51264/60000]\n",
      "loss: 0.371229  [57664/60000]\n",
      "Average training loss: 0.33183891939392474\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.324565 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.380424 \n",
      "\n",
      "Change in average loss: 0.008255878920112869\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.216819  [   64/60000]\n",
      "loss: 0.331623  [ 6464/60000]\n",
      "loss: 0.184694  [12864/60000]\n",
      "loss: 0.357145  [19264/60000]\n",
      "loss: 0.313791  [25664/60000]\n",
      "loss: 0.335547  [32064/60000]\n",
      "loss: 0.326597  [38464/60000]\n",
      "loss: 0.439291  [44864/60000]\n",
      "loss: 0.473021  [51264/60000]\n",
      "loss: 0.364430  [57664/60000]\n",
      "Average training loss: 0.3244960175028869\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.317634 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.376025 \n",
      "\n",
      "Change in average loss: 0.007342901891037812\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.213645  [   64/60000]\n",
      "loss: 0.325110  [ 6464/60000]\n",
      "loss: 0.181204  [12864/60000]\n",
      "loss: 0.345237  [19264/60000]\n",
      "loss: 0.309584  [25664/60000]\n",
      "loss: 0.328297  [32064/60000]\n",
      "loss: 0.321031  [38464/60000]\n",
      "loss: 0.430536  [44864/60000]\n",
      "loss: 0.461556  [51264/60000]\n",
      "loss: 0.359130  [57664/60000]\n",
      "Average training loss: 0.317861212286423\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.311176 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.371868 \n",
      "\n",
      "Change in average loss: 0.006634805216463924\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.212015  [   64/60000]\n",
      "loss: 0.320007  [ 6464/60000]\n",
      "loss: 0.178316  [12864/60000]\n",
      "loss: 0.336020  [19264/60000]\n",
      "loss: 0.306111  [25664/60000]\n",
      "loss: 0.322313  [32064/60000]\n",
      "loss: 0.317952  [38464/60000]\n",
      "loss: 0.422371  [44864/60000]\n",
      "loss: 0.450418  [51264/60000]\n",
      "loss: 0.354073  [57664/60000]\n",
      "Average training loss: 0.31172046738901116\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.305271 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.368552 \n",
      "\n",
      "Change in average loss: 0.006140744897411843\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.209881  [   64/60000]\n",
      "loss: 0.316371  [ 6464/60000]\n",
      "loss: 0.175525  [12864/60000]\n",
      "loss: 0.328086  [19264/60000]\n",
      "loss: 0.302308  [25664/60000]\n",
      "loss: 0.316238  [32064/60000]\n",
      "loss: 0.314144  [38464/60000]\n",
      "loss: 0.417047  [44864/60000]\n",
      "loss: 0.439112  [51264/60000]\n",
      "loss: 0.352869  [57664/60000]\n",
      "Average training loss: 0.306054757403603\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.299882 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.365548 \n",
      "\n",
      "Change in average loss: 0.005665709985408163\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.207776  [   64/60000]\n",
      "loss: 0.312234  [ 6464/60000]\n",
      "loss: 0.173676  [12864/60000]\n",
      "loss: 0.320916  [19264/60000]\n",
      "loss: 0.298878  [25664/60000]\n",
      "loss: 0.313649  [32064/60000]\n",
      "loss: 0.311147  [38464/60000]\n",
      "loss: 0.408106  [44864/60000]\n",
      "loss: 0.429092  [51264/60000]\n",
      "loss: 0.347181  [57664/60000]\n",
      "Average training loss: 0.3006971580689269\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.294743 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.363088 \n",
      "\n",
      "Change in average loss: 0.005357599334676111\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.207277  [   64/60000]\n",
      "loss: 0.308934  [ 6464/60000]\n",
      "loss: 0.172467  [12864/60000]\n",
      "loss: 0.314649  [19264/60000]\n",
      "loss: 0.297005  [25664/60000]\n",
      "loss: 0.309799  [32064/60000]\n",
      "loss: 0.307783  [38464/60000]\n",
      "loss: 0.398434  [44864/60000]\n",
      "loss: 0.419564  [51264/60000]\n",
      "loss: 0.342840  [57664/60000]\n",
      "Average training loss: 0.2956294143902086\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.289846 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.360460 \n",
      "\n",
      "Change in average loss: 0.005067743678718262\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.206795  [   64/60000]\n",
      "loss: 0.305555  [ 6464/60000]\n",
      "loss: 0.170863  [12864/60000]\n",
      "loss: 0.309163  [19264/60000]\n",
      "loss: 0.293409  [25664/60000]\n",
      "loss: 0.307823  [32064/60000]\n",
      "loss: 0.304247  [38464/60000]\n",
      "loss: 0.390075  [44864/60000]\n",
      "loss: 0.411505  [51264/60000]\n",
      "loss: 0.337557  [57664/60000]\n",
      "Average training loss: 0.290776020888962\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.285192 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.358246 \n",
      "\n",
      "Change in average loss: 0.004853393501246639\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.205729  [   64/60000]\n",
      "loss: 0.302314  [ 6464/60000]\n",
      "loss: 0.169678  [12864/60000]\n",
      "loss: 0.303669  [19264/60000]\n",
      "loss: 0.290623  [25664/60000]\n",
      "loss: 0.307142  [32064/60000]\n",
      "loss: 0.301016  [38464/60000]\n",
      "loss: 0.381203  [44864/60000]\n",
      "loss: 0.404639  [51264/60000]\n",
      "loss: 0.331481  [57664/60000]\n",
      "Average training loss: 0.2862279826183436\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.280850 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.356266 \n",
      "\n",
      "Change in average loss: 0.004548038270618393\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.204681  [   64/60000]\n",
      "loss: 0.298430  [ 6464/60000]\n",
      "loss: 0.167792  [12864/60000]\n",
      "loss: 0.299534  [19264/60000]\n",
      "loss: 0.288172  [25664/60000]\n",
      "loss: 0.306094  [32064/60000]\n",
      "loss: 0.297163  [38464/60000]\n",
      "loss: 0.373808  [44864/60000]\n",
      "loss: 0.397260  [51264/60000]\n",
      "loss: 0.327800  [57664/60000]\n",
      "Average training loss: 0.28190614866104713\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.276616 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.354301 \n",
      "\n",
      "Change in average loss: 0.0043218339572964615\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.204835  [   64/60000]\n",
      "loss: 0.294922  [ 6464/60000]\n",
      "loss: 0.165986  [12864/60000]\n",
      "loss: 0.295345  [19264/60000]\n",
      "loss: 0.284590  [25664/60000]\n",
      "loss: 0.305722  [32064/60000]\n",
      "loss: 0.294293  [38464/60000]\n",
      "loss: 0.366416  [44864/60000]\n",
      "loss: 0.390229  [51264/60000]\n",
      "loss: 0.320432  [57664/60000]\n",
      "Average training loss: 0.27772922794829047\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.272596 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.352234 \n",
      "\n",
      "Change in average loss: 0.00417692071275666\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.203772  [   64/60000]\n",
      "loss: 0.292377  [ 6464/60000]\n",
      "loss: 0.164185  [12864/60000]\n",
      "loss: 0.290798  [19264/60000]\n",
      "loss: 0.283150  [25664/60000]\n",
      "loss: 0.304180  [32064/60000]\n",
      "loss: 0.291383  [38464/60000]\n",
      "loss: 0.359688  [44864/60000]\n",
      "loss: 0.384705  [51264/60000]\n",
      "loss: 0.316565  [57664/60000]\n",
      "Average training loss: 0.2737484009328809\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.268800 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.350776 \n",
      "\n",
      "Change in average loss: 0.003980827015409583\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.202654  [   64/60000]\n",
      "loss: 0.288121  [ 6464/60000]\n",
      "loss: 0.162932  [12864/60000]\n",
      "loss: 0.286955  [19264/60000]\n",
      "loss: 0.280796  [25664/60000]\n",
      "loss: 0.304004  [32064/60000]\n",
      "loss: 0.287889  [38464/60000]\n",
      "loss: 0.353883  [44864/60000]\n",
      "loss: 0.379073  [51264/60000]\n",
      "loss: 0.312384  [57664/60000]\n",
      "Average training loss: 0.26992131234295585\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.264903 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.348957 \n",
      "\n",
      "Change in average loss: 0.0038270885899250318\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.200512  [   64/60000]\n",
      "loss: 0.284771  [ 6464/60000]\n",
      "loss: 0.161606  [12864/60000]\n",
      "loss: 0.282241  [19264/60000]\n",
      "loss: 0.278907  [25664/60000]\n",
      "loss: 0.303035  [32064/60000]\n",
      "loss: 0.285451  [38464/60000]\n",
      "loss: 0.348234  [44864/60000]\n",
      "loss: 0.374695  [51264/60000]\n",
      "loss: 0.308718  [57664/60000]\n",
      "Average training loss: 0.26624463849675173\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.261264 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.347524 \n",
      "\n",
      "Change in average loss: 0.003676673846204126\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.199616  [   64/60000]\n",
      "loss: 0.281374  [ 6464/60000]\n",
      "loss: 0.160602  [12864/60000]\n",
      "loss: 0.277868  [19264/60000]\n",
      "loss: 0.277409  [25664/60000]\n",
      "loss: 0.300930  [32064/60000]\n",
      "loss: 0.282488  [38464/60000]\n",
      "loss: 0.341124  [44864/60000]\n",
      "loss: 0.367603  [51264/60000]\n",
      "loss: 0.306278  [57664/60000]\n",
      "Average training loss: 0.26268298055793937\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.257795 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.346398 \n",
      "\n",
      "Change in average loss: 0.003561657938812357\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.197261  [   64/60000]\n",
      "loss: 0.278511  [ 6464/60000]\n",
      "loss: 0.159682  [12864/60000]\n",
      "loss: 0.273641  [19264/60000]\n",
      "loss: 0.274838  [25664/60000]\n",
      "loss: 0.299592  [32064/60000]\n",
      "loss: 0.279924  [38464/60000]\n",
      "loss: 0.335681  [44864/60000]\n",
      "loss: 0.362771  [51264/60000]\n",
      "loss: 0.301797  [57664/60000]\n",
      "Average training loss: 0.25923617415304884\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.254455 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.345429 \n",
      "\n",
      "Change in average loss: 0.0034468064048905345\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.195062  [   64/60000]\n",
      "loss: 0.275178  [ 6464/60000]\n",
      "loss: 0.159669  [12864/60000]\n",
      "loss: 0.269511  [19264/60000]\n",
      "loss: 0.272334  [25664/60000]\n",
      "loss: 0.298872  [32064/60000]\n",
      "loss: 0.277829  [38464/60000]\n",
      "loss: 0.330178  [44864/60000]\n",
      "loss: 0.359237  [51264/60000]\n",
      "loss: 0.299653  [57664/60000]\n",
      "Average training loss: 0.25593554217423964\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.251139 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.344463 \n",
      "\n",
      "Change in average loss: 0.0033006319788091987\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.193712  [   64/60000]\n",
      "loss: 0.272355  [ 6464/60000]\n",
      "loss: 0.159051  [12864/60000]\n",
      "loss: 0.266575  [19264/60000]\n",
      "loss: 0.271171  [25664/60000]\n",
      "loss: 0.297744  [32064/60000]\n",
      "loss: 0.274226  [38464/60000]\n",
      "loss: 0.325078  [44864/60000]\n",
      "loss: 0.354457  [51264/60000]\n",
      "loss: 0.297477  [57664/60000]\n",
      "Average training loss: 0.25272048365618627\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.247994 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.343669 \n",
      "\n",
      "Change in average loss: 0.003215058518053371\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.192345  [   64/60000]\n",
      "loss: 0.268899  [ 6464/60000]\n",
      "loss: 0.158225  [12864/60000]\n",
      "loss: 0.261808  [19264/60000]\n",
      "loss: 0.269046  [25664/60000]\n",
      "loss: 0.295985  [32064/60000]\n",
      "loss: 0.271448  [38464/60000]\n",
      "loss: 0.320355  [44864/60000]\n",
      "loss: 0.350042  [51264/60000]\n",
      "loss: 0.295212  [57664/60000]\n",
      "Average training loss: 0.2495829806144812\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.244952 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.342969 \n",
      "\n",
      "Change in average loss: 0.0031375030417050698\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.189636  [   64/60000]\n",
      "loss: 0.267722  [ 6464/60000]\n",
      "loss: 0.157805  [12864/60000]\n",
      "loss: 0.258204  [19264/60000]\n",
      "loss: 0.268524  [25664/60000]\n",
      "loss: 0.295854  [32064/60000]\n",
      "loss: 0.270887  [38464/60000]\n",
      "loss: 0.315808  [44864/60000]\n",
      "loss: 0.345520  [51264/60000]\n",
      "loss: 0.292306  [57664/60000]\n",
      "Average training loss: 0.24653511027346797\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.241878 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.342218 \n",
      "\n",
      "Change in average loss: 0.0030478703410132257\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.187203  [   64/60000]\n",
      "loss: 0.264814  [ 6464/60000]\n",
      "loss: 0.156434  [12864/60000]\n",
      "loss: 0.253013  [19264/60000]\n",
      "loss: 0.266908  [25664/60000]\n",
      "loss: 0.293374  [32064/60000]\n",
      "loss: 0.266476  [38464/60000]\n",
      "loss: 0.309643  [44864/60000]\n",
      "loss: 0.341222  [51264/60000]\n",
      "loss: 0.290179  [57664/60000]\n",
      "Average training loss: 0.24354004001280646\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.238914 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.341506 \n",
      "\n",
      "Change in average loss: 0.0029950702606615087\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.185902  [   64/60000]\n",
      "loss: 0.261026  [ 6464/60000]\n",
      "loss: 0.154882  [12864/60000]\n",
      "loss: 0.249098  [19264/60000]\n",
      "loss: 0.266610  [25664/60000]\n",
      "loss: 0.293626  [32064/60000]\n",
      "loss: 0.262524  [38464/60000]\n",
      "loss: 0.304543  [44864/60000]\n",
      "loss: 0.337392  [51264/60000]\n",
      "loss: 0.287614  [57664/60000]\n",
      "Average training loss: 0.24061240283633345\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.235933 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.340931 \n",
      "\n",
      "Change in average loss: 0.0029276371764730136\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.182605  [   64/60000]\n",
      "loss: 0.258866  [ 6464/60000]\n",
      "loss: 0.154011  [12864/60000]\n",
      "loss: 0.244763  [19264/60000]\n",
      "loss: 0.265872  [25664/60000]\n",
      "loss: 0.293348  [32064/60000]\n",
      "loss: 0.258596  [38464/60000]\n",
      "loss: 0.299760  [44864/60000]\n",
      "loss: 0.333736  [51264/60000]\n",
      "loss: 0.285234  [57664/60000]\n",
      "Average training loss: 0.23775002516027707\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.233173 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.340344 \n",
      "\n",
      "Change in average loss: 0.002862377676056377\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.179595  [   64/60000]\n",
      "loss: 0.256156  [ 6464/60000]\n",
      "loss: 0.153394  [12864/60000]\n",
      "loss: 0.240977  [19264/60000]\n",
      "loss: 0.265288  [25664/60000]\n",
      "loss: 0.293345  [32064/60000]\n",
      "loss: 0.254764  [38464/60000]\n",
      "loss: 0.293737  [44864/60000]\n",
      "loss: 0.330651  [51264/60000]\n",
      "loss: 0.282506  [57664/60000]\n",
      "Average training loss: 0.2349877798083876\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.230470 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.339965 \n",
      "\n",
      "Change in average loss: 0.0027622453518894863\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.177637  [   64/60000]\n",
      "loss: 0.254692  [ 6464/60000]\n",
      "loss: 0.151897  [12864/60000]\n",
      "loss: 0.237320  [19264/60000]\n",
      "loss: 0.263382  [25664/60000]\n",
      "loss: 0.293151  [32064/60000]\n",
      "loss: 0.250736  [38464/60000]\n",
      "loss: 0.289255  [44864/60000]\n",
      "loss: 0.326439  [51264/60000]\n",
      "loss: 0.280315  [57664/60000]\n",
      "Average training loss: 0.2322441983912418\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.227749 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.339419 \n",
      "\n",
      "Change in average loss: 0.002743581417145785\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.175454  [   64/60000]\n",
      "loss: 0.252468  [ 6464/60000]\n",
      "loss: 0.150726  [12864/60000]\n",
      "loss: 0.234413  [19264/60000]\n",
      "loss: 0.261386  [25664/60000]\n",
      "loss: 0.292872  [32064/60000]\n",
      "loss: 0.247029  [38464/60000]\n",
      "loss: 0.284150  [44864/60000]\n",
      "loss: 0.324384  [51264/60000]\n",
      "loss: 0.278234  [57664/60000]\n",
      "Average training loss: 0.22956259070492502\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.225188 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.339024 \n",
      "\n",
      "Change in average loss: 0.0026816076863167837\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.174583  [   64/60000]\n",
      "loss: 0.250851  [ 6464/60000]\n",
      "loss: 0.149944  [12864/60000]\n",
      "loss: 0.231068  [19264/60000]\n",
      "loss: 0.260346  [25664/60000]\n",
      "loss: 0.292950  [32064/60000]\n",
      "loss: 0.243316  [38464/60000]\n",
      "loss: 0.278615  [44864/60000]\n",
      "loss: 0.321847  [51264/60000]\n",
      "loss: 0.275189  [57664/60000]\n",
      "Average training loss: 0.2269685161806373\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.222597 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.338864 \n",
      "\n",
      "Change in average loss: 0.002594074524287726\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.172410  [   64/60000]\n",
      "loss: 0.248407  [ 6464/60000]\n",
      "loss: 0.148933  [12864/60000]\n",
      "loss: 0.228308  [19264/60000]\n",
      "loss: 0.257526  [25664/60000]\n",
      "loss: 0.292981  [32064/60000]\n",
      "loss: 0.240332  [38464/60000]\n",
      "loss: 0.273126  [44864/60000]\n",
      "loss: 0.318934  [51264/60000]\n",
      "loss: 0.273461  [57664/60000]\n",
      "Average training loss: 0.22445076800533323\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.220201 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.338855 \n",
      "\n",
      "Change in average loss: 0.002517748175304063\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.172220  [   64/60000]\n",
      "loss: 0.245479  [ 6464/60000]\n",
      "loss: 0.147920  [12864/60000]\n",
      "loss: 0.226577  [19264/60000]\n",
      "loss: 0.255131  [25664/60000]\n",
      "loss: 0.292688  [32064/60000]\n",
      "loss: 0.238889  [38464/60000]\n",
      "loss: 0.267522  [44864/60000]\n",
      "loss: 0.316990  [51264/60000]\n",
      "loss: 0.269731  [57664/60000]\n",
      "Average training loss: 0.22196383911298154\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.217736 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.338830 \n",
      "\n",
      "Change in average loss: 0.002486928892351692\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.169934  [   64/60000]\n",
      "loss: 0.241888  [ 6464/60000]\n",
      "loss: 0.147178  [12864/60000]\n",
      "loss: 0.224692  [19264/60000]\n",
      "loss: 0.254824  [25664/60000]\n",
      "loss: 0.292270  [32064/60000]\n",
      "loss: 0.236720  [38464/60000]\n",
      "loss: 0.263190  [44864/60000]\n",
      "loss: 0.316494  [51264/60000]\n",
      "loss: 0.268594  [57664/60000]\n",
      "Average training loss: 0.2195586613270202\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.215411 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.338807 \n",
      "\n",
      "Change in average loss: 0.002405177785961332\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.168874  [   64/60000]\n",
      "loss: 0.239632  [ 6464/60000]\n",
      "loss: 0.145519  [12864/60000]\n",
      "loss: 0.221783  [19264/60000]\n",
      "loss: 0.253105  [25664/60000]\n",
      "loss: 0.291822  [32064/60000]\n",
      "loss: 0.232839  [38464/60000]\n",
      "loss: 0.259008  [44864/60000]\n",
      "loss: 0.314241  [51264/60000]\n",
      "loss: 0.267094  [57664/60000]\n",
      "Average training loss: 0.2172048013728819\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.213056 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.338965 \n",
      "\n",
      "Change in average loss: 0.00235385995413831\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.167161  [   64/60000]\n",
      "loss: 0.237608  [ 6464/60000]\n",
      "loss: 0.144106  [12864/60000]\n",
      "loss: 0.219056  [19264/60000]\n",
      "loss: 0.251397  [25664/60000]\n",
      "loss: 0.290166  [32064/60000]\n",
      "loss: 0.230402  [38464/60000]\n",
      "loss: 0.254799  [44864/60000]\n",
      "loss: 0.309829  [51264/60000]\n",
      "loss: 0.264832  [57664/60000]\n",
      "Average training loss: 0.214915611437643\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.210769 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.338984 \n",
      "\n",
      "Change in average loss: 0.0022891899352388834\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.165529  [   64/60000]\n",
      "loss: 0.235240  [ 6464/60000]\n",
      "loss: 0.142395  [12864/60000]\n",
      "loss: 0.216401  [19264/60000]\n",
      "loss: 0.249507  [25664/60000]\n",
      "loss: 0.288519  [32064/60000]\n",
      "loss: 0.227552  [38464/60000]\n",
      "loss: 0.249301  [44864/60000]\n",
      "loss: 0.307579  [51264/60000]\n",
      "loss: 0.263443  [57664/60000]\n",
      "Average training loss: 0.21262987311492595\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.208677 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.339259 \n",
      "\n",
      "Change in average loss: 0.002285738322717057\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.165060  [   64/60000]\n",
      "loss: 0.234123  [ 6464/60000]\n",
      "loss: 0.139993  [12864/60000]\n",
      "loss: 0.213366  [19264/60000]\n",
      "loss: 0.247523  [25664/60000]\n",
      "loss: 0.288055  [32064/60000]\n",
      "loss: 0.224780  [38464/60000]\n",
      "loss: 0.246239  [44864/60000]\n",
      "loss: 0.305514  [51264/60000]\n",
      "loss: 0.263990  [57664/60000]\n",
      "Average training loss: 0.21044237154331416\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.206411 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.339569 \n",
      "\n",
      "Change in average loss: 0.002187501571611794\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.162802  [   64/60000]\n",
      "loss: 0.232138  [ 6464/60000]\n",
      "loss: 0.138630  [12864/60000]\n",
      "loss: 0.210909  [19264/60000]\n",
      "loss: 0.247069  [25664/60000]\n",
      "loss: 0.287364  [32064/60000]\n",
      "loss: 0.223266  [38464/60000]\n",
      "loss: 0.241294  [44864/60000]\n",
      "loss: 0.302899  [51264/60000]\n",
      "loss: 0.261752  [57664/60000]\n",
      "Average training loss: 0.2082517134275899\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.204439 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.339757 \n",
      "\n",
      "Change in average loss: 0.0021906581157242477\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.161761  [   64/60000]\n",
      "loss: 0.229572  [ 6464/60000]\n",
      "loss: 0.137345  [12864/60000]\n",
      "loss: 0.208089  [19264/60000]\n",
      "loss: 0.246441  [25664/60000]\n",
      "loss: 0.286903  [32064/60000]\n",
      "loss: 0.221301  [38464/60000]\n",
      "loss: 0.237682  [44864/60000]\n",
      "loss: 0.301975  [51264/60000]\n",
      "loss: 0.258946  [57664/60000]\n",
      "Average training loss: 0.2061554618171855\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.202307 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.339965 \n",
      "\n",
      "Change in average loss: 0.002096251610404415\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.159562  [   64/60000]\n",
      "loss: 0.228481  [ 6464/60000]\n",
      "loss: 0.135076  [12864/60000]\n",
      "loss: 0.205693  [19264/60000]\n",
      "loss: 0.243559  [25664/60000]\n",
      "loss: 0.285519  [32064/60000]\n",
      "loss: 0.218536  [38464/60000]\n",
      "loss: 0.233662  [44864/60000]\n",
      "loss: 0.299149  [51264/60000]\n",
      "loss: 0.256874  [57664/60000]\n",
      "Average training loss: 0.20406998405610322\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.200286 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.340272 \n",
      "\n",
      "Change in average loss: 0.0020854777610822828\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.157177  [   64/60000]\n",
      "loss: 0.226728  [ 6464/60000]\n",
      "loss: 0.133337  [12864/60000]\n",
      "loss: 0.203401  [19264/60000]\n",
      "loss: 0.241962  [25664/60000]\n",
      "loss: 0.285650  [32064/60000]\n",
      "loss: 0.215776  [38464/60000]\n",
      "loss: 0.230648  [44864/60000]\n",
      "loss: 0.297668  [51264/60000]\n",
      "loss: 0.255107  [57664/60000]\n",
      "Average training loss: 0.2019694511300084\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.198292 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.340595 \n",
      "\n",
      "Change in average loss: 0.002100532926094828\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.155276  [   64/60000]\n",
      "loss: 0.225460  [ 6464/60000]\n",
      "loss: 0.132377  [12864/60000]\n",
      "loss: 0.201085  [19264/60000]\n",
      "loss: 0.241476  [25664/60000]\n",
      "loss: 0.284583  [32064/60000]\n",
      "loss: 0.212580  [38464/60000]\n",
      "loss: 0.226456  [44864/60000]\n",
      "loss: 0.295660  [51264/60000]\n",
      "loss: 0.252353  [57664/60000]\n",
      "Average training loss: 0.19997046082449366\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.196291 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.340990 \n",
      "\n",
      "Change in average loss: 0.0019989903055147307\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.154791  [   64/60000]\n",
      "loss: 0.223789  [ 6464/60000]\n",
      "loss: 0.131439  [12864/60000]\n",
      "loss: 0.199189  [19264/60000]\n",
      "loss: 0.238949  [25664/60000]\n",
      "loss: 0.284184  [32064/60000]\n",
      "loss: 0.209669  [38464/60000]\n",
      "loss: 0.221841  [44864/60000]\n",
      "loss: 0.293221  [51264/60000]\n",
      "loss: 0.251489  [57664/60000]\n",
      "Average training loss: 0.1979665790261554\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.194364 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.341397 \n",
      "\n",
      "Change in average loss: 0.0020038817983382695\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.152692  [   64/60000]\n",
      "loss: 0.221174  [ 6464/60000]\n",
      "loss: 0.130700  [12864/60000]\n",
      "loss: 0.197330  [19264/60000]\n",
      "loss: 0.237612  [25664/60000]\n",
      "loss: 0.282582  [32064/60000]\n",
      "loss: 0.207011  [38464/60000]\n",
      "loss: 0.217573  [44864/60000]\n",
      "loss: 0.290470  [51264/60000]\n",
      "loss: 0.247716  [57664/60000]\n",
      "Average training loss: 0.19603311072296298\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.192521 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.341893 \n",
      "\n",
      "Change in average loss: 0.0019334683031924094\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.151206  [   64/60000]\n",
      "loss: 0.220444  [ 6464/60000]\n",
      "loss: 0.130060  [12864/60000]\n",
      "loss: 0.195051  [19264/60000]\n",
      "loss: 0.236757  [25664/60000]\n",
      "loss: 0.281500  [32064/60000]\n",
      "loss: 0.203789  [38464/60000]\n",
      "loss: 0.213745  [44864/60000]\n",
      "loss: 0.288092  [51264/60000]\n",
      "loss: 0.245868  [57664/60000]\n",
      "Average training loss: 0.194101267329443\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.190580 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.342208 \n",
      "\n",
      "Change in average loss: 0.001931843393519983\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.149775  [   64/60000]\n",
      "loss: 0.218404  [ 6464/60000]\n",
      "loss: 0.127614  [12864/60000]\n",
      "loss: 0.192048  [19264/60000]\n",
      "loss: 0.234036  [25664/60000]\n",
      "loss: 0.281412  [32064/60000]\n",
      "loss: 0.201171  [38464/60000]\n",
      "loss: 0.209760  [44864/60000]\n",
      "loss: 0.285741  [51264/60000]\n",
      "loss: 0.242960  [57664/60000]\n",
      "Average training loss: 0.1921774543075165\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.188705 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.342593 \n",
      "\n",
      "Change in average loss: 0.0019238130219264848\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.148719  [   64/60000]\n",
      "loss: 0.216541  [ 6464/60000]\n",
      "loss: 0.126832  [12864/60000]\n",
      "loss: 0.191073  [19264/60000]\n",
      "loss: 0.232178  [25664/60000]\n",
      "loss: 0.280076  [32064/60000]\n",
      "loss: 0.198325  [38464/60000]\n",
      "loss: 0.206716  [44864/60000]\n",
      "loss: 0.282643  [51264/60000]\n",
      "loss: 0.240406  [57664/60000]\n",
      "Average training loss: 0.19032779765694635\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.186910 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.343076 \n",
      "\n",
      "Change in average loss: 0.0018496566505701584\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.147102  [   64/60000]\n",
      "loss: 0.215115  [ 6464/60000]\n",
      "loss: 0.126594  [12864/60000]\n",
      "loss: 0.189157  [19264/60000]\n",
      "loss: 0.231516  [25664/60000]\n",
      "loss: 0.279817  [32064/60000]\n",
      "loss: 0.197170  [38464/60000]\n",
      "loss: 0.203147  [44864/60000]\n",
      "loss: 0.279263  [51264/60000]\n",
      "loss: 0.236261  [57664/60000]\n",
      "Average training loss: 0.18850731724567377\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.185081 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.343716 \n",
      "\n",
      "Change in average loss: 0.0018204804112725848\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.146256  [   64/60000]\n",
      "loss: 0.212831  [ 6464/60000]\n",
      "loss: 0.125309  [12864/60000]\n",
      "loss: 0.188229  [19264/60000]\n",
      "loss: 0.230122  [25664/60000]\n",
      "loss: 0.279557  [32064/60000]\n",
      "loss: 0.195015  [38464/60000]\n",
      "loss: 0.199755  [44864/60000]\n",
      "loss: 0.277959  [51264/60000]\n",
      "loss: 0.235118  [57664/60000]\n",
      "Average training loss: 0.18668626109038844\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.183349 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.344316 \n",
      "\n",
      "Change in average loss: 0.0018210561552853277\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.144100  [   64/60000]\n",
      "loss: 0.209951  [ 6464/60000]\n",
      "loss: 0.125027  [12864/60000]\n",
      "loss: 0.186676  [19264/60000]\n",
      "loss: 0.229442  [25664/60000]\n",
      "loss: 0.277749  [32064/60000]\n",
      "loss: 0.193418  [38464/60000]\n",
      "loss: 0.197920  [44864/60000]\n",
      "loss: 0.277249  [51264/60000]\n",
      "loss: 0.231503  [57664/60000]\n",
      "Average training loss: 0.18491395163748944\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.181579 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.344892 \n",
      "\n",
      "Change in average loss: 0.0017723094528989958\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.143372  [   64/60000]\n",
      "loss: 0.207720  [ 6464/60000]\n",
      "loss: 0.124143  [12864/60000]\n",
      "loss: 0.185711  [19264/60000]\n",
      "loss: 0.228690  [25664/60000]\n",
      "loss: 0.278454  [32064/60000]\n",
      "loss: 0.190798  [38464/60000]\n",
      "loss: 0.195230  [44864/60000]\n",
      "loss: 0.277771  [51264/60000]\n",
      "loss: 0.228532  [57664/60000]\n",
      "Average training loss: 0.18322384500030134\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.179931 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.345534 \n",
      "\n",
      "Change in average loss: 0.0016901066371881046\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.140941  [   64/60000]\n",
      "loss: 0.204618  [ 6464/60000]\n",
      "loss: 0.123685  [12864/60000]\n",
      "loss: 0.183524  [19264/60000]\n",
      "loss: 0.225598  [25664/60000]\n",
      "loss: 0.278025  [32064/60000]\n",
      "loss: 0.188546  [38464/60000]\n",
      "loss: 0.191838  [44864/60000]\n",
      "loss: 0.275296  [51264/60000]\n",
      "loss: 0.224676  [57664/60000]\n",
      "Average training loss: 0.18147173757428553\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.178285 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.346154 \n",
      "\n",
      "Change in average loss: 0.0017521074260158087\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.140733  [   64/60000]\n",
      "loss: 0.202537  [ 6464/60000]\n",
      "loss: 0.123766  [12864/60000]\n",
      "loss: 0.181037  [19264/60000]\n",
      "loss: 0.223998  [25664/60000]\n",
      "loss: 0.275960  [32064/60000]\n",
      "loss: 0.186370  [38464/60000]\n",
      "loss: 0.189574  [44864/60000]\n",
      "loss: 0.274043  [51264/60000]\n",
      "loss: 0.222790  [57664/60000]\n",
      "Average training loss: 0.1797643812226334\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.176635 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.346764 \n",
      "\n",
      "Change in average loss: 0.0017073563516521173\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.140363  [   64/60000]\n",
      "loss: 0.200278  [ 6464/60000]\n",
      "loss: 0.122605  [12864/60000]\n",
      "loss: 0.180119  [19264/60000]\n",
      "loss: 0.222196  [25664/60000]\n",
      "loss: 0.275960  [32064/60000]\n",
      "loss: 0.184297  [38464/60000]\n",
      "loss: 0.186736  [44864/60000]\n",
      "loss: 0.274699  [51264/60000]\n",
      "loss: 0.218703  [57664/60000]\n",
      "Average training loss: 0.17812416455877234\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.175081 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.347632 \n",
      "\n",
      "Change in average loss: 0.0016402166638610771\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.138542  [   64/60000]\n",
      "loss: 0.197690  [ 6464/60000]\n",
      "loss: 0.121845  [12864/60000]\n",
      "loss: 0.177474  [19264/60000]\n",
      "loss: 0.219526  [25664/60000]\n",
      "loss: 0.274863  [32064/60000]\n",
      "loss: 0.182135  [38464/60000]\n",
      "loss: 0.184467  [44864/60000]\n",
      "loss: 0.271933  [51264/60000]\n",
      "loss: 0.217383  [57664/60000]\n",
      "Average training loss: 0.17648214361529108\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.173492 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.348448 \n",
      "\n",
      "Change in average loss: 0.0016420209434812594\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.136665  [   64/60000]\n",
      "loss: 0.194834  [ 6464/60000]\n",
      "loss: 0.121396  [12864/60000]\n",
      "loss: 0.176441  [19264/60000]\n",
      "loss: 0.217130  [25664/60000]\n",
      "loss: 0.274099  [32064/60000]\n",
      "loss: 0.178803  [38464/60000]\n",
      "loss: 0.182437  [44864/60000]\n",
      "loss: 0.269247  [51264/60000]\n",
      "loss: 0.215122  [57664/60000]\n",
      "Average training loss: 0.17486807192439463\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.171893 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.349026 \n",
      "\n",
      "Change in average loss: 0.0016140716908964459\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.136091  [   64/60000]\n",
      "loss: 0.192373  [ 6464/60000]\n",
      "loss: 0.121513  [12864/60000]\n",
      "loss: 0.175249  [19264/60000]\n",
      "loss: 0.215434  [25664/60000]\n",
      "loss: 0.271870  [32064/60000]\n",
      "loss: 0.177882  [38464/60000]\n",
      "loss: 0.180761  [44864/60000]\n",
      "loss: 0.267278  [51264/60000]\n",
      "loss: 0.212525  [57664/60000]\n",
      "Average training loss: 0.17326965386758864\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.170435 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.349988 \n",
      "\n",
      "Change in average loss: 0.0015984180568059947\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.134811  [   64/60000]\n",
      "loss: 0.190813  [ 6464/60000]\n",
      "loss: 0.120576  [12864/60000]\n",
      "loss: 0.172500  [19264/60000]\n",
      "loss: 0.213586  [25664/60000]\n",
      "loss: 0.273037  [32064/60000]\n",
      "loss: 0.176413  [38464/60000]\n",
      "loss: 0.177304  [44864/60000]\n",
      "loss: 0.264729  [51264/60000]\n",
      "loss: 0.210564  [57664/60000]\n",
      "Average training loss: 0.17171311178314153\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.168777 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.350485 \n",
      "\n",
      "Change in average loss: 0.00155654208444711\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.132473  [   64/60000]\n",
      "loss: 0.189129  [ 6464/60000]\n",
      "loss: 0.119468  [12864/60000]\n",
      "loss: 0.171124  [19264/60000]\n",
      "loss: 0.212049  [25664/60000]\n",
      "loss: 0.271995  [32064/60000]\n",
      "loss: 0.173309  [38464/60000]\n",
      "loss: 0.174258  [44864/60000]\n",
      "loss: 0.262281  [51264/60000]\n",
      "loss: 0.208164  [57664/60000]\n",
      "Average training loss: 0.1701606541498701\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.167326 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.351088 \n",
      "\n",
      "Change in average loss: 0.0015524576332714146\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.131018  [   64/60000]\n",
      "loss: 0.187302  [ 6464/60000]\n",
      "loss: 0.119724  [12864/60000]\n",
      "loss: 0.168714  [19264/60000]\n",
      "loss: 0.210502  [25664/60000]\n",
      "loss: 0.270390  [32064/60000]\n",
      "loss: 0.172009  [38464/60000]\n",
      "loss: 0.172102  [44864/60000]\n",
      "loss: 0.260045  [51264/60000]\n",
      "loss: 0.207461  [57664/60000]\n",
      "Average training loss: 0.16861322388720157\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.165873 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.351939 \n",
      "\n",
      "Change in average loss: 0.0015474302626685421\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.129653  [   64/60000]\n",
      "loss: 0.184565  [ 6464/60000]\n",
      "loss: 0.117867  [12864/60000]\n",
      "loss: 0.167130  [19264/60000]\n",
      "loss: 0.207743  [25664/60000]\n",
      "loss: 0.268353  [32064/60000]\n",
      "loss: 0.169614  [38464/60000]\n",
      "loss: 0.168910  [44864/60000]\n",
      "loss: 0.258683  [51264/60000]\n",
      "loss: 0.202338  [57664/60000]\n",
      "Average training loss: 0.16708994609142927\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.164186 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.352505 \n",
      "\n",
      "Change in average loss: 0.0015232777957722987\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.129203  [   64/60000]\n",
      "loss: 0.181543  [ 6464/60000]\n",
      "loss: 0.117185  [12864/60000]\n",
      "loss: 0.165529  [19264/60000]\n",
      "loss: 0.206530  [25664/60000]\n",
      "loss: 0.268090  [32064/60000]\n",
      "loss: 0.166991  [38464/60000]\n",
      "loss: 0.166590  [44864/60000]\n",
      "loss: 0.255759  [51264/60000]\n",
      "loss: 0.200861  [57664/60000]\n",
      "Average training loss: 0.16560362163446604\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.162850 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.353435 \n",
      "\n",
      "Change in average loss: 0.0014863244569632283\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.127622  [   64/60000]\n",
      "loss: 0.180372  [ 6464/60000]\n",
      "loss: 0.117396  [12864/60000]\n",
      "loss: 0.163977  [19264/60000]\n",
      "loss: 0.204274  [25664/60000]\n",
      "loss: 0.265738  [32064/60000]\n",
      "loss: 0.165500  [38464/60000]\n",
      "loss: 0.164442  [44864/60000]\n",
      "loss: 0.254600  [51264/60000]\n",
      "loss: 0.198681  [57664/60000]\n",
      "Average training loss: 0.16408462462617135\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.161442 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.354326 \n",
      "\n",
      "Change in average loss: 0.0015189970082946924\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.127733  [   64/60000]\n",
      "loss: 0.179433  [ 6464/60000]\n",
      "loss: 0.116212  [12864/60000]\n",
      "loss: 0.164110  [19264/60000]\n",
      "loss: 0.202939  [25664/60000]\n",
      "loss: 0.262370  [32064/60000]\n",
      "loss: 0.163688  [38464/60000]\n",
      "loss: 0.162077  [44864/60000]\n",
      "loss: 0.252064  [51264/60000]\n",
      "loss: 0.197600  [57664/60000]\n",
      "Average training loss: 0.16265989961956484\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.160101 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.355223 \n",
      "\n",
      "Change in average loss: 0.0014247250066065087\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.126555  [   64/60000]\n",
      "loss: 0.177463  [ 6464/60000]\n",
      "loss: 0.115832  [12864/60000]\n",
      "loss: 0.162567  [19264/60000]\n",
      "loss: 0.200300  [25664/60000]\n",
      "loss: 0.261650  [32064/60000]\n",
      "loss: 0.162889  [38464/60000]\n",
      "loss: 0.158836  [44864/60000]\n",
      "loss: 0.250742  [51264/60000]\n",
      "loss: 0.194380  [57664/60000]\n",
      "Average training loss: 0.16117082958393641\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.158684 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.355938 \n",
      "\n",
      "Change in average loss: 0.0014890700356284259\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.125437  [   64/60000]\n",
      "loss: 0.176579  [ 6464/60000]\n",
      "loss: 0.115000  [12864/60000]\n",
      "loss: 0.161794  [19264/60000]\n",
      "loss: 0.198937  [25664/60000]\n",
      "loss: 0.260183  [32064/60000]\n",
      "loss: 0.160409  [38464/60000]\n",
      "loss: 0.155650  [44864/60000]\n",
      "loss: 0.247101  [51264/60000]\n",
      "loss: 0.192173  [57664/60000]\n",
      "Average training loss: 0.1597511029716875\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.157423 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.357235 \n",
      "\n",
      "Change in average loss: 0.0014197266122489283\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.124804  [   64/60000]\n",
      "loss: 0.174423  [ 6464/60000]\n",
      "loss: 0.114847  [12864/60000]\n",
      "loss: 0.160236  [19264/60000]\n",
      "loss: 0.197424  [25664/60000]\n",
      "loss: 0.257377  [32064/60000]\n",
      "loss: 0.156972  [38464/60000]\n",
      "loss: 0.153477  [44864/60000]\n",
      "loss: 0.244216  [51264/60000]\n",
      "loss: 0.190203  [57664/60000]\n",
      "Average training loss: 0.15838304980953874\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.156120 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.358259 \n",
      "\n",
      "Change in average loss: 0.0013680531621487468\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.123463  [   64/60000]\n",
      "loss: 0.174466  [ 6464/60000]\n",
      "loss: 0.114429  [12864/60000]\n",
      "loss: 0.158535  [19264/60000]\n",
      "loss: 0.196342  [25664/60000]\n",
      "loss: 0.256623  [32064/60000]\n",
      "loss: 0.155234  [38464/60000]\n",
      "loss: 0.151262  [44864/60000]\n",
      "loss: 0.242021  [51264/60000]\n",
      "loss: 0.188004  [57664/60000]\n",
      "Average training loss: 0.15696883564993644\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.154727 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.359162 \n",
      "\n",
      "Change in average loss: 0.0014142141596023006\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.122268  [   64/60000]\n",
      "loss: 0.173655  [ 6464/60000]\n",
      "loss: 0.114429  [12864/60000]\n",
      "loss: 0.158699  [19264/60000]\n",
      "loss: 0.194473  [25664/60000]\n",
      "loss: 0.253931  [32064/60000]\n",
      "loss: 0.153241  [38464/60000]\n",
      "loss: 0.149747  [44864/60000]\n",
      "loss: 0.242307  [51264/60000]\n",
      "loss: 0.185415  [57664/60000]\n",
      "Average training loss: 0.15559175276537057\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.153540 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.360365 \n",
      "\n",
      "Change in average loss: 0.0013770828845658667\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.121026  [   64/60000]\n",
      "loss: 0.171506  [ 6464/60000]\n",
      "loss: 0.113706  [12864/60000]\n",
      "loss: 0.156034  [19264/60000]\n",
      "loss: 0.192436  [25664/60000]\n",
      "loss: 0.251885  [32064/60000]\n",
      "loss: 0.151402  [38464/60000]\n",
      "loss: 0.146868  [44864/60000]\n",
      "loss: 0.238513  [51264/60000]\n",
      "loss: 0.182774  [57664/60000]\n",
      "Average training loss: 0.15426196699251116\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.152187 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.361099 \n",
      "\n",
      "Change in average loss: 0.0013297857728594153\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.119499  [   64/60000]\n",
      "loss: 0.171644  [ 6464/60000]\n",
      "loss: 0.112929  [12864/60000]\n",
      "loss: 0.154471  [19264/60000]\n",
      "loss: 0.191123  [25664/60000]\n",
      "loss: 0.250272  [32064/60000]\n",
      "loss: 0.150205  [38464/60000]\n",
      "loss: 0.145655  [44864/60000]\n",
      "loss: 0.237537  [51264/60000]\n",
      "loss: 0.180610  [57664/60000]\n",
      "Average training loss: 0.1529272745992901\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.151027 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.362546 \n",
      "\n",
      "Change in average loss: 0.0013346923932210653\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.118390  [   64/60000]\n",
      "loss: 0.169138  [ 6464/60000]\n",
      "loss: 0.112499  [12864/60000]\n",
      "loss: 0.152760  [19264/60000]\n",
      "loss: 0.189379  [25664/60000]\n",
      "loss: 0.248476  [32064/60000]\n",
      "loss: 0.148127  [38464/60000]\n",
      "loss: 0.144062  [44864/60000]\n",
      "loss: 0.235764  [51264/60000]\n",
      "loss: 0.178125  [57664/60000]\n",
      "Average training loss: 0.1516062472301569\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.149703 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.363587 \n",
      "\n",
      "Change in average loss: 0.0013210273691331986\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.117371  [   64/60000]\n",
      "loss: 0.169208  [ 6464/60000]\n",
      "loss: 0.112076  [12864/60000]\n",
      "loss: 0.152389  [19264/60000]\n",
      "loss: 0.187116  [25664/60000]\n",
      "loss: 0.246770  [32064/60000]\n",
      "loss: 0.147389  [38464/60000]\n",
      "loss: 0.142517  [44864/60000]\n",
      "loss: 0.233572  [51264/60000]\n",
      "loss: 0.174365  [57664/60000]\n",
      "Average training loss: 0.15028645687186515\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.148387 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.364374 \n",
      "\n",
      "Change in average loss: 0.0013197903582917447\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.115549  [   64/60000]\n",
      "loss: 0.168102  [ 6464/60000]\n",
      "loss: 0.112357  [12864/60000]\n",
      "loss: 0.150620  [19264/60000]\n",
      "loss: 0.185598  [25664/60000]\n",
      "loss: 0.245322  [32064/60000]\n",
      "loss: 0.144895  [38464/60000]\n",
      "loss: 0.141539  [44864/60000]\n",
      "loss: 0.232386  [51264/60000]\n",
      "loss: 0.172559  [57664/60000]\n",
      "Average training loss: 0.14901016181958382\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.147479 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.365788 \n",
      "\n",
      "Change in average loss: 0.0012762950522813288\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.114742  [   64/60000]\n",
      "loss: 0.166572  [ 6464/60000]\n",
      "loss: 0.111723  [12864/60000]\n",
      "loss: 0.148910  [19264/60000]\n",
      "loss: 0.183596  [25664/60000]\n",
      "loss: 0.243089  [32064/60000]\n",
      "loss: 0.142782  [38464/60000]\n",
      "loss: 0.139913  [44864/60000]\n",
      "loss: 0.230265  [51264/60000]\n",
      "loss: 0.170564  [57664/60000]\n",
      "Average training loss: 0.14774049499999486\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.146271 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.366922 \n",
      "\n",
      "Change in average loss: 0.0012696668195889549\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.114518  [   64/60000]\n",
      "loss: 0.165078  [ 6464/60000]\n",
      "loss: 0.111552  [12864/60000]\n",
      "loss: 0.149898  [19264/60000]\n",
      "loss: 0.181860  [25664/60000]\n",
      "loss: 0.241936  [32064/60000]\n",
      "loss: 0.141309  [38464/60000]\n",
      "loss: 0.137394  [44864/60000]\n",
      "loss: 0.227902  [51264/60000]\n",
      "loss: 0.167754  [57664/60000]\n",
      "Average training loss: 0.14650603577589938\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.144826 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.367824 \n",
      "\n",
      "Change in average loss: 0.0012344592240954855\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.113157  [   64/60000]\n",
      "loss: 0.164819  [ 6464/60000]\n",
      "loss: 0.112217  [12864/60000]\n",
      "loss: 0.147029  [19264/60000]\n",
      "loss: 0.179274  [25664/60000]\n",
      "loss: 0.242337  [32064/60000]\n",
      "loss: 0.141368  [38464/60000]\n",
      "loss: 0.134866  [44864/60000]\n",
      "loss: 0.227502  [51264/60000]\n",
      "loss: 0.163871  [57664/60000]\n",
      "Average training loss: 0.14523801641630085\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.143821 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.369019 \n",
      "\n",
      "Change in average loss: 0.0012680193595985267\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.111779  [   64/60000]\n",
      "loss: 0.162637  [ 6464/60000]\n",
      "loss: 0.112107  [12864/60000]\n",
      "loss: 0.146692  [19264/60000]\n",
      "loss: 0.177815  [25664/60000]\n",
      "loss: 0.240237  [32064/60000]\n",
      "loss: 0.138805  [38464/60000]\n",
      "loss: 0.132638  [44864/60000]\n",
      "loss: 0.223881  [51264/60000]\n",
      "loss: 0.163438  [57664/60000]\n",
      "Average training loss: 0.14400693796066713\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.142826 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.370457 \n",
      "\n",
      "Change in average loss: 0.0012310784556337218\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.112365  [   64/60000]\n",
      "loss: 0.163395  [ 6464/60000]\n",
      "loss: 0.110758  [12864/60000]\n",
      "loss: 0.143976  [19264/60000]\n",
      "loss: 0.176227  [25664/60000]\n",
      "loss: 0.238354  [32064/60000]\n",
      "loss: 0.138000  [38464/60000]\n",
      "loss: 0.130635  [44864/60000]\n",
      "loss: 0.222979  [51264/60000]\n",
      "loss: 0.161420  [57664/60000]\n",
      "Average training loss: 0.14276518950711437\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.141567 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.371483 \n",
      "\n",
      "Change in average loss: 0.0012417484535527656\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.111101  [   64/60000]\n",
      "loss: 0.161479  [ 6464/60000]\n",
      "loss: 0.111731  [12864/60000]\n",
      "loss: 0.143478  [19264/60000]\n",
      "loss: 0.174854  [25664/60000]\n",
      "loss: 0.236039  [32064/60000]\n",
      "loss: 0.136811  [38464/60000]\n",
      "loss: 0.127879  [44864/60000]\n",
      "loss: 0.220700  [51264/60000]\n",
      "loss: 0.158440  [57664/60000]\n",
      "Average training loss: 0.14156911983641227\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.140419 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.372533 \n",
      "\n",
      "Change in average loss: 0.001196069670702099\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.110477  [   64/60000]\n",
      "loss: 0.160093  [ 6464/60000]\n",
      "loss: 0.111210  [12864/60000]\n",
      "loss: 0.142596  [19264/60000]\n",
      "loss: 0.172532  [25664/60000]\n",
      "loss: 0.234105  [32064/60000]\n",
      "loss: 0.135445  [38464/60000]\n",
      "loss: 0.126844  [44864/60000]\n",
      "loss: 0.220277  [51264/60000]\n",
      "loss: 0.158329  [57664/60000]\n",
      "Average training loss: 0.14033441798392135\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.139144 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.373711 \n",
      "\n",
      "Change in average loss: 0.001234701852490916\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.109685  [   64/60000]\n",
      "loss: 0.157708  [ 6464/60000]\n",
      "loss: 0.112424  [12864/60000]\n",
      "loss: 0.141439  [19264/60000]\n",
      "loss: 0.169590  [25664/60000]\n",
      "loss: 0.233149  [32064/60000]\n",
      "loss: 0.133973  [38464/60000]\n",
      "loss: 0.126454  [44864/60000]\n",
      "loss: 0.216347  [51264/60000]\n",
      "loss: 0.155185  [57664/60000]\n",
      "Average training loss: 0.13917390001751084\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.138389 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.375183 \n",
      "\n",
      "Change in average loss: 0.001160517966410507\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.108513  [   64/60000]\n",
      "loss: 0.157753  [ 6464/60000]\n",
      "loss: 0.110666  [12864/60000]\n",
      "loss: 0.139976  [19264/60000]\n",
      "loss: 0.169000  [25664/60000]\n",
      "loss: 0.231281  [32064/60000]\n",
      "loss: 0.133713  [38464/60000]\n",
      "loss: 0.123435  [44864/60000]\n",
      "loss: 0.214879  [51264/60000]\n",
      "loss: 0.152071  [57664/60000]\n",
      "Average training loss: 0.1380440530889451\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.137353 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.376124 \n",
      "\n",
      "Change in average loss: 0.001129846928565742\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.107774  [   64/60000]\n",
      "loss: 0.156102  [ 6464/60000]\n",
      "loss: 0.111086  [12864/60000]\n",
      "loss: 0.138796  [19264/60000]\n",
      "loss: 0.168164  [25664/60000]\n",
      "loss: 0.229852  [32064/60000]\n",
      "loss: 0.133407  [38464/60000]\n",
      "loss: 0.122293  [44864/60000]\n",
      "loss: 0.212186  [51264/60000]\n",
      "loss: 0.150332  [57664/60000]\n",
      "Average training loss: 0.13682100011595785\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.136322 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.377413 \n",
      "\n",
      "Change in average loss: 0.0012230529729872486\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.107386  [   64/60000]\n",
      "loss: 0.157211  [ 6464/60000]\n",
      "loss: 0.109746  [12864/60000]\n",
      "loss: 0.137141  [19264/60000]\n",
      "loss: 0.165358  [25664/60000]\n",
      "loss: 0.228453  [32064/60000]\n",
      "loss: 0.130737  [38464/60000]\n",
      "loss: 0.119428  [44864/60000]\n",
      "loss: 0.209264  [51264/60000]\n",
      "loss: 0.147500  [57664/60000]\n",
      "Average training loss: 0.135728941058744\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.135097 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.378590 \n",
      "\n",
      "Change in average loss: 0.001092059057213851\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.105779  [   64/60000]\n",
      "loss: 0.156173  [ 6464/60000]\n",
      "loss: 0.109794  [12864/60000]\n",
      "loss: 0.136174  [19264/60000]\n",
      "loss: 0.165444  [25664/60000]\n",
      "loss: 0.226545  [32064/60000]\n",
      "loss: 0.130867  [38464/60000]\n",
      "loss: 0.119028  [44864/60000]\n",
      "loss: 0.207114  [51264/60000]\n",
      "loss: 0.147752  [57664/60000]\n",
      "Average training loss: 0.13463068998126843\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.134277 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.379623 \n",
      "\n",
      "Change in average loss: 0.001098251077475576\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.104913  [   64/60000]\n",
      "loss: 0.154534  [ 6464/60000]\n",
      "loss: 0.109743  [12864/60000]\n",
      "loss: 0.135260  [19264/60000]\n",
      "loss: 0.163406  [25664/60000]\n",
      "loss: 0.226277  [32064/60000]\n",
      "loss: 0.128848  [38464/60000]\n",
      "loss: 0.117337  [44864/60000]\n",
      "loss: 0.205615  [51264/60000]\n",
      "loss: 0.144948  [57664/60000]\n",
      "Average training loss: 0.13350865821133673\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.133179 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.380815 \n",
      "\n",
      "Change in average loss: 0.0011220317699316973\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.104236  [   64/60000]\n",
      "loss: 0.154911  [ 6464/60000]\n",
      "loss: 0.109859  [12864/60000]\n",
      "loss: 0.132403  [19264/60000]\n",
      "loss: 0.161373  [25664/60000]\n",
      "loss: 0.224424  [32064/60000]\n",
      "loss: 0.128784  [38464/60000]\n",
      "loss: 0.116800  [44864/60000]\n",
      "loss: 0.203214  [51264/60000]\n",
      "loss: 0.144832  [57664/60000]\n",
      "Average training loss: 0.13238943436109563\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.132243 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.382342 \n",
      "\n",
      "Change in average loss: 0.0011192238502411023\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.103070  [   64/60000]\n",
      "loss: 0.153435  [ 6464/60000]\n",
      "loss: 0.107692  [12864/60000]\n",
      "loss: 0.130803  [19264/60000]\n",
      "loss: 0.160088  [25664/60000]\n",
      "loss: 0.221862  [32064/60000]\n",
      "loss: 0.126515  [38464/60000]\n",
      "loss: 0.115074  [44864/60000]\n",
      "loss: 0.200692  [51264/60000]\n",
      "loss: 0.144206  [57664/60000]\n",
      "Average training loss: 0.13127766374840158\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.131157 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.383474 \n",
      "\n",
      "Change in average loss: 0.0011117706126940463\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.103098  [   64/60000]\n",
      "loss: 0.153909  [ 6464/60000]\n",
      "loss: 0.107767  [12864/60000]\n",
      "loss: 0.130931  [19264/60000]\n",
      "loss: 0.158839  [25664/60000]\n",
      "loss: 0.220515  [32064/60000]\n",
      "loss: 0.125753  [38464/60000]\n",
      "loss: 0.112155  [44864/60000]\n",
      "loss: 0.200008  [51264/60000]\n",
      "loss: 0.140795  [57664/60000]\n",
      "Average training loss: 0.13022283275228447\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.130129 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.384233 \n",
      "\n",
      "Change in average loss: 0.001054830996117112\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.101785  [   64/60000]\n",
      "loss: 0.153171  [ 6464/60000]\n",
      "loss: 0.107214  [12864/60000]\n",
      "loss: 0.129330  [19264/60000]\n",
      "loss: 0.158380  [25664/60000]\n",
      "loss: 0.218214  [32064/60000]\n",
      "loss: 0.124306  [38464/60000]\n",
      "loss: 0.112547  [44864/60000]\n",
      "loss: 0.199587  [51264/60000]\n",
      "loss: 0.139483  [57664/60000]\n",
      "Average training loss: 0.12915161254404706\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.7%, Avg loss: 0.129233 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.385566 \n",
      "\n",
      "Change in average loss: 0.0010712202082374112\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.100424  [   64/60000]\n",
      "loss: 0.150772  [ 6464/60000]\n",
      "loss: 0.105389  [12864/60000]\n",
      "loss: 0.128725  [19264/60000]\n",
      "loss: 0.157020  [25664/60000]\n",
      "loss: 0.216773  [32064/60000]\n",
      "loss: 0.125026  [38464/60000]\n",
      "loss: 0.110449  [44864/60000]\n",
      "loss: 0.197060  [51264/60000]\n",
      "loss: 0.137598  [57664/60000]\n",
      "Average training loss: 0.12806592172762352\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.7%, Avg loss: 0.128342 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.387296 \n",
      "\n",
      "Change in average loss: 0.0010856908164235402\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.100121  [   64/60000]\n",
      "loss: 0.148251  [ 6464/60000]\n",
      "loss: 0.105219  [12864/60000]\n",
      "loss: 0.126818  [19264/60000]\n",
      "loss: 0.155653  [25664/60000]\n",
      "loss: 0.216394  [32064/60000]\n",
      "loss: 0.121925  [38464/60000]\n",
      "loss: 0.109993  [44864/60000]\n",
      "loss: 0.198085  [51264/60000]\n",
      "loss: 0.134944  [57664/60000]\n",
      "Average training loss: 0.1270320343493081\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.127314 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.387922 \n",
      "\n",
      "Change in average loss: 0.0010338873783154179\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.097542  [   64/60000]\n",
      "loss: 0.148835  [ 6464/60000]\n",
      "loss: 0.104534  [12864/60000]\n",
      "loss: 0.126612  [19264/60000]\n",
      "loss: 0.153452  [25664/60000]\n",
      "loss: 0.215403  [32064/60000]\n",
      "loss: 0.120339  [38464/60000]\n",
      "loss: 0.108454  [44864/60000]\n",
      "loss: 0.195045  [51264/60000]\n",
      "loss: 0.133958  [57664/60000]\n",
      "Average training loss: 0.12600259739818223\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.126430 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.389568 \n",
      "\n",
      "Change in average loss: 0.0010294369511258672\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.097194  [   64/60000]\n",
      "loss: 0.148296  [ 6464/60000]\n",
      "loss: 0.103453  [12864/60000]\n",
      "loss: 0.124604  [19264/60000]\n",
      "loss: 0.152979  [25664/60000]\n",
      "loss: 0.211569  [32064/60000]\n",
      "loss: 0.118944  [38464/60000]\n",
      "loss: 0.107641  [44864/60000]\n",
      "loss: 0.194008  [51264/60000]\n",
      "loss: 0.132013  [57664/60000]\n",
      "Average training loss: 0.12494812633540393\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.125739 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.391005 \n",
      "\n",
      "Change in average loss: 0.0010544710627782977\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.097843  [   64/60000]\n",
      "loss: 0.145508  [ 6464/60000]\n",
      "loss: 0.101288  [12864/60000]\n",
      "loss: 0.124864  [19264/60000]\n",
      "loss: 0.151807  [25664/60000]\n",
      "loss: 0.210688  [32064/60000]\n",
      "loss: 0.118293  [38464/60000]\n",
      "loss: 0.106019  [44864/60000]\n",
      "loss: 0.191718  [51264/60000]\n",
      "loss: 0.130012  [57664/60000]\n",
      "Average training loss: 0.12394414496407516\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.124548 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.391902 \n",
      "\n",
      "Change in average loss: 0.0010039813713287743\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.096052  [   64/60000]\n",
      "loss: 0.144250  [ 6464/60000]\n",
      "loss: 0.100629  [12864/60000]\n",
      "loss: 0.123204  [19264/60000]\n",
      "loss: 0.150699  [25664/60000]\n",
      "loss: 0.207579  [32064/60000]\n",
      "loss: 0.117296  [38464/60000]\n",
      "loss: 0.104360  [44864/60000]\n",
      "loss: 0.190461  [51264/60000]\n",
      "loss: 0.129665  [57664/60000]\n",
      "Average training loss: 0.12294055162859496\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.123698 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.393459 \n",
      "\n",
      "Change in average loss: 0.0010035933354801962\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.096033  [   64/60000]\n",
      "loss: 0.143117  [ 6464/60000]\n",
      "loss: 0.099926  [12864/60000]\n",
      "loss: 0.121665  [19264/60000]\n",
      "loss: 0.148853  [25664/60000]\n",
      "loss: 0.205615  [32064/60000]\n",
      "loss: 0.115171  [38464/60000]\n",
      "loss: 0.103747  [44864/60000]\n",
      "loss: 0.190625  [51264/60000]\n",
      "loss: 0.128556  [57664/60000]\n",
      "Average training loss: 0.12194720363176899\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.123090 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.395063 \n",
      "\n",
      "Change in average loss: 0.0009933479968259745\n",
      "TRAINING DONE!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_fn, optimizer, train_dataloader, dev_dataloader, end_training_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8907b-9816-4281-ac80-0e9f55b383e4",
   "metadata": {},
   "source": [
    "### Diagnose of our training\n",
    "\n",
    "Now that we're done training, we can analyse the performance of our model both on the training set and the dev set.\n",
    "\n",
    "Training accuracy: 95.9%\n",
    "Dev accuracy: 88.9%\n",
    "\n",
    "From these two values, we can see that we are doing pretty good on the training data, but more poorly on the dev set. This indicates our model suffers from high variance (overfitting).\n",
    "\n",
    "Let's try using a smaller network (a simpler model) to see if it helps reducing overfitting while keeping good performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dad58c01-f515-4674-ad81-c87bfa0b5105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [input_dim, 32, output_dim]\n",
    "layers_activations = [nn.ReLU(), None]\n",
    "\n",
    "model = NeuralNetwork(layers_dims, layers_activations).to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92d0c075-b44e-4392-9f00-a4c77fef7af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.288157  [   64/60000]\n",
      "loss: 1.235242  [ 6464/60000]\n",
      "loss: 0.672514  [12864/60000]\n",
      "loss: 0.901535  [19264/60000]\n",
      "loss: 0.695214  [25664/60000]\n",
      "loss: 0.677244  [32064/60000]\n",
      "loss: 0.612013  [38464/60000]\n",
      "loss: 0.601675  [44864/60000]\n",
      "loss: 0.666102  [51264/60000]\n",
      "loss: 0.586080  [57664/60000]\n",
      "Average training loss: 0.7640148355508409\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.510036 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.531903 \n",
      "\n",
      "Change in average loss: inf\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.405024  [   64/60000]\n",
      "loss: 0.535282  [ 6464/60000]\n",
      "loss: 0.315106  [12864/60000]\n",
      "loss: 0.606443  [19264/60000]\n",
      "loss: 0.488605  [25664/60000]\n",
      "loss: 0.492738  [32064/60000]\n",
      "loss: 0.441893  [38464/60000]\n",
      "loss: 0.575523  [44864/60000]\n",
      "loss: 0.608279  [51264/60000]\n",
      "loss: 0.466340  [57664/60000]\n",
      "Average training loss: 0.4677850847273493\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.432683 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.462081 \n",
      "\n",
      "Change in average loss: 0.2962297508234916\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.314713  [   64/60000]\n",
      "loss: 0.451530  [ 6464/60000]\n",
      "loss: 0.252735  [12864/60000]\n",
      "loss: 0.539751  [19264/60000]\n",
      "loss: 0.404719  [25664/60000]\n",
      "loss: 0.437565  [32064/60000]\n",
      "loss: 0.389830  [38464/60000]\n",
      "loss: 0.559633  [44864/60000]\n",
      "loss: 0.581090  [51264/60000]\n",
      "loss: 0.432253  [57664/60000]\n",
      "Average training loss: 0.4190980871952673\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.400901 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.434502 \n",
      "\n",
      "Change in average loss: 0.048686997532082\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.279648  [   64/60000]\n",
      "loss: 0.412801  [ 6464/60000]\n",
      "loss: 0.228422  [12864/60000]\n",
      "loss: 0.497532  [19264/60000]\n",
      "loss: 0.364529  [25664/60000]\n",
      "loss: 0.410789  [32064/60000]\n",
      "loss: 0.367619  [38464/60000]\n",
      "loss: 0.539588  [44864/60000]\n",
      "loss: 0.567221  [51264/60000]\n",
      "loss: 0.419720  [57664/60000]\n",
      "Average training loss: 0.3943785412638172\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.381396 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.419719 \n",
      "\n",
      "Change in average loss: 0.024719545931450093\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.258392  [   64/60000]\n",
      "loss: 0.386696  [ 6464/60000]\n",
      "loss: 0.213957  [12864/60000]\n",
      "loss: 0.465109  [19264/60000]\n",
      "loss: 0.344536  [25664/60000]\n",
      "loss: 0.394544  [32064/60000]\n",
      "loss: 0.351784  [38464/60000]\n",
      "loss: 0.519956  [44864/60000]\n",
      "loss: 0.557695  [51264/60000]\n",
      "loss: 0.412993  [57664/60000]\n",
      "Average training loss: 0.37800525435443116\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.367475 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.409636 \n",
      "\n",
      "Change in average loss: 0.016373286909386053\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.246285  [   64/60000]\n",
      "loss: 0.369153  [ 6464/60000]\n",
      "loss: 0.204549  [12864/60000]\n",
      "loss: 0.440824  [19264/60000]\n",
      "loss: 0.330413  [25664/60000]\n",
      "loss: 0.380642  [32064/60000]\n",
      "loss: 0.341324  [38464/60000]\n",
      "loss: 0.501819  [44864/60000]\n",
      "loss: 0.549295  [51264/60000]\n",
      "loss: 0.407402  [57664/60000]\n",
      "Average training loss: 0.3658016444459907\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.356675 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.402252 \n",
      "\n",
      "Change in average loss: 0.012203609908440438\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.238964  [   64/60000]\n",
      "loss: 0.355285  [ 6464/60000]\n",
      "loss: 0.199918  [12864/60000]\n",
      "loss: 0.420971  [19264/60000]\n",
      "loss: 0.321398  [25664/60000]\n",
      "loss: 0.368774  [32064/60000]\n",
      "loss: 0.334195  [38464/60000]\n",
      "loss: 0.486099  [44864/60000]\n",
      "loss: 0.542163  [51264/60000]\n",
      "loss: 0.404226  [57664/60000]\n",
      "Average training loss: 0.35605310623261976\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.347843 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.396239 \n",
      "\n",
      "Change in average loss: 0.00974853821337096\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.234558  [   64/60000]\n",
      "loss: 0.344139  [ 6464/60000]\n",
      "loss: 0.197134  [12864/60000]\n",
      "loss: 0.405909  [19264/60000]\n",
      "loss: 0.313500  [25664/60000]\n",
      "loss: 0.357082  [32064/60000]\n",
      "loss: 0.327251  [38464/60000]\n",
      "loss: 0.471719  [44864/60000]\n",
      "loss: 0.533183  [51264/60000]\n",
      "loss: 0.400602  [57664/60000]\n",
      "Average training loss: 0.3478484366939012\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.340325 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.391447 \n",
      "\n",
      "Change in average loss: 0.008204669538718568\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.229724  [   64/60000]\n",
      "loss: 0.336258  [ 6464/60000]\n",
      "loss: 0.196073  [12864/60000]\n",
      "loss: 0.393705  [19264/60000]\n",
      "loss: 0.308595  [25664/60000]\n",
      "loss: 0.349653  [32064/60000]\n",
      "loss: 0.324591  [38464/60000]\n",
      "loss: 0.459936  [44864/60000]\n",
      "loss: 0.525781  [51264/60000]\n",
      "loss: 0.399007  [57664/60000]\n",
      "Average training loss: 0.3408098579136166\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.333764 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.387773 \n",
      "\n",
      "Change in average loss: 0.007038578780284577\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.226688  [   64/60000]\n",
      "loss: 0.329713  [ 6464/60000]\n",
      "loss: 0.194261  [12864/60000]\n",
      "loss: 0.383039  [19264/60000]\n",
      "loss: 0.304597  [25664/60000]\n",
      "loss: 0.342468  [32064/60000]\n",
      "loss: 0.319198  [38464/60000]\n",
      "loss: 0.443921  [44864/60000]\n",
      "loss: 0.516823  [51264/60000]\n",
      "loss: 0.397422  [57664/60000]\n",
      "Average training loss: 0.33458704409251083\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.327873 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.384541 \n",
      "\n",
      "Change in average loss: 0.006222813821105788\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.224141  [   64/60000]\n",
      "loss: 0.323004  [ 6464/60000]\n",
      "loss: 0.192270  [12864/60000]\n",
      "loss: 0.375362  [19264/60000]\n",
      "loss: 0.302308  [25664/60000]\n",
      "loss: 0.336607  [32064/60000]\n",
      "loss: 0.315538  [38464/60000]\n",
      "loss: 0.431032  [44864/60000]\n",
      "loss: 0.504821  [51264/60000]\n",
      "loss: 0.395461  [57664/60000]\n",
      "Average training loss: 0.32895748197301616\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.322591 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.382077 \n",
      "\n",
      "Change in average loss: 0.005629562119494669\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.221538  [   64/60000]\n",
      "loss: 0.317756  [ 6464/60000]\n",
      "loss: 0.190415  [12864/60000]\n",
      "loss: 0.367724  [19264/60000]\n",
      "loss: 0.298753  [25664/60000]\n",
      "loss: 0.331950  [32064/60000]\n",
      "loss: 0.313080  [38464/60000]\n",
      "loss: 0.419288  [44864/60000]\n",
      "loss: 0.495334  [51264/60000]\n",
      "loss: 0.391576  [57664/60000]\n",
      "Average training loss: 0.32386030789726833\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.317838 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.379949 \n",
      "\n",
      "Change in average loss: 0.0050971740757478345\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.220099  [   64/60000]\n",
      "loss: 0.313650  [ 6464/60000]\n",
      "loss: 0.189807  [12864/60000]\n",
      "loss: 0.360766  [19264/60000]\n",
      "loss: 0.296656  [25664/60000]\n",
      "loss: 0.327125  [32064/60000]\n",
      "loss: 0.311210  [38464/60000]\n",
      "loss: 0.408006  [44864/60000]\n",
      "loss: 0.487891  [51264/60000]\n",
      "loss: 0.387546  [57664/60000]\n",
      "Average training loss: 0.31912897522452033\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.313346 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.378030 \n",
      "\n",
      "Change in average loss: 0.004731332672748001\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.220051  [   64/60000]\n",
      "loss: 0.308756  [ 6464/60000]\n",
      "loss: 0.189093  [12864/60000]\n",
      "loss: 0.354650  [19264/60000]\n",
      "loss: 0.294966  [25664/60000]\n",
      "loss: 0.322139  [32064/60000]\n",
      "loss: 0.308561  [38464/60000]\n",
      "loss: 0.399469  [44864/60000]\n",
      "loss: 0.478789  [51264/60000]\n",
      "loss: 0.383867  [57664/60000]\n",
      "Average training loss: 0.314728950434275\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.309068 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.376513 \n",
      "\n",
      "Change in average loss: 0.004400024790245316\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.217864  [   64/60000]\n",
      "loss: 0.302960  [ 6464/60000]\n",
      "loss: 0.189171  [12864/60000]\n",
      "loss: 0.348414  [19264/60000]\n",
      "loss: 0.293787  [25664/60000]\n",
      "loss: 0.317264  [32064/60000]\n",
      "loss: 0.305771  [38464/60000]\n",
      "loss: 0.391422  [44864/60000]\n",
      "loss: 0.471955  [51264/60000]\n",
      "loss: 0.379656  [57664/60000]\n",
      "Average training loss: 0.3105498857573787\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.305063 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.375146 \n",
      "\n",
      "Change in average loss: 0.004179064676896327\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.217388  [   64/60000]\n",
      "loss: 0.299226  [ 6464/60000]\n",
      "loss: 0.190452  [12864/60000]\n",
      "loss: 0.344012  [19264/60000]\n",
      "loss: 0.292136  [25664/60000]\n",
      "loss: 0.312258  [32064/60000]\n",
      "loss: 0.302203  [38464/60000]\n",
      "loss: 0.381421  [44864/60000]\n",
      "loss: 0.465508  [51264/60000]\n",
      "loss: 0.378083  [57664/60000]\n",
      "Average training loss: 0.30663770198154805\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.301240 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.374042 \n",
      "\n",
      "Change in average loss: 0.0039121837758306355\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.215133  [   64/60000]\n",
      "loss: 0.295936  [ 6464/60000]\n",
      "loss: 0.190168  [12864/60000]\n",
      "loss: 0.340867  [19264/60000]\n",
      "loss: 0.291774  [25664/60000]\n",
      "loss: 0.307975  [32064/60000]\n",
      "loss: 0.297885  [38464/60000]\n",
      "loss: 0.372533  [44864/60000]\n",
      "loss: 0.457857  [51264/60000]\n",
      "loss: 0.374428  [57664/60000]\n",
      "Average training loss: 0.3028861793882049\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.297650 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.373306 \n",
      "\n",
      "Change in average loss: 0.0037515225933431595\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.213804  [   64/60000]\n",
      "loss: 0.292218  [ 6464/60000]\n",
      "loss: 0.189887  [12864/60000]\n",
      "loss: 0.336778  [19264/60000]\n",
      "loss: 0.290761  [25664/60000]\n",
      "loss: 0.303057  [32064/60000]\n",
      "loss: 0.295206  [38464/60000]\n",
      "loss: 0.363463  [44864/60000]\n",
      "loss: 0.449645  [51264/60000]\n",
      "loss: 0.370591  [57664/60000]\n",
      "Average training loss: 0.2993148287523911\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.294172 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.372331 \n",
      "\n",
      "Change in average loss: 0.0035713506358137637\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.212482  [   64/60000]\n",
      "loss: 0.290020  [ 6464/60000]\n",
      "loss: 0.189126  [12864/60000]\n",
      "loss: 0.331899  [19264/60000]\n",
      "loss: 0.290330  [25664/60000]\n",
      "loss: 0.298805  [32064/60000]\n",
      "loss: 0.292887  [38464/60000]\n",
      "loss: 0.355508  [44864/60000]\n",
      "loss: 0.442308  [51264/60000]\n",
      "loss: 0.366966  [57664/60000]\n",
      "Average training loss: 0.2959353256978587\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.290937 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.371771 \n",
      "\n",
      "Change in average loss: 0.00337950305453244\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.211493  [   64/60000]\n",
      "loss: 0.286375  [ 6464/60000]\n",
      "loss: 0.188923  [12864/60000]\n",
      "loss: 0.328036  [19264/60000]\n",
      "loss: 0.289310  [25664/60000]\n",
      "loss: 0.294587  [32064/60000]\n",
      "loss: 0.289767  [38464/60000]\n",
      "loss: 0.347941  [44864/60000]\n",
      "loss: 0.434263  [51264/60000]\n",
      "loss: 0.365465  [57664/60000]\n",
      "Average training loss: 0.2926701094979035\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.287924 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.371517 \n",
      "\n",
      "Change in average loss: 0.0032652161999551788\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.209288  [   64/60000]\n",
      "loss: 0.284547  [ 6464/60000]\n",
      "loss: 0.189162  [12864/60000]\n",
      "loss: 0.325544  [19264/60000]\n",
      "loss: 0.287755  [25664/60000]\n",
      "loss: 0.288749  [32064/60000]\n",
      "loss: 0.286272  [38464/60000]\n",
      "loss: 0.340914  [44864/60000]\n",
      "loss: 0.428033  [51264/60000]\n",
      "loss: 0.361270  [57664/60000]\n",
      "Average training loss: 0.28961719135676367\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.284927 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.371249 \n",
      "\n",
      "Change in average loss: 0.0030529181411398376\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.207879  [   64/60000]\n",
      "loss: 0.281792  [ 6464/60000]\n",
      "loss: 0.189168  [12864/60000]\n",
      "loss: 0.322531  [19264/60000]\n",
      "loss: 0.286189  [25664/60000]\n",
      "loss: 0.285295  [32064/60000]\n",
      "loss: 0.284138  [38464/60000]\n",
      "loss: 0.333315  [44864/60000]\n",
      "loss: 0.421535  [51264/60000]\n",
      "loss: 0.358785  [57664/60000]\n",
      "Average training loss: 0.28664932830501466\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.282017 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.370675 \n",
      "\n",
      "Change in average loss: 0.002967863051749009\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.206729  [   64/60000]\n",
      "loss: 0.279577  [ 6464/60000]\n",
      "loss: 0.188941  [12864/60000]\n",
      "loss: 0.320690  [19264/60000]\n",
      "loss: 0.283193  [25664/60000]\n",
      "loss: 0.282202  [32064/60000]\n",
      "loss: 0.280641  [38464/60000]\n",
      "loss: 0.326551  [44864/60000]\n",
      "loss: 0.415218  [51264/60000]\n",
      "loss: 0.355173  [57664/60000]\n",
      "Average training loss: 0.2837588467053386\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.279391 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.370487 \n",
      "\n",
      "Change in average loss: 0.002890481599676087\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.205166  [   64/60000]\n",
      "loss: 0.277238  [ 6464/60000]\n",
      "loss: 0.188834  [12864/60000]\n",
      "loss: 0.317894  [19264/60000]\n",
      "loss: 0.281713  [25664/60000]\n",
      "loss: 0.278044  [32064/60000]\n",
      "loss: 0.276852  [38464/60000]\n",
      "loss: 0.322845  [44864/60000]\n",
      "loss: 0.408966  [51264/60000]\n",
      "loss: 0.352162  [57664/60000]\n",
      "Average training loss: 0.2810333195541586\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.276613 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.370010 \n",
      "\n",
      "Change in average loss: 0.002725527151179963\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.202313  [   64/60000]\n",
      "loss: 0.274659  [ 6464/60000]\n",
      "loss: 0.188819  [12864/60000]\n",
      "loss: 0.317781  [19264/60000]\n",
      "loss: 0.279733  [25664/60000]\n",
      "loss: 0.275167  [32064/60000]\n",
      "loss: 0.272163  [38464/60000]\n",
      "loss: 0.315953  [44864/60000]\n",
      "loss: 0.402744  [51264/60000]\n",
      "loss: 0.347795  [57664/60000]\n",
      "Average training loss: 0.2783274632463577\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.274112 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.369915 \n",
      "\n",
      "Change in average loss: 0.0027058563078009024\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.199899  [   64/60000]\n",
      "loss: 0.272680  [ 6464/60000]\n",
      "loss: 0.188950  [12864/60000]\n",
      "loss: 0.316131  [19264/60000]\n",
      "loss: 0.277862  [25664/60000]\n",
      "loss: 0.271797  [32064/60000]\n",
      "loss: 0.269227  [38464/60000]\n",
      "loss: 0.310237  [44864/60000]\n",
      "loss: 0.397413  [51264/60000]\n",
      "loss: 0.346397  [57664/60000]\n",
      "Average training loss: 0.2757880294691525\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.271539 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.369392 \n",
      "\n",
      "Change in average loss: 0.0025394337772052245\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.197352  [   64/60000]\n",
      "loss: 0.270652  [ 6464/60000]\n",
      "loss: 0.189428  [12864/60000]\n",
      "loss: 0.314303  [19264/60000]\n",
      "loss: 0.274844  [25664/60000]\n",
      "loss: 0.268001  [32064/60000]\n",
      "loss: 0.266169  [38464/60000]\n",
      "loss: 0.305846  [44864/60000]\n",
      "loss: 0.393151  [51264/60000]\n",
      "loss: 0.343254  [57664/60000]\n",
      "Average training loss: 0.2732794573947565\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.269145 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.369141 \n",
      "\n",
      "Change in average loss: 0.002508572074395976\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.195686  [   64/60000]\n",
      "loss: 0.269485  [ 6464/60000]\n",
      "loss: 0.189912  [12864/60000]\n",
      "loss: 0.311779  [19264/60000]\n",
      "loss: 0.272388  [25664/60000]\n",
      "loss: 0.263377  [32064/60000]\n",
      "loss: 0.261761  [38464/60000]\n",
      "loss: 0.300786  [44864/60000]\n",
      "loss: 0.388227  [51264/60000]\n",
      "loss: 0.339299  [57664/60000]\n",
      "Average training loss: 0.27082901474223464\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.266725 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368962 \n",
      "\n",
      "Change in average loss: 0.002450442652521867\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.193572  [   64/60000]\n",
      "loss: 0.266590  [ 6464/60000]\n",
      "loss: 0.190339  [12864/60000]\n",
      "loss: 0.309453  [19264/60000]\n",
      "loss: 0.270493  [25664/60000]\n",
      "loss: 0.260248  [32064/60000]\n",
      "loss: 0.258323  [38464/60000]\n",
      "loss: 0.295261  [44864/60000]\n",
      "loss: 0.383486  [51264/60000]\n",
      "loss: 0.335969  [57664/60000]\n",
      "Average training loss: 0.2684529011787128\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.264372 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.368789 \n",
      "\n",
      "Change in average loss: 0.0023761135635218533\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.192660  [   64/60000]\n",
      "loss: 0.265259  [ 6464/60000]\n",
      "loss: 0.190834  [12864/60000]\n",
      "loss: 0.306903  [19264/60000]\n",
      "loss: 0.267923  [25664/60000]\n",
      "loss: 0.255521  [32064/60000]\n",
      "loss: 0.252705  [38464/60000]\n",
      "loss: 0.290126  [44864/60000]\n",
      "loss: 0.377928  [51264/60000]\n",
      "loss: 0.332548  [57664/60000]\n",
      "Average training loss: 0.2661547617935168\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.262005 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368613 \n",
      "\n",
      "Change in average loss: 0.002298139385195963\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.189453  [   64/60000]\n",
      "loss: 0.263314  [ 6464/60000]\n",
      "loss: 0.190129  [12864/60000]\n",
      "loss: 0.304780  [19264/60000]\n",
      "loss: 0.265265  [25664/60000]\n",
      "loss: 0.252870  [32064/60000]\n",
      "loss: 0.251237  [38464/60000]\n",
      "loss: 0.285743  [44864/60000]\n",
      "loss: 0.373118  [51264/60000]\n",
      "loss: 0.329780  [57664/60000]\n",
      "Average training loss: 0.2639314335054045\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.259856 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.368477 \n",
      "\n",
      "Change in average loss: 0.002223328288112336\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.188582  [   64/60000]\n",
      "loss: 0.261216  [ 6464/60000]\n",
      "loss: 0.192200  [12864/60000]\n",
      "loss: 0.302370  [19264/60000]\n",
      "loss: 0.262490  [25664/60000]\n",
      "loss: 0.249417  [32064/60000]\n",
      "loss: 0.248319  [38464/60000]\n",
      "loss: 0.281781  [44864/60000]\n",
      "loss: 0.368622  [51264/60000]\n",
      "loss: 0.327001  [57664/60000]\n",
      "Average training loss: 0.26174357442109825\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.257692 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368187 \n",
      "\n",
      "Change in average loss: 0.002187859084306243\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.187275  [   64/60000]\n",
      "loss: 0.258469  [ 6464/60000]\n",
      "loss: 0.192091  [12864/60000]\n",
      "loss: 0.299415  [19264/60000]\n",
      "loss: 0.260719  [25664/60000]\n",
      "loss: 0.244500  [32064/60000]\n",
      "loss: 0.245883  [38464/60000]\n",
      "loss: 0.276620  [44864/60000]\n",
      "loss: 0.362596  [51264/60000]\n",
      "loss: 0.323691  [57664/60000]\n",
      "Average training loss: 0.2595938006793258\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.255584 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368292 \n",
      "\n",
      "Change in average loss: 0.0021497737417724427\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.185946  [   64/60000]\n",
      "loss: 0.257166  [ 6464/60000]\n",
      "loss: 0.193198  [12864/60000]\n",
      "loss: 0.297238  [19264/60000]\n",
      "loss: 0.258121  [25664/60000]\n",
      "loss: 0.242227  [32064/60000]\n",
      "loss: 0.244311  [38464/60000]\n",
      "loss: 0.272595  [44864/60000]\n",
      "loss: 0.358230  [51264/60000]\n",
      "loss: 0.321071  [57664/60000]\n",
      "Average training loss: 0.25750901509545\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.253517 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368295 \n",
      "\n",
      "Change in average loss: 0.0020847855838758145\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.184272  [   64/60000]\n",
      "loss: 0.255909  [ 6464/60000]\n",
      "loss: 0.193715  [12864/60000]\n",
      "loss: 0.294068  [19264/60000]\n",
      "loss: 0.255866  [25664/60000]\n",
      "loss: 0.239356  [32064/60000]\n",
      "loss: 0.242058  [38464/60000]\n",
      "loss: 0.269108  [44864/60000]\n",
      "loss: 0.353815  [51264/60000]\n",
      "loss: 0.320698  [57664/60000]\n",
      "Average training loss: 0.255491039225224\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.251402 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368084 \n",
      "\n",
      "Change in average loss: 0.0020179758702260076\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.182185  [   64/60000]\n",
      "loss: 0.253656  [ 6464/60000]\n",
      "loss: 0.193974  [12864/60000]\n",
      "loss: 0.290689  [19264/60000]\n",
      "loss: 0.253562  [25664/60000]\n",
      "loss: 0.236126  [32064/60000]\n",
      "loss: 0.240785  [38464/60000]\n",
      "loss: 0.266023  [44864/60000]\n",
      "loss: 0.349351  [51264/60000]\n",
      "loss: 0.318864  [57664/60000]\n",
      "Average training loss: 0.25352528496687093\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.249552 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368327 \n",
      "\n",
      "Change in average loss: 0.0019657542583530474\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.180622  [   64/60000]\n",
      "loss: 0.252063  [ 6464/60000]\n",
      "loss: 0.194413  [12864/60000]\n",
      "loss: 0.288906  [19264/60000]\n",
      "loss: 0.250900  [25664/60000]\n",
      "loss: 0.233928  [32064/60000]\n",
      "loss: 0.239387  [38464/60000]\n",
      "loss: 0.262992  [44864/60000]\n",
      "loss: 0.345052  [51264/60000]\n",
      "loss: 0.317571  [57664/60000]\n",
      "Average training loss: 0.2515795254456336\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.247597 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368555 \n",
      "\n",
      "Change in average loss: 0.0019457595212373224\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.179068  [   64/60000]\n",
      "loss: 0.250628  [ 6464/60000]\n",
      "loss: 0.192483  [12864/60000]\n",
      "loss: 0.286540  [19264/60000]\n",
      "loss: 0.248216  [25664/60000]\n",
      "loss: 0.231122  [32064/60000]\n",
      "loss: 0.238923  [38464/60000]\n",
      "loss: 0.257918  [44864/60000]\n",
      "loss: 0.341941  [51264/60000]\n",
      "loss: 0.314666  [57664/60000]\n",
      "Average training loss: 0.24974133107644408\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.245646 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.368290 \n",
      "\n",
      "Change in average loss: 0.0018381943691895275\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.178250  [   64/60000]\n",
      "loss: 0.247550  [ 6464/60000]\n",
      "loss: 0.194909  [12864/60000]\n",
      "loss: 0.285124  [19264/60000]\n",
      "loss: 0.245123  [25664/60000]\n",
      "loss: 0.227740  [32064/60000]\n",
      "loss: 0.237367  [38464/60000]\n",
      "loss: 0.256054  [44864/60000]\n",
      "loss: 0.339212  [51264/60000]\n",
      "loss: 0.312168  [57664/60000]\n",
      "Average training loss: 0.24788644614377256\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.243724 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.368437 \n",
      "\n",
      "Change in average loss: 0.0018548849326715244\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.176472  [   64/60000]\n",
      "loss: 0.246849  [ 6464/60000]\n",
      "loss: 0.194013  [12864/60000]\n",
      "loss: 0.282657  [19264/60000]\n",
      "loss: 0.242727  [25664/60000]\n",
      "loss: 0.224863  [32064/60000]\n",
      "loss: 0.237222  [38464/60000]\n",
      "loss: 0.252881  [44864/60000]\n",
      "loss: 0.335473  [51264/60000]\n",
      "loss: 0.310992  [57664/60000]\n",
      "Average training loss: 0.24608184859506102\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.241932 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.368702 \n",
      "\n",
      "Change in average loss: 0.0018045975487115395\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.174705  [   64/60000]\n",
      "loss: 0.245754  [ 6464/60000]\n",
      "loss: 0.194318  [12864/60000]\n",
      "loss: 0.280196  [19264/60000]\n",
      "loss: 0.240592  [25664/60000]\n",
      "loss: 0.222604  [32064/60000]\n",
      "loss: 0.237578  [38464/60000]\n",
      "loss: 0.249846  [44864/60000]\n",
      "loss: 0.333505  [51264/60000]\n",
      "loss: 0.309138  [57664/60000]\n",
      "Average training loss: 0.24428896223907787\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.240178 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.369023 \n",
      "\n",
      "Change in average loss: 0.0017928863559831532\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.173036  [   64/60000]\n",
      "loss: 0.244385  [ 6464/60000]\n",
      "loss: 0.194121  [12864/60000]\n",
      "loss: 0.278354  [19264/60000]\n",
      "loss: 0.239300  [25664/60000]\n",
      "loss: 0.220075  [32064/60000]\n",
      "loss: 0.236403  [38464/60000]\n",
      "loss: 0.246185  [44864/60000]\n",
      "loss: 0.329184  [51264/60000]\n",
      "loss: 0.305948  [57664/60000]\n",
      "Average training loss: 0.2425246888807397\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.238525 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.369323 \n",
      "\n",
      "Change in average loss: 0.0017642733583381753\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.172175  [   64/60000]\n",
      "loss: 0.243673  [ 6464/60000]\n",
      "loss: 0.194347  [12864/60000]\n",
      "loss: 0.275823  [19264/60000]\n",
      "loss: 0.236721  [25664/60000]\n",
      "loss: 0.216678  [32064/60000]\n",
      "loss: 0.236024  [38464/60000]\n",
      "loss: 0.244468  [44864/60000]\n",
      "loss: 0.325429  [51264/60000]\n",
      "loss: 0.304396  [57664/60000]\n",
      "Average training loss: 0.24085748889870734\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.236716 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.369420 \n",
      "\n",
      "Change in average loss: 0.0016671999820323524\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.170047  [   64/60000]\n",
      "loss: 0.242962  [ 6464/60000]\n",
      "loss: 0.194385  [12864/60000]\n",
      "loss: 0.274015  [19264/60000]\n",
      "loss: 0.234194  [25664/60000]\n",
      "loss: 0.215116  [32064/60000]\n",
      "loss: 0.234842  [38464/60000]\n",
      "loss: 0.242487  [44864/60000]\n",
      "loss: 0.321151  [51264/60000]\n",
      "loss: 0.301534  [57664/60000]\n",
      "Average training loss: 0.2392206414541138\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.235026 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.369859 \n",
      "\n",
      "Change in average loss: 0.0016368474445935255\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.168739  [   64/60000]\n",
      "loss: 0.243645  [ 6464/60000]\n",
      "loss: 0.193935  [12864/60000]\n",
      "loss: 0.270944  [19264/60000]\n",
      "loss: 0.232300  [25664/60000]\n",
      "loss: 0.211979  [32064/60000]\n",
      "loss: 0.233540  [38464/60000]\n",
      "loss: 0.240592  [44864/60000]\n",
      "loss: 0.318438  [51264/60000]\n",
      "loss: 0.299336  [57664/60000]\n",
      "Average training loss: 0.23760164164102027\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.233390 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.370178 \n",
      "\n",
      "Change in average loss: 0.0016189998130935468\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.167004  [   64/60000]\n",
      "loss: 0.240949  [ 6464/60000]\n",
      "loss: 0.194753  [12864/60000]\n",
      "loss: 0.268994  [19264/60000]\n",
      "loss: 0.230216  [25664/60000]\n",
      "loss: 0.210771  [32064/60000]\n",
      "loss: 0.232110  [38464/60000]\n",
      "loss: 0.237708  [44864/60000]\n",
      "loss: 0.316667  [51264/60000]\n",
      "loss: 0.297765  [57664/60000]\n",
      "Average training loss: 0.23600344953220537\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.231884 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.370637 \n",
      "\n",
      "Change in average loss: 0.0015981921088148998\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.163855  [   64/60000]\n",
      "loss: 0.241555  [ 6464/60000]\n",
      "loss: 0.195284  [12864/60000]\n",
      "loss: 0.266872  [19264/60000]\n",
      "loss: 0.228012  [25664/60000]\n",
      "loss: 0.209327  [32064/60000]\n",
      "loss: 0.231048  [38464/60000]\n",
      "loss: 0.235841  [44864/60000]\n",
      "loss: 0.311946  [51264/60000]\n",
      "loss: 0.294867  [57664/60000]\n",
      "Average training loss: 0.23444463151381978\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.230411 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.371210 \n",
      "\n",
      "Change in average loss: 0.0015588180183855882\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.165324  [   64/60000]\n",
      "loss: 0.241619  [ 6464/60000]\n",
      "loss: 0.195512  [12864/60000]\n",
      "loss: 0.264599  [19264/60000]\n",
      "loss: 0.227026  [25664/60000]\n",
      "loss: 0.208613  [32064/60000]\n",
      "loss: 0.231200  [38464/60000]\n",
      "loss: 0.233391  [44864/60000]\n",
      "loss: 0.310227  [51264/60000]\n",
      "loss: 0.292901  [57664/60000]\n",
      "Average training loss: 0.23292297578013654\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.228880 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.371861 \n",
      "\n",
      "Change in average loss: 0.001521655733683236\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.163532  [   64/60000]\n",
      "loss: 0.241684  [ 6464/60000]\n",
      "loss: 0.195441  [12864/60000]\n",
      "loss: 0.261962  [19264/60000]\n",
      "loss: 0.226141  [25664/60000]\n",
      "loss: 0.206814  [32064/60000]\n",
      "loss: 0.231170  [38464/60000]\n",
      "loss: 0.231128  [44864/60000]\n",
      "loss: 0.308513  [51264/60000]\n",
      "loss: 0.291489  [57664/60000]\n",
      "Average training loss: 0.23142604318572513\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.227427 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.372360 \n",
      "\n",
      "Change in average loss: 0.0014969325944114154\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.162679  [   64/60000]\n",
      "loss: 0.239344  [ 6464/60000]\n",
      "loss: 0.195424  [12864/60000]\n",
      "loss: 0.259888  [19264/60000]\n",
      "loss: 0.224771  [25664/60000]\n",
      "loss: 0.206152  [32064/60000]\n",
      "loss: 0.229932  [38464/60000]\n",
      "loss: 0.230367  [44864/60000]\n",
      "loss: 0.307007  [51264/60000]\n",
      "loss: 0.291264  [57664/60000]\n",
      "Average training loss: 0.2299716153116559\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.225833 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.373009 \n",
      "\n",
      "Change in average loss: 0.001454427874069214\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.160491  [   64/60000]\n",
      "loss: 0.237915  [ 6464/60000]\n",
      "loss: 0.195290  [12864/60000]\n",
      "loss: 0.257328  [19264/60000]\n",
      "loss: 0.223789  [25664/60000]\n",
      "loss: 0.205572  [32064/60000]\n",
      "loss: 0.230061  [38464/60000]\n",
      "loss: 0.229202  [44864/60000]\n",
      "loss: 0.304823  [51264/60000]\n",
      "loss: 0.289328  [57664/60000]\n",
      "Average training loss: 0.2285173718315134\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.224475 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.373514 \n",
      "\n",
      "Change in average loss: 0.001454243480142503\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.160853  [   64/60000]\n",
      "loss: 0.237003  [ 6464/60000]\n",
      "loss: 0.195366  [12864/60000]\n",
      "loss: 0.256624  [19264/60000]\n",
      "loss: 0.224187  [25664/60000]\n",
      "loss: 0.202419  [32064/60000]\n",
      "loss: 0.230195  [38464/60000]\n",
      "loss: 0.228903  [44864/60000]\n",
      "loss: 0.304088  [51264/60000]\n",
      "loss: 0.287696  [57664/60000]\n",
      "Average training loss: 0.2270643933535194\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.223090 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.374170 \n",
      "\n",
      "Change in average loss: 0.0014529784779939992\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.160285  [   64/60000]\n",
      "loss: 0.236614  [ 6464/60000]\n",
      "loss: 0.195115  [12864/60000]\n",
      "loss: 0.253833  [19264/60000]\n",
      "loss: 0.223356  [25664/60000]\n",
      "loss: 0.201674  [32064/60000]\n",
      "loss: 0.229229  [38464/60000]\n",
      "loss: 0.227965  [44864/60000]\n",
      "loss: 0.301351  [51264/60000]\n",
      "loss: 0.285920  [57664/60000]\n",
      "Average training loss: 0.22561991153590716\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.221712 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.374928 \n",
      "\n",
      "Change in average loss: 0.0014444818176122531\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.159955  [   64/60000]\n",
      "loss: 0.236019  [ 6464/60000]\n",
      "loss: 0.195797  [12864/60000]\n",
      "loss: 0.252122  [19264/60000]\n",
      "loss: 0.222548  [25664/60000]\n",
      "loss: 0.200367  [32064/60000]\n",
      "loss: 0.228212  [38464/60000]\n",
      "loss: 0.225202  [44864/60000]\n",
      "loss: 0.301191  [51264/60000]\n",
      "loss: 0.286618  [57664/60000]\n",
      "Average training loss: 0.2242336687899983\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.220378 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.375481 \n",
      "\n",
      "Change in average loss: 0.0013862427459088444\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.158913  [   64/60000]\n",
      "loss: 0.235310  [ 6464/60000]\n",
      "loss: 0.194519  [12864/60000]\n",
      "loss: 0.250781  [19264/60000]\n",
      "loss: 0.222446  [25664/60000]\n",
      "loss: 0.199506  [32064/60000]\n",
      "loss: 0.228009  [38464/60000]\n",
      "loss: 0.224349  [44864/60000]\n",
      "loss: 0.299632  [51264/60000]\n",
      "loss: 0.284186  [57664/60000]\n",
      "Average training loss: 0.22286923867521255\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.218972 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.376097 \n",
      "\n",
      "Change in average loss: 0.001364430114785764\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.158023  [   64/60000]\n",
      "loss: 0.234898  [ 6464/60000]\n",
      "loss: 0.193688  [12864/60000]\n",
      "loss: 0.249299  [19264/60000]\n",
      "loss: 0.221534  [25664/60000]\n",
      "loss: 0.197565  [32064/60000]\n",
      "loss: 0.227430  [38464/60000]\n",
      "loss: 0.223701  [44864/60000]\n",
      "loss: 0.298256  [51264/60000]\n",
      "loss: 0.282522  [57664/60000]\n",
      "Average training loss: 0.22149083817373716\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.217662 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.376726 \n",
      "\n",
      "Change in average loss: 0.0013784005014753908\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.158139  [   64/60000]\n",
      "loss: 0.235421  [ 6464/60000]\n",
      "loss: 0.194204  [12864/60000]\n",
      "loss: 0.248921  [19264/60000]\n",
      "loss: 0.221241  [25664/60000]\n",
      "loss: 0.194647  [32064/60000]\n",
      "loss: 0.227428  [38464/60000]\n",
      "loss: 0.222564  [44864/60000]\n",
      "loss: 0.296301  [51264/60000]\n",
      "loss: 0.282096  [57664/60000]\n",
      "Average training loss: 0.22015233491974345\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.216452 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.377380 \n",
      "\n",
      "Change in average loss: 0.0013385032539937058\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.155749  [   64/60000]\n",
      "loss: 0.233137  [ 6464/60000]\n",
      "loss: 0.193841  [12864/60000]\n",
      "loss: 0.247414  [19264/60000]\n",
      "loss: 0.221583  [25664/60000]\n",
      "loss: 0.194327  [32064/60000]\n",
      "loss: 0.225904  [38464/60000]\n",
      "loss: 0.221680  [44864/60000]\n",
      "loss: 0.294777  [51264/60000]\n",
      "loss: 0.279655  [57664/60000]\n",
      "Average training loss: 0.21884674617428895\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.215112 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.377975 \n",
      "\n",
      "Change in average loss: 0.0013055887454545\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.155039  [   64/60000]\n",
      "loss: 0.232102  [ 6464/60000]\n",
      "loss: 0.192748  [12864/60000]\n",
      "loss: 0.245641  [19264/60000]\n",
      "loss: 0.221106  [25664/60000]\n",
      "loss: 0.193274  [32064/60000]\n",
      "loss: 0.225238  [38464/60000]\n",
      "loss: 0.220483  [44864/60000]\n",
      "loss: 0.292697  [51264/60000]\n",
      "loss: 0.280062  [57664/60000]\n",
      "Average training loss: 0.21756674309989918\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.213857 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.378647 \n",
      "\n",
      "Change in average loss: 0.001280003074389774\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.153076  [   64/60000]\n",
      "loss: 0.231639  [ 6464/60000]\n",
      "loss: 0.193360  [12864/60000]\n",
      "loss: 0.243929  [19264/60000]\n",
      "loss: 0.221416  [25664/60000]\n",
      "loss: 0.192451  [32064/60000]\n",
      "loss: 0.223522  [38464/60000]\n",
      "loss: 0.220121  [44864/60000]\n",
      "loss: 0.290488  [51264/60000]\n",
      "loss: 0.278857  [57664/60000]\n",
      "Average training loss: 0.21629578377138067\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.212574 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.379392 \n",
      "\n",
      "Change in average loss: 0.0012709593285185061\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.152632  [   64/60000]\n",
      "loss: 0.231570  [ 6464/60000]\n",
      "loss: 0.192186  [12864/60000]\n",
      "loss: 0.243019  [19264/60000]\n",
      "loss: 0.219937  [25664/60000]\n",
      "loss: 0.190582  [32064/60000]\n",
      "loss: 0.223019  [38464/60000]\n",
      "loss: 0.220684  [44864/60000]\n",
      "loss: 0.289534  [51264/60000]\n",
      "loss: 0.277305  [57664/60000]\n",
      "Average training loss: 0.21507345987503718\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.211322 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.380019 \n",
      "\n",
      "Change in average loss: 0.0012223238963434968\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.151391  [   64/60000]\n",
      "loss: 0.230940  [ 6464/60000]\n",
      "loss: 0.192272  [12864/60000]\n",
      "loss: 0.241485  [19264/60000]\n",
      "loss: 0.220918  [25664/60000]\n",
      "loss: 0.189899  [32064/60000]\n",
      "loss: 0.221911  [38464/60000]\n",
      "loss: 0.218580  [44864/60000]\n",
      "loss: 0.287271  [51264/60000]\n",
      "loss: 0.277886  [57664/60000]\n",
      "Average training loss: 0.2138779669650582\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.210119 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.380902 \n",
      "\n",
      "Change in average loss: 0.0011954929099789735\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.150657  [   64/60000]\n",
      "loss: 0.230027  [ 6464/60000]\n",
      "loss: 0.192059  [12864/60000]\n",
      "loss: 0.240978  [19264/60000]\n",
      "loss: 0.220384  [25664/60000]\n",
      "loss: 0.189453  [32064/60000]\n",
      "loss: 0.221886  [38464/60000]\n",
      "loss: 0.219070  [44864/60000]\n",
      "loss: 0.285764  [51264/60000]\n",
      "loss: 0.276417  [57664/60000]\n",
      "Average training loss: 0.21267212217629972\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.209046 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.381797 \n",
      "\n",
      "Change in average loss: 0.0012058447887584867\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.149551  [   64/60000]\n",
      "loss: 0.229153  [ 6464/60000]\n",
      "loss: 0.191484  [12864/60000]\n",
      "loss: 0.239489  [19264/60000]\n",
      "loss: 0.220750  [25664/60000]\n",
      "loss: 0.187226  [32064/60000]\n",
      "loss: 0.222463  [38464/60000]\n",
      "loss: 0.216838  [44864/60000]\n",
      "loss: 0.285987  [51264/60000]\n",
      "loss: 0.275142  [57664/60000]\n",
      "Average training loss: 0.21151823000803685\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.207882 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.382638 \n",
      "\n",
      "Change in average loss: 0.0011538921682628656\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.149678  [   64/60000]\n",
      "loss: 0.229369  [ 6464/60000]\n",
      "loss: 0.190987  [12864/60000]\n",
      "loss: 0.237587  [19264/60000]\n",
      "loss: 0.220287  [25664/60000]\n",
      "loss: 0.188364  [32064/60000]\n",
      "loss: 0.221318  [38464/60000]\n",
      "loss: 0.215522  [44864/60000]\n",
      "loss: 0.282798  [51264/60000]\n",
      "loss: 0.272636  [57664/60000]\n",
      "Average training loss: 0.21030824443996587\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.206785 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.383218 \n",
      "\n",
      "Change in average loss: 0.001209985568070976\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.148512  [   64/60000]\n",
      "loss: 0.228876  [ 6464/60000]\n",
      "loss: 0.190012  [12864/60000]\n",
      "loss: 0.237306  [19264/60000]\n",
      "loss: 0.221094  [25664/60000]\n",
      "loss: 0.187075  [32064/60000]\n",
      "loss: 0.221753  [38464/60000]\n",
      "loss: 0.214868  [44864/60000]\n",
      "loss: 0.283512  [51264/60000]\n",
      "loss: 0.270957  [57664/60000]\n",
      "Average training loss: 0.2091682862275953\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.205529 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.384176 \n",
      "\n",
      "Change in average loss: 0.001139958212370562\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.147597  [   64/60000]\n",
      "loss: 0.228695  [ 6464/60000]\n",
      "loss: 0.189723  [12864/60000]\n",
      "loss: 0.234829  [19264/60000]\n",
      "loss: 0.220630  [25664/60000]\n",
      "loss: 0.186803  [32064/60000]\n",
      "loss: 0.220629  [38464/60000]\n",
      "loss: 0.214475  [44864/60000]\n",
      "loss: 0.281311  [51264/60000]\n",
      "loss: 0.270152  [57664/60000]\n",
      "Average training loss: 0.2079786479171278\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.204452 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.385146 \n",
      "\n",
      "Change in average loss: 0.0011896383104675057\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.146815  [   64/60000]\n",
      "loss: 0.230357  [ 6464/60000]\n",
      "loss: 0.188511  [12864/60000]\n",
      "loss: 0.234871  [19264/60000]\n",
      "loss: 0.221241  [25664/60000]\n",
      "loss: 0.187705  [32064/60000]\n",
      "loss: 0.220485  [38464/60000]\n",
      "loss: 0.212885  [44864/60000]\n",
      "loss: 0.280633  [51264/60000]\n",
      "loss: 0.270995  [57664/60000]\n",
      "Average training loss: 0.20688786674568901\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.203361 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.385852 \n",
      "\n",
      "Change in average loss: 0.0010907811714387927\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.145859  [   64/60000]\n",
      "loss: 0.229412  [ 6464/60000]\n",
      "loss: 0.189190  [12864/60000]\n",
      "loss: 0.233266  [19264/60000]\n",
      "loss: 0.219611  [25664/60000]\n",
      "loss: 0.185528  [32064/60000]\n",
      "loss: 0.219693  [38464/60000]\n",
      "loss: 0.212963  [44864/60000]\n",
      "loss: 0.278548  [51264/60000]\n",
      "loss: 0.267323  [57664/60000]\n",
      "Average training loss: 0.20572347797230997\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.202279 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.386829 \n",
      "\n",
      "Change in average loss: 0.0011643887733790437\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.144836  [   64/60000]\n",
      "loss: 0.228278  [ 6464/60000]\n",
      "loss: 0.187924  [12864/60000]\n",
      "loss: 0.232549  [19264/60000]\n",
      "loss: 0.220020  [25664/60000]\n",
      "loss: 0.185961  [32064/60000]\n",
      "loss: 0.219625  [38464/60000]\n",
      "loss: 0.212381  [44864/60000]\n",
      "loss: 0.276775  [51264/60000]\n",
      "loss: 0.267375  [57664/60000]\n",
      "Average training loss: 0.20462021438170597\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.201106 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.387522 \n",
      "\n",
      "Change in average loss: 0.0011032635906040034\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.143982  [   64/60000]\n",
      "loss: 0.231212  [ 6464/60000]\n",
      "loss: 0.187022  [12864/60000]\n",
      "loss: 0.230530  [19264/60000]\n",
      "loss: 0.220247  [25664/60000]\n",
      "loss: 0.184910  [32064/60000]\n",
      "loss: 0.218594  [38464/60000]\n",
      "loss: 0.210492  [44864/60000]\n",
      "loss: 0.276509  [51264/60000]\n",
      "loss: 0.266021  [57664/60000]\n",
      "Average training loss: 0.2035844351556192\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.199944 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.388080 \n",
      "\n",
      "Change in average loss: 0.0010357792260867749\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.141924  [   64/60000]\n",
      "loss: 0.228906  [ 6464/60000]\n",
      "loss: 0.186940  [12864/60000]\n",
      "loss: 0.229874  [19264/60000]\n",
      "loss: 0.219281  [25664/60000]\n",
      "loss: 0.185907  [32064/60000]\n",
      "loss: 0.217008  [38464/60000]\n",
      "loss: 0.209100  [44864/60000]\n",
      "loss: 0.274451  [51264/60000]\n",
      "loss: 0.264445  [57664/60000]\n",
      "Average training loss: 0.20244121460168601\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.198985 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.388829 \n",
      "\n",
      "Change in average loss: 0.0011432205539331775\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.140251  [   64/60000]\n",
      "loss: 0.229353  [ 6464/60000]\n",
      "loss: 0.187581  [12864/60000]\n",
      "loss: 0.228602  [19264/60000]\n",
      "loss: 0.218538  [25664/60000]\n",
      "loss: 0.185679  [32064/60000]\n",
      "loss: 0.217086  [38464/60000]\n",
      "loss: 0.209771  [44864/60000]\n",
      "loss: 0.272931  [51264/60000]\n",
      "loss: 0.262600  [57664/60000]\n",
      "Average training loss: 0.20138842341249813\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.197825 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.389674 \n",
      "\n",
      "Change in average loss: 0.001052791189187885\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.139837  [   64/60000]\n",
      "loss: 0.229358  [ 6464/60000]\n",
      "loss: 0.186309  [12864/60000]\n",
      "loss: 0.227208  [19264/60000]\n",
      "loss: 0.219016  [25664/60000]\n",
      "loss: 0.184531  [32064/60000]\n",
      "loss: 0.216868  [38464/60000]\n",
      "loss: 0.207338  [44864/60000]\n",
      "loss: 0.270915  [51264/60000]\n",
      "loss: 0.262428  [57664/60000]\n",
      "Average training loss: 0.20033782221742277\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.196866 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.390378 \n",
      "\n",
      "Change in average loss: 0.0010506011950753569\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.138857  [   64/60000]\n",
      "loss: 0.229033  [ 6464/60000]\n",
      "loss: 0.186406  [12864/60000]\n",
      "loss: 0.225325  [19264/60000]\n",
      "loss: 0.219597  [25664/60000]\n",
      "loss: 0.183293  [32064/60000]\n",
      "loss: 0.214018  [38464/60000]\n",
      "loss: 0.206260  [44864/60000]\n",
      "loss: 0.268026  [51264/60000]\n",
      "loss: 0.261448  [57664/60000]\n",
      "Average training loss: 0.19932886400917318\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.195826 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.391340 \n",
      "\n",
      "Change in average loss: 0.0010089582082495885\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.137881  [   64/60000]\n",
      "loss: 0.227288  [ 6464/60000]\n",
      "loss: 0.187297  [12864/60000]\n",
      "loss: 0.223234  [19264/60000]\n",
      "loss: 0.220125  [25664/60000]\n",
      "loss: 0.182469  [32064/60000]\n",
      "loss: 0.213908  [38464/60000]\n",
      "loss: 0.206078  [44864/60000]\n",
      "loss: 0.268652  [51264/60000]\n",
      "loss: 0.259645  [57664/60000]\n",
      "Average training loss: 0.1983029770889262\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.194839 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.392136 \n",
      "\n",
      "Change in average loss: 0.0010258869202469934\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.137999  [   64/60000]\n",
      "loss: 0.227829  [ 6464/60000]\n",
      "loss: 0.187115  [12864/60000]\n",
      "loss: 0.222032  [19264/60000]\n",
      "loss: 0.219020  [25664/60000]\n",
      "loss: 0.182628  [32064/60000]\n",
      "loss: 0.213479  [38464/60000]\n",
      "loss: 0.203663  [44864/60000]\n",
      "loss: 0.267835  [51264/60000]\n",
      "loss: 0.259112  [57664/60000]\n",
      "Average training loss: 0.19730057811606794\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.193863 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.393407 \n",
      "\n",
      "Change in average loss: 0.0010023989728582539\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.135788  [   64/60000]\n",
      "loss: 0.226645  [ 6464/60000]\n",
      "loss: 0.186225  [12864/60000]\n",
      "loss: 0.220253  [19264/60000]\n",
      "loss: 0.219020  [25664/60000]\n",
      "loss: 0.181692  [32064/60000]\n",
      "loss: 0.212222  [38464/60000]\n",
      "loss: 0.201706  [44864/60000]\n",
      "loss: 0.265991  [51264/60000]\n",
      "loss: 0.257013  [57664/60000]\n",
      "Average training loss: 0.19626360023215508\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.192884 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.394186 \n",
      "\n",
      "Change in average loss: 0.001036977883912854\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.135162  [   64/60000]\n",
      "loss: 0.226984  [ 6464/60000]\n",
      "loss: 0.185472  [12864/60000]\n",
      "loss: 0.219791  [19264/60000]\n",
      "loss: 0.218803  [25664/60000]\n",
      "loss: 0.181202  [32064/60000]\n",
      "loss: 0.211598  [38464/60000]\n",
      "loss: 0.201138  [44864/60000]\n",
      "loss: 0.263382  [51264/60000]\n",
      "loss: 0.258305  [57664/60000]\n",
      "Average training loss: 0.19535499529988526\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.191875 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.395064 \n",
      "\n",
      "Change in average loss: 0.0009086049322698242\n",
      "TRAINING DONE!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_fn, optimizer, train_dataloader, dev_dataloader, end_training_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b75aeb-aa61-4d50-a191-145b939e77fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To fix that, we can add regularization to our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e3ef7-8c73-408c-a11a-30d83196659a",
   "metadata": {},
   "source": [
    "### One hidden layer with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7373a66d-8408-47b6-81c4-338cfdd5be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(layers_dims, layers_activations).to(device)\n",
    "print(model)\n",
    "\n",
    "weight_decay = 1e-4\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "974fd9a0-e041-40c8-ab84-66f236ceb7ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.316161  [   64/60000]\n",
      "loss: 1.182007  [ 6464/60000]\n",
      "loss: 0.661612  [12864/60000]\n",
      "loss: 0.883894  [19264/60000]\n",
      "loss: 0.686040  [25664/60000]\n",
      "loss: 0.663665  [32064/60000]\n",
      "loss: 0.593773  [38464/60000]\n",
      "loss: 0.632216  [44864/60000]\n",
      "loss: 0.655966  [51264/60000]\n",
      "loss: 0.577709  [57664/60000]\n",
      "Average training loss: 0.7552932457629044\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.506807 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.527547 \n",
      "\n",
      "Change in average loss: inf\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.409752  [   64/60000]\n",
      "loss: 0.529091  [ 6464/60000]\n",
      "loss: 0.351718  [12864/60000]\n",
      "loss: 0.629278  [19264/60000]\n",
      "loss: 0.476292  [25664/60000]\n",
      "loss: 0.500463  [32064/60000]\n",
      "loss: 0.438405  [38464/60000]\n",
      "loss: 0.577945  [44864/60000]\n",
      "loss: 0.600128  [51264/60000]\n",
      "loss: 0.466588  [57664/60000]\n",
      "Average training loss: 0.46700824296741344\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.432371 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.461777 \n",
      "\n",
      "Change in average loss: 0.2882850027954909\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.316784  [   64/60000]\n",
      "loss: 0.479900  [ 6464/60000]\n",
      "loss: 0.293812  [12864/60000]\n",
      "loss: 0.570340  [19264/60000]\n",
      "loss: 0.412600  [25664/60000]\n",
      "loss: 0.428030  [32064/60000]\n",
      "loss: 0.386954  [38464/60000]\n",
      "loss: 0.553011  [44864/60000]\n",
      "loss: 0.564386  [51264/60000]\n",
      "loss: 0.427132  [57664/60000]\n",
      "Average training loss: 0.418662677171515\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.399439 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.434449 \n",
      "\n",
      "Change in average loss: 0.04834556579589844\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.280079  [   64/60000]\n",
      "loss: 0.445858  [ 6464/60000]\n",
      "loss: 0.261570  [12864/60000]\n",
      "loss: 0.526907  [19264/60000]\n",
      "loss: 0.379700  [25664/60000]\n",
      "loss: 0.388619  [32064/60000]\n",
      "loss: 0.357929  [38464/60000]\n",
      "loss: 0.525017  [44864/60000]\n",
      "loss: 0.533914  [51264/60000]\n",
      "loss: 0.411221  [57664/60000]\n",
      "Average training loss: 0.39247484190631776\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.378314 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.417605 \n",
      "\n",
      "Change in average loss: 0.026187835265197246\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.259243  [   64/60000]\n",
      "loss: 0.419517  [ 6464/60000]\n",
      "loss: 0.240135  [12864/60000]\n",
      "loss: 0.487211  [19264/60000]\n",
      "loss: 0.356595  [25664/60000]\n",
      "loss: 0.365898  [32064/60000]\n",
      "loss: 0.337845  [38464/60000]\n",
      "loss: 0.500672  [44864/60000]\n",
      "loss: 0.505006  [51264/60000]\n",
      "loss: 0.402855  [57664/60000]\n",
      "Average training loss: 0.37441127631328763\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.362748 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.405208 \n",
      "\n",
      "Change in average loss: 0.01806356559303013\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.245969  [   64/60000]\n",
      "loss: 0.398189  [ 6464/60000]\n",
      "loss: 0.225023  [12864/60000]\n",
      "loss: 0.456248  [19264/60000]\n",
      "loss: 0.338069  [25664/60000]\n",
      "loss: 0.355203  [32064/60000]\n",
      "loss: 0.325986  [38464/60000]\n",
      "loss: 0.476460  [44864/60000]\n",
      "loss: 0.481513  [51264/60000]\n",
      "loss: 0.396190  [57664/60000]\n",
      "Average training loss: 0.36056199788983695\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.350350 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.395869 \n",
      "\n",
      "Change in average loss: 0.013849278423450684\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.236136  [   64/60000]\n",
      "loss: 0.380908  [ 6464/60000]\n",
      "loss: 0.215699  [12864/60000]\n",
      "loss: 0.431995  [19264/60000]\n",
      "loss: 0.323937  [25664/60000]\n",
      "loss: 0.345189  [32064/60000]\n",
      "loss: 0.314943  [38464/60000]\n",
      "loss: 0.455978  [44864/60000]\n",
      "loss: 0.461354  [51264/60000]\n",
      "loss: 0.392313  [57664/60000]\n",
      "Average training loss: 0.3492854794284809\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.340077 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.388898 \n",
      "\n",
      "Change in average loss: 0.011276518461356022\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.226128  [   64/60000]\n",
      "loss: 0.365553  [ 6464/60000]\n",
      "loss: 0.211002  [12864/60000]\n",
      "loss: 0.413811  [19264/60000]\n",
      "loss: 0.315255  [25664/60000]\n",
      "loss: 0.337263  [32064/60000]\n",
      "loss: 0.306289  [38464/60000]\n",
      "loss: 0.438833  [44864/60000]\n",
      "loss: 0.446970  [51264/60000]\n",
      "loss: 0.389042  [57664/60000]\n",
      "Average training loss: 0.33986310773630385\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.331447 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.383321 \n",
      "\n",
      "Change in average loss: 0.009422371692177078\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.219201  [   64/60000]\n",
      "loss: 0.355625  [ 6464/60000]\n",
      "loss: 0.206009  [12864/60000]\n",
      "loss: 0.398994  [19264/60000]\n",
      "loss: 0.306202  [25664/60000]\n",
      "loss: 0.329824  [32064/60000]\n",
      "loss: 0.297037  [38464/60000]\n",
      "loss: 0.426105  [44864/60000]\n",
      "loss: 0.435293  [51264/60000]\n",
      "loss: 0.384741  [57664/60000]\n",
      "Average training loss: 0.33173004618045615\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323885 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.378599 \n",
      "\n",
      "Change in average loss: 0.008133061555847698\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.213489  [   64/60000]\n",
      "loss: 0.345739  [ 6464/60000]\n",
      "loss: 0.202167  [12864/60000]\n",
      "loss: 0.387192  [19264/60000]\n",
      "loss: 0.299993  [25664/60000]\n",
      "loss: 0.323021  [32064/60000]\n",
      "loss: 0.288920  [38464/60000]\n",
      "loss: 0.416045  [44864/60000]\n",
      "loss: 0.424344  [51264/60000]\n",
      "loss: 0.380343  [57664/60000]\n",
      "Average training loss: 0.3245729063349619\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.317175 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.374412 \n",
      "\n",
      "Change in average loss: 0.007157139845494265\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.209245  [   64/60000]\n",
      "loss: 0.337840  [ 6464/60000]\n",
      "loss: 0.199448  [12864/60000]\n",
      "loss: 0.377184  [19264/60000]\n",
      "loss: 0.293684  [25664/60000]\n",
      "loss: 0.317623  [32064/60000]\n",
      "loss: 0.281036  [38464/60000]\n",
      "loss: 0.406565  [44864/60000]\n",
      "loss: 0.415392  [51264/60000]\n",
      "loss: 0.376105  [57664/60000]\n",
      "Average training loss: 0.3181339422427515\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.311020 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.370482 \n",
      "\n",
      "Change in average loss: 0.0064389640922103974\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.206901  [   64/60000]\n",
      "loss: 0.331384  [ 6464/60000]\n",
      "loss: 0.196289  [12864/60000]\n",
      "loss: 0.366967  [19264/60000]\n",
      "loss: 0.289080  [25664/60000]\n",
      "loss: 0.311507  [32064/60000]\n",
      "loss: 0.275622  [38464/60000]\n",
      "loss: 0.396952  [44864/60000]\n",
      "loss: 0.406085  [51264/60000]\n",
      "loss: 0.373085  [57664/60000]\n",
      "Average training loss: 0.31217968340780433\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.305260 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.366939 \n",
      "\n",
      "Change in average loss: 0.005954258834947157\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.205353  [   64/60000]\n",
      "loss: 0.326585  [ 6464/60000]\n",
      "loss: 0.193667  [12864/60000]\n",
      "loss: 0.359494  [19264/60000]\n",
      "loss: 0.284836  [25664/60000]\n",
      "loss: 0.306481  [32064/60000]\n",
      "loss: 0.269077  [38464/60000]\n",
      "loss: 0.386689  [44864/60000]\n",
      "loss: 0.396892  [51264/60000]\n",
      "loss: 0.369098  [57664/60000]\n",
      "Average training loss: 0.30667575567897193\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.299954 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.364109 \n",
      "\n",
      "Change in average loss: 0.005503927728832403\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.203735  [   64/60000]\n",
      "loss: 0.322508  [ 6464/60000]\n",
      "loss: 0.192683  [12864/60000]\n",
      "loss: 0.352689  [19264/60000]\n",
      "loss: 0.279469  [25664/60000]\n",
      "loss: 0.302711  [32064/60000]\n",
      "loss: 0.263955  [38464/60000]\n",
      "loss: 0.374272  [44864/60000]\n",
      "loss: 0.388218  [51264/60000]\n",
      "loss: 0.363726  [57664/60000]\n",
      "Average training loss: 0.30146975596068004\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.294950 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.361385 \n",
      "\n",
      "Change in average loss: 0.005205999718291887\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.202703  [   64/60000]\n",
      "loss: 0.319958  [ 6464/60000]\n",
      "loss: 0.190536  [12864/60000]\n",
      "loss: 0.346291  [19264/60000]\n",
      "loss: 0.276771  [25664/60000]\n",
      "loss: 0.300251  [32064/60000]\n",
      "loss: 0.259499  [38464/60000]\n",
      "loss: 0.367712  [44864/60000]\n",
      "loss: 0.380017  [51264/60000]\n",
      "loss: 0.360090  [57664/60000]\n",
      "Average training loss: 0.2965753007211538\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.290212 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.358925 \n",
      "\n",
      "Change in average loss: 0.004894455239526252\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.201154  [   64/60000]\n",
      "loss: 0.316938  [ 6464/60000]\n",
      "loss: 0.189634  [12864/60000]\n",
      "loss: 0.338142  [19264/60000]\n",
      "loss: 0.271966  [25664/60000]\n",
      "loss: 0.296454  [32064/60000]\n",
      "loss: 0.255487  [38464/60000]\n",
      "loss: 0.360390  [44864/60000]\n",
      "loss: 0.371967  [51264/60000]\n",
      "loss: 0.356457  [57664/60000]\n",
      "Average training loss: 0.2919312718707615\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.285645 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.356738 \n",
      "\n",
      "Change in average loss: 0.004644028850392279\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.200289  [   64/60000]\n",
      "loss: 0.313992  [ 6464/60000]\n",
      "loss: 0.189216  [12864/60000]\n",
      "loss: 0.330956  [19264/60000]\n",
      "loss: 0.269727  [25664/60000]\n",
      "loss: 0.292681  [32064/60000]\n",
      "loss: 0.252847  [38464/60000]\n",
      "loss: 0.353854  [44864/60000]\n",
      "loss: 0.363172  [51264/60000]\n",
      "loss: 0.352244  [57664/60000]\n",
      "Average training loss: 0.2874699831644355\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.281306 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.354684 \n",
      "\n",
      "Change in average loss: 0.004461288706326005\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.198901  [   64/60000]\n",
      "loss: 0.312946  [ 6464/60000]\n",
      "loss: 0.187463  [12864/60000]\n",
      "loss: 0.324332  [19264/60000]\n",
      "loss: 0.266290  [25664/60000]\n",
      "loss: 0.289190  [32064/60000]\n",
      "loss: 0.249331  [38464/60000]\n",
      "loss: 0.349377  [44864/60000]\n",
      "loss: 0.354981  [51264/60000]\n",
      "loss: 0.350405  [57664/60000]\n",
      "Average training loss: 0.2831807793425853\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.277192 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.352831 \n",
      "\n",
      "Change in average loss: 0.004289203821850218\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.197524  [   64/60000]\n",
      "loss: 0.311262  [ 6464/60000]\n",
      "loss: 0.186116  [12864/60000]\n",
      "loss: 0.318351  [19264/60000]\n",
      "loss: 0.263530  [25664/60000]\n",
      "loss: 0.285907  [32064/60000]\n",
      "loss: 0.246117  [38464/60000]\n",
      "loss: 0.343995  [44864/60000]\n",
      "loss: 0.346889  [51264/60000]\n",
      "loss: 0.346012  [57664/60000]\n",
      "Average training loss: 0.2790865460470287\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.273234 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.351251 \n",
      "\n",
      "Change in average loss: 0.004094233295556582\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.197525  [   64/60000]\n",
      "loss: 0.308371  [ 6464/60000]\n",
      "loss: 0.185468  [12864/60000]\n",
      "loss: 0.314048  [19264/60000]\n",
      "loss: 0.261548  [25664/60000]\n",
      "loss: 0.282571  [32064/60000]\n",
      "loss: 0.243650  [38464/60000]\n",
      "loss: 0.338487  [44864/60000]\n",
      "loss: 0.338544  [51264/60000]\n",
      "loss: 0.342631  [57664/60000]\n",
      "Average training loss: 0.2751678804884841\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.269282 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.349584 \n",
      "\n",
      "Change in average loss: 0.003918665558544587\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.196672  [   64/60000]\n",
      "loss: 0.306281  [ 6464/60000]\n",
      "loss: 0.184155  [12864/60000]\n",
      "loss: 0.309161  [19264/60000]\n",
      "loss: 0.259110  [25664/60000]\n",
      "loss: 0.279095  [32064/60000]\n",
      "loss: 0.240323  [38464/60000]\n",
      "loss: 0.333202  [44864/60000]\n",
      "loss: 0.329932  [51264/60000]\n",
      "loss: 0.340336  [57664/60000]\n",
      "Average training loss: 0.27141058773024757\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.265620 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.348345 \n",
      "\n",
      "Change in average loss: 0.0037572927582365523\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.195353  [   64/60000]\n",
      "loss: 0.304133  [ 6464/60000]\n",
      "loss: 0.183803  [12864/60000]\n",
      "loss: 0.303676  [19264/60000]\n",
      "loss: 0.257114  [25664/60000]\n",
      "loss: 0.275503  [32064/60000]\n",
      "loss: 0.237097  [38464/60000]\n",
      "loss: 0.328678  [44864/60000]\n",
      "loss: 0.323061  [51264/60000]\n",
      "loss: 0.338432  [57664/60000]\n",
      "Average training loss: 0.2677586812899311\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.262018 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.347184 \n",
      "\n",
      "Change in average loss: 0.003651906440316488\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.193243  [   64/60000]\n",
      "loss: 0.302691  [ 6464/60000]\n",
      "loss: 0.181193  [12864/60000]\n",
      "loss: 0.299084  [19264/60000]\n",
      "loss: 0.253394  [25664/60000]\n",
      "loss: 0.273406  [32064/60000]\n",
      "loss: 0.233839  [38464/60000]\n",
      "loss: 0.323475  [44864/60000]\n",
      "loss: 0.314333  [51264/60000]\n",
      "loss: 0.335695  [57664/60000]\n",
      "Average training loss: 0.26422810108105005\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.258498 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.345962 \n",
      "\n",
      "Change in average loss: 0.003530580208881029\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.191138  [   64/60000]\n",
      "loss: 0.300664  [ 6464/60000]\n",
      "loss: 0.179683  [12864/60000]\n",
      "loss: 0.295535  [19264/60000]\n",
      "loss: 0.250472  [25664/60000]\n",
      "loss: 0.270442  [32064/60000]\n",
      "loss: 0.231746  [38464/60000]\n",
      "loss: 0.317589  [44864/60000]\n",
      "loss: 0.308769  [51264/60000]\n",
      "loss: 0.334197  [57664/60000]\n",
      "Average training loss: 0.2608235094275302\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.255102 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.344950 \n",
      "\n",
      "Change in average loss: 0.0034045916535198617\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.189394  [   64/60000]\n",
      "loss: 0.299224  [ 6464/60000]\n",
      "loss: 0.178626  [12864/60000]\n",
      "loss: 0.293023  [19264/60000]\n",
      "loss: 0.247225  [25664/60000]\n",
      "loss: 0.267030  [32064/60000]\n",
      "loss: 0.229324  [38464/60000]\n",
      "loss: 0.311836  [44864/60000]\n",
      "loss: 0.302416  [51264/60000]\n",
      "loss: 0.330767  [57664/60000]\n",
      "Average training loss: 0.25749090329797536\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.251886 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.344261 \n",
      "\n",
      "Change in average loss: 0.003332606129554827\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.187566  [   64/60000]\n",
      "loss: 0.296462  [ 6464/60000]\n",
      "loss: 0.177986  [12864/60000]\n",
      "loss: 0.290077  [19264/60000]\n",
      "loss: 0.245478  [25664/60000]\n",
      "loss: 0.265046  [32064/60000]\n",
      "loss: 0.226831  [38464/60000]\n",
      "loss: 0.307987  [44864/60000]\n",
      "loss: 0.298583  [51264/60000]\n",
      "loss: 0.328887  [57664/60000]\n",
      "Average training loss: 0.25427448280108\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.248708 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.343574 \n",
      "\n",
      "Change in average loss: 0.003216420496895356\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.187703  [   64/60000]\n",
      "loss: 0.297172  [ 6464/60000]\n",
      "loss: 0.174602  [12864/60000]\n",
      "loss: 0.286794  [19264/60000]\n",
      "loss: 0.243262  [25664/60000]\n",
      "loss: 0.262858  [32064/60000]\n",
      "loss: 0.225952  [38464/60000]\n",
      "loss: 0.303762  [44864/60000]\n",
      "loss: 0.292915  [51264/60000]\n",
      "loss: 0.326430  [57664/60000]\n",
      "Average training loss: 0.2511391950759298\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.245688 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.343143 \n",
      "\n",
      "Change in average loss: 0.0031352877251502043\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.185962  [   64/60000]\n",
      "loss: 0.295351  [ 6464/60000]\n",
      "loss: 0.173310  [12864/60000]\n",
      "loss: 0.283626  [19264/60000]\n",
      "loss: 0.241646  [25664/60000]\n",
      "loss: 0.260715  [32064/60000]\n",
      "loss: 0.222907  [38464/60000]\n",
      "loss: 0.301704  [44864/60000]\n",
      "loss: 0.287293  [51264/60000]\n",
      "loss: 0.322585  [57664/60000]\n",
      "Average training loss: 0.2480767130422821\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.242616 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.342868 \n",
      "\n",
      "Change in average loss: 0.003062482033647701\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.183725  [   64/60000]\n",
      "loss: 0.293139  [ 6464/60000]\n",
      "loss: 0.172595  [12864/60000]\n",
      "loss: 0.280578  [19264/60000]\n",
      "loss: 0.239299  [25664/60000]\n",
      "loss: 0.259339  [32064/60000]\n",
      "loss: 0.220912  [38464/60000]\n",
      "loss: 0.296778  [44864/60000]\n",
      "loss: 0.283423  [51264/60000]\n",
      "loss: 0.320192  [57664/60000]\n",
      "Average training loss: 0.2450984303217961\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.239677 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.342249 \n",
      "\n",
      "Change in average loss: 0.002978282720486003\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.182313  [   64/60000]\n",
      "loss: 0.291198  [ 6464/60000]\n",
      "loss: 0.171191  [12864/60000]\n",
      "loss: 0.275545  [19264/60000]\n",
      "loss: 0.238106  [25664/60000]\n",
      "loss: 0.256167  [32064/60000]\n",
      "loss: 0.218526  [38464/60000]\n",
      "loss: 0.293212  [44864/60000]\n",
      "loss: 0.278825  [51264/60000]\n",
      "loss: 0.316920  [57664/60000]\n",
      "Average training loss: 0.2421948410141697\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.236785 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.341809 \n",
      "\n",
      "Change in average loss: 0.0029035893076264085\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.181736  [   64/60000]\n",
      "loss: 0.288775  [ 6464/60000]\n",
      "loss: 0.170530  [12864/60000]\n",
      "loss: 0.271651  [19264/60000]\n",
      "loss: 0.236950  [25664/60000]\n",
      "loss: 0.253480  [32064/60000]\n",
      "loss: 0.215645  [38464/60000]\n",
      "loss: 0.289488  [44864/60000]\n",
      "loss: 0.275024  [51264/60000]\n",
      "loss: 0.314274  [57664/60000]\n",
      "Average training loss: 0.23934640870419646\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.234029 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.341620 \n",
      "\n",
      "Change in average loss: 0.0028484323099732256\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.180598  [   64/60000]\n",
      "loss: 0.286249  [ 6464/60000]\n",
      "loss: 0.169753  [12864/60000]\n",
      "loss: 0.266440  [19264/60000]\n",
      "loss: 0.234598  [25664/60000]\n",
      "loss: 0.250064  [32064/60000]\n",
      "loss: 0.213237  [38464/60000]\n",
      "loss: 0.287049  [44864/60000]\n",
      "loss: 0.270858  [51264/60000]\n",
      "loss: 0.312728  [57664/60000]\n",
      "Average training loss: 0.23659317791144222\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.231236 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.341334 \n",
      "\n",
      "Change in average loss: 0.0027532307927542465\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.178632  [   64/60000]\n",
      "loss: 0.284920  [ 6464/60000]\n",
      "loss: 0.168955  [12864/60000]\n",
      "loss: 0.261979  [19264/60000]\n",
      "loss: 0.233383  [25664/60000]\n",
      "loss: 0.248733  [32064/60000]\n",
      "loss: 0.210390  [38464/60000]\n",
      "loss: 0.283862  [44864/60000]\n",
      "loss: 0.267976  [51264/60000]\n",
      "loss: 0.309490  [57664/60000]\n",
      "Average training loss: 0.233921657405746\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.228647 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.341092 \n",
      "\n",
      "Change in average loss: 0.0026715205056962177\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.177246  [   64/60000]\n",
      "loss: 0.283014  [ 6464/60000]\n",
      "loss: 0.168618  [12864/60000]\n",
      "loss: 0.258251  [19264/60000]\n",
      "loss: 0.230646  [25664/60000]\n",
      "loss: 0.245911  [32064/60000]\n",
      "loss: 0.208504  [38464/60000]\n",
      "loss: 0.280541  [44864/60000]\n",
      "loss: 0.264423  [51264/60000]\n",
      "loss: 0.307857  [57664/60000]\n",
      "Average training loss: 0.23130535841909552\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.226006 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.341005 \n",
      "\n",
      "Change in average loss: 0.0026162989866504727\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.174806  [   64/60000]\n",
      "loss: 0.281521  [ 6464/60000]\n",
      "loss: 0.167660  [12864/60000]\n",
      "loss: 0.253622  [19264/60000]\n",
      "loss: 0.229511  [25664/60000]\n",
      "loss: 0.244691  [32064/60000]\n",
      "loss: 0.207417  [38464/60000]\n",
      "loss: 0.276626  [44864/60000]\n",
      "loss: 0.261088  [51264/60000]\n",
      "loss: 0.305111  [57664/60000]\n",
      "Average training loss: 0.2287015277248964\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.223485 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.340802 \n",
      "\n",
      "Change in average loss: 0.0026038306941991163\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.173059  [   64/60000]\n",
      "loss: 0.280416  [ 6464/60000]\n",
      "loss: 0.166409  [12864/60000]\n",
      "loss: 0.250197  [19264/60000]\n",
      "loss: 0.226818  [25664/60000]\n",
      "loss: 0.242314  [32064/60000]\n",
      "loss: 0.205221  [38464/60000]\n",
      "loss: 0.273122  [44864/60000]\n",
      "loss: 0.257818  [51264/60000]\n",
      "loss: 0.302298  [57664/60000]\n",
      "Average training loss: 0.22620251764461938\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.221088 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.340875 \n",
      "\n",
      "Change in average loss: 0.002499010080277031\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.172940  [   64/60000]\n",
      "loss: 0.278228  [ 6464/60000]\n",
      "loss: 0.166534  [12864/60000]\n",
      "loss: 0.247951  [19264/60000]\n",
      "loss: 0.225162  [25664/60000]\n",
      "loss: 0.239891  [32064/60000]\n",
      "loss: 0.203445  [38464/60000]\n",
      "loss: 0.269173  [44864/60000]\n",
      "loss: 0.254513  [51264/60000]\n",
      "loss: 0.300226  [57664/60000]\n",
      "Average training loss: 0.2237722639288348\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.218690 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.340811 \n",
      "\n",
      "Change in average loss: 0.002430253715784564\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.170535  [   64/60000]\n",
      "loss: 0.274895  [ 6464/60000]\n",
      "loss: 0.165386  [12864/60000]\n",
      "loss: 0.244998  [19264/60000]\n",
      "loss: 0.223989  [25664/60000]\n",
      "loss: 0.237758  [32064/60000]\n",
      "loss: 0.201867  [38464/60000]\n",
      "loss: 0.266443  [44864/60000]\n",
      "loss: 0.252295  [51264/60000]\n",
      "loss: 0.298556  [57664/60000]\n",
      "Average training loss: 0.2213594873529126\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.216351 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.340993 \n",
      "\n",
      "Change in average loss: 0.002412776575922204\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.169422  [   64/60000]\n",
      "loss: 0.273671  [ 6464/60000]\n",
      "loss: 0.164820  [12864/60000]\n",
      "loss: 0.241320  [19264/60000]\n",
      "loss: 0.221768  [25664/60000]\n",
      "loss: 0.234836  [32064/60000]\n",
      "loss: 0.200770  [38464/60000]\n",
      "loss: 0.263887  [44864/60000]\n",
      "loss: 0.249305  [51264/60000]\n",
      "loss: 0.296288  [57664/60000]\n",
      "Average training loss: 0.21900427423274593\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.214074 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.341010 \n",
      "\n",
      "Change in average loss: 0.002355213120166677\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.169422  [   64/60000]\n",
      "loss: 0.272096  [ 6464/60000]\n",
      "loss: 0.163954  [12864/60000]\n",
      "loss: 0.239639  [19264/60000]\n",
      "loss: 0.221067  [25664/60000]\n",
      "loss: 0.232187  [32064/60000]\n",
      "loss: 0.199396  [38464/60000]\n",
      "loss: 0.261117  [44864/60000]\n",
      "loss: 0.247143  [51264/60000]\n",
      "loss: 0.295302  [57664/60000]\n",
      "Average training loss: 0.21674534602205892\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.211971 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.341205 \n",
      "\n",
      "Change in average loss: 0.0022589282106870168\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.167291  [   64/60000]\n",
      "loss: 0.268081  [ 6464/60000]\n",
      "loss: 0.163799  [12864/60000]\n",
      "loss: 0.236297  [19264/60000]\n",
      "loss: 0.218556  [25664/60000]\n",
      "loss: 0.232363  [32064/60000]\n",
      "loss: 0.197984  [38464/60000]\n",
      "loss: 0.256292  [44864/60000]\n",
      "loss: 0.245023  [51264/60000]\n",
      "loss: 0.292327  [57664/60000]\n",
      "Average training loss: 0.21451325052932127\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.209708 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.341382 \n",
      "\n",
      "Change in average loss: 0.0022320954927376457\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.166262  [   64/60000]\n",
      "loss: 0.266538  [ 6464/60000]\n",
      "loss: 0.163522  [12864/60000]\n",
      "loss: 0.234028  [19264/60000]\n",
      "loss: 0.216567  [25664/60000]\n",
      "loss: 0.231240  [32064/60000]\n",
      "loss: 0.196161  [38464/60000]\n",
      "loss: 0.255369  [44864/60000]\n",
      "loss: 0.241560  [51264/60000]\n",
      "loss: 0.290572  [57664/60000]\n",
      "Average training loss: 0.21228384759539226\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.207448 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.341274 \n",
      "\n",
      "Change in average loss: 0.002229402933929009\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.164132  [   64/60000]\n",
      "loss: 0.263729  [ 6464/60000]\n",
      "loss: 0.161818  [12864/60000]\n",
      "loss: 0.230260  [19264/60000]\n",
      "loss: 0.216288  [25664/60000]\n",
      "loss: 0.229073  [32064/60000]\n",
      "loss: 0.195117  [38464/60000]\n",
      "loss: 0.252831  [44864/60000]\n",
      "loss: 0.241478  [51264/60000]\n",
      "loss: 0.288208  [57664/60000]\n",
      "Average training loss: 0.21016598780399193\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.205436 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.341838 \n",
      "\n",
      "Change in average loss: 0.002117859791400334\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.161662  [   64/60000]\n",
      "loss: 0.261143  [ 6464/60000]\n",
      "loss: 0.162787  [12864/60000]\n",
      "loss: 0.227352  [19264/60000]\n",
      "loss: 0.214259  [25664/60000]\n",
      "loss: 0.226865  [32064/60000]\n",
      "loss: 0.193781  [38464/60000]\n",
      "loss: 0.249570  [44864/60000]\n",
      "loss: 0.238763  [51264/60000]\n",
      "loss: 0.286070  [57664/60000]\n",
      "Average training loss: 0.2080212208841528\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.203393 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.342067 \n",
      "\n",
      "Change in average loss: 0.002144766919839114\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.158880  [   64/60000]\n",
      "loss: 0.257692  [ 6464/60000]\n",
      "loss: 0.161866  [12864/60000]\n",
      "loss: 0.225196  [19264/60000]\n",
      "loss: 0.213339  [25664/60000]\n",
      "loss: 0.225660  [32064/60000]\n",
      "loss: 0.194113  [38464/60000]\n",
      "loss: 0.247234  [44864/60000]\n",
      "loss: 0.236198  [51264/60000]\n",
      "loss: 0.283062  [57664/60000]\n",
      "Average training loss: 0.20591583241150577\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.201463 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.342182 \n",
      "\n",
      "Change in average loss: 0.002105388472647046\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.156184  [   64/60000]\n",
      "loss: 0.254145  [ 6464/60000]\n",
      "loss: 0.161192  [12864/60000]\n",
      "loss: 0.223009  [19264/60000]\n",
      "loss: 0.211679  [25664/60000]\n",
      "loss: 0.223821  [32064/60000]\n",
      "loss: 0.193241  [38464/60000]\n",
      "loss: 0.243386  [44864/60000]\n",
      "loss: 0.234604  [51264/60000]\n",
      "loss: 0.280840  [57664/60000]\n",
      "Average training loss: 0.20393205331618597\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.199481 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.342544 \n",
      "\n",
      "Change in average loss: 0.0019837790953197987\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.153618  [   64/60000]\n",
      "loss: 0.252014  [ 6464/60000]\n",
      "loss: 0.161071  [12864/60000]\n",
      "loss: 0.221060  [19264/60000]\n",
      "loss: 0.211053  [25664/60000]\n",
      "loss: 0.222555  [32064/60000]\n",
      "loss: 0.191919  [38464/60000]\n",
      "loss: 0.240845  [44864/60000]\n",
      "loss: 0.232939  [51264/60000]\n",
      "loss: 0.276629  [57664/60000]\n",
      "Average training loss: 0.2018749141600976\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.197506 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.342671 \n",
      "\n",
      "Change in average loss: 0.002057139156088378\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.151576  [   64/60000]\n",
      "loss: 0.248338  [ 6464/60000]\n",
      "loss: 0.161531  [12864/60000]\n",
      "loss: 0.218473  [19264/60000]\n",
      "loss: 0.209057  [25664/60000]\n",
      "loss: 0.221789  [32064/60000]\n",
      "loss: 0.191049  [38464/60000]\n",
      "loss: 0.239496  [44864/60000]\n",
      "loss: 0.230294  [51264/60000]\n",
      "loss: 0.274350  [57664/60000]\n",
      "Average training loss: 0.19993505463290062\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.195618 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.343188 \n",
      "\n",
      "Change in average loss: 0.0019398595271969687\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.150286  [   64/60000]\n",
      "loss: 0.245287  [ 6464/60000]\n",
      "loss: 0.160386  [12864/60000]\n",
      "loss: 0.216844  [19264/60000]\n",
      "loss: 0.206945  [25664/60000]\n",
      "loss: 0.221075  [32064/60000]\n",
      "loss: 0.189928  [38464/60000]\n",
      "loss: 0.235211  [44864/60000]\n",
      "loss: 0.227806  [51264/60000]\n",
      "loss: 0.270810  [57664/60000]\n",
      "Average training loss: 0.19794629789046894\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.193773 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.343430 \n",
      "\n",
      "Change in average loss: 0.00198875674243168\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.147399  [   64/60000]\n",
      "loss: 0.244575  [ 6464/60000]\n",
      "loss: 0.160098  [12864/60000]\n",
      "loss: 0.214596  [19264/60000]\n",
      "loss: 0.206066  [25664/60000]\n",
      "loss: 0.221588  [32064/60000]\n",
      "loss: 0.189638  [38464/60000]\n",
      "loss: 0.232975  [44864/60000]\n",
      "loss: 0.226725  [51264/60000]\n",
      "loss: 0.269095  [57664/60000]\n",
      "Average training loss: 0.19601480013875563\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.192026 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.343864 \n",
      "\n",
      "Change in average loss: 0.001931497751713307\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.146133  [   64/60000]\n",
      "loss: 0.240999  [ 6464/60000]\n",
      "loss: 0.158717  [12864/60000]\n",
      "loss: 0.212078  [19264/60000]\n",
      "loss: 0.204668  [25664/60000]\n",
      "loss: 0.220369  [32064/60000]\n",
      "loss: 0.189553  [38464/60000]\n",
      "loss: 0.228800  [44864/60000]\n",
      "loss: 0.224564  [51264/60000]\n",
      "loss: 0.267062  [57664/60000]\n",
      "Average training loss: 0.19415121133933697\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.190113 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.344250 \n",
      "\n",
      "Change in average loss: 0.0018635887994186695\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.144234  [   64/60000]\n",
      "loss: 0.237696  [ 6464/60000]\n",
      "loss: 0.158357  [12864/60000]\n",
      "loss: 0.209427  [19264/60000]\n",
      "loss: 0.203004  [25664/60000]\n",
      "loss: 0.220025  [32064/60000]\n",
      "loss: 0.187343  [38464/60000]\n",
      "loss: 0.226940  [44864/60000]\n",
      "loss: 0.223813  [51264/60000]\n",
      "loss: 0.263843  [57664/60000]\n",
      "Average training loss: 0.19224898938907745\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.188400 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.344889 \n",
      "\n",
      "Change in average loss: 0.0019022219502595195\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.142144  [   64/60000]\n",
      "loss: 0.234818  [ 6464/60000]\n",
      "loss: 0.158241  [12864/60000]\n",
      "loss: 0.208086  [19264/60000]\n",
      "loss: 0.201338  [25664/60000]\n",
      "loss: 0.219031  [32064/60000]\n",
      "loss: 0.186698  [38464/60000]\n",
      "loss: 0.225794  [44864/60000]\n",
      "loss: 0.221733  [51264/60000]\n",
      "loss: 0.260968  [57664/60000]\n",
      "Average training loss: 0.19038330275478013\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.186622 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.345320 \n",
      "\n",
      "Change in average loss: 0.0018656866342973144\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.139927  [   64/60000]\n",
      "loss: 0.233124  [ 6464/60000]\n",
      "loss: 0.158263  [12864/60000]\n",
      "loss: 0.206524  [19264/60000]\n",
      "loss: 0.200251  [25664/60000]\n",
      "loss: 0.217421  [32064/60000]\n",
      "loss: 0.184148  [38464/60000]\n",
      "loss: 0.224263  [44864/60000]\n",
      "loss: 0.221519  [51264/60000]\n",
      "loss: 0.257049  [57664/60000]\n",
      "Average training loss: 0.1885669300837049\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.184928 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.345796 \n",
      "\n",
      "Change in average loss: 0.0018163726710752337\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.137631  [   64/60000]\n",
      "loss: 0.229794  [ 6464/60000]\n",
      "loss: 0.157311  [12864/60000]\n",
      "loss: 0.203945  [19264/60000]\n",
      "loss: 0.198324  [25664/60000]\n",
      "loss: 0.215420  [32064/60000]\n",
      "loss: 0.183665  [38464/60000]\n",
      "loss: 0.222167  [44864/60000]\n",
      "loss: 0.220792  [51264/60000]\n",
      "loss: 0.255106  [57664/60000]\n",
      "Average training loss: 0.18679374396038462\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.183322 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.346125 \n",
      "\n",
      "Change in average loss: 0.0017731861233202806\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.135901  [   64/60000]\n",
      "loss: 0.227670  [ 6464/60000]\n",
      "loss: 0.156557  [12864/60000]\n",
      "loss: 0.202628  [19264/60000]\n",
      "loss: 0.197406  [25664/60000]\n",
      "loss: 0.214563  [32064/60000]\n",
      "loss: 0.182360  [38464/60000]\n",
      "loss: 0.217916  [44864/60000]\n",
      "loss: 0.218883  [51264/60000]\n",
      "loss: 0.252349  [57664/60000]\n",
      "Average training loss: 0.18503399625388797\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.181670 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.346758 \n",
      "\n",
      "Change in average loss: 0.001759747706496645\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.134292  [   64/60000]\n",
      "loss: 0.223671  [ 6464/60000]\n",
      "loss: 0.156027  [12864/60000]\n",
      "loss: 0.200915  [19264/60000]\n",
      "loss: 0.195482  [25664/60000]\n",
      "loss: 0.213614  [32064/60000]\n",
      "loss: 0.178587  [38464/60000]\n",
      "loss: 0.217365  [44864/60000]\n",
      "loss: 0.216906  [51264/60000]\n",
      "loss: 0.249591  [57664/60000]\n",
      "Average training loss: 0.18328578074746676\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.179883 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.347019 \n",
      "\n",
      "Change in average loss: 0.001748215506421208\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.133215  [   64/60000]\n",
      "loss: 0.221742  [ 6464/60000]\n",
      "loss: 0.156622  [12864/60000]\n",
      "loss: 0.199999  [19264/60000]\n",
      "loss: 0.193914  [25664/60000]\n",
      "loss: 0.212114  [32064/60000]\n",
      "loss: 0.178310  [38464/60000]\n",
      "loss: 0.214508  [44864/60000]\n",
      "loss: 0.215527  [51264/60000]\n",
      "loss: 0.248330  [57664/60000]\n",
      "Average training loss: 0.18156814369073174\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.178329 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.347523 \n",
      "\n",
      "Change in average loss: 0.0017176370567350274\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.131015  [   64/60000]\n",
      "loss: 0.219363  [ 6464/60000]\n",
      "loss: 0.154292  [12864/60000]\n",
      "loss: 0.197989  [19264/60000]\n",
      "loss: 0.191564  [25664/60000]\n",
      "loss: 0.210821  [32064/60000]\n",
      "loss: 0.176351  [38464/60000]\n",
      "loss: 0.213298  [44864/60000]\n",
      "loss: 0.212382  [51264/60000]\n",
      "loss: 0.245654  [57664/60000]\n",
      "Average training loss: 0.17989974799774475\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.176630 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.347853 \n",
      "\n",
      "Change in average loss: 0.0016683956929869903\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.129747  [   64/60000]\n",
      "loss: 0.215946  [ 6464/60000]\n",
      "loss: 0.154371  [12864/60000]\n",
      "loss: 0.195429  [19264/60000]\n",
      "loss: 0.190631  [25664/60000]\n",
      "loss: 0.208482  [32064/60000]\n",
      "loss: 0.174559  [38464/60000]\n",
      "loss: 0.209507  [44864/60000]\n",
      "loss: 0.211566  [51264/60000]\n",
      "loss: 0.243947  [57664/60000]\n",
      "Average training loss: 0.1782097233487154\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.175164 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.348716 \n",
      "\n",
      "Change in average loss: 0.0016900246490293425\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.128282  [   64/60000]\n",
      "loss: 0.212861  [ 6464/60000]\n",
      "loss: 0.153924  [12864/60000]\n",
      "loss: 0.194994  [19264/60000]\n",
      "loss: 0.188560  [25664/60000]\n",
      "loss: 0.207305  [32064/60000]\n",
      "loss: 0.172457  [38464/60000]\n",
      "loss: 0.207098  [44864/60000]\n",
      "loss: 0.210863  [51264/60000]\n",
      "loss: 0.242527  [57664/60000]\n",
      "Average training loss: 0.17653980929015287\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.173574 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.349131 \n",
      "\n",
      "Change in average loss: 0.0016699140585625383\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.126922  [   64/60000]\n",
      "loss: 0.212467  [ 6464/60000]\n",
      "loss: 0.153439  [12864/60000]\n",
      "loss: 0.192150  [19264/60000]\n",
      "loss: 0.185730  [25664/60000]\n",
      "loss: 0.205491  [32064/60000]\n",
      "loss: 0.171417  [38464/60000]\n",
      "loss: 0.206663  [44864/60000]\n",
      "loss: 0.209703  [51264/60000]\n",
      "loss: 0.238616  [57664/60000]\n",
      "Average training loss: 0.17491869347642608\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.172073 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.349695 \n",
      "\n",
      "Change in average loss: 0.0016211158137267867\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.126111  [   64/60000]\n",
      "loss: 0.210334  [ 6464/60000]\n",
      "loss: 0.151452  [12864/60000]\n",
      "loss: 0.189925  [19264/60000]\n",
      "loss: 0.184212  [25664/60000]\n",
      "loss: 0.203523  [32064/60000]\n",
      "loss: 0.170488  [38464/60000]\n",
      "loss: 0.205128  [44864/60000]\n",
      "loss: 0.208610  [51264/60000]\n",
      "loss: 0.237620  [57664/60000]\n",
      "Average training loss: 0.1733045095621523\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.170743 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.350315 \n",
      "\n",
      "Change in average loss: 0.0016141839142737868\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.124225  [   64/60000]\n",
      "loss: 0.206507  [ 6464/60000]\n",
      "loss: 0.151338  [12864/60000]\n",
      "loss: 0.188538  [19264/60000]\n",
      "loss: 0.182657  [25664/60000]\n",
      "loss: 0.202248  [32064/60000]\n",
      "loss: 0.168796  [38464/60000]\n",
      "loss: 0.203133  [44864/60000]\n",
      "loss: 0.206033  [51264/60000]\n",
      "loss: 0.235718  [57664/60000]\n",
      "Average training loss: 0.17175285870443657\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.169220 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.350534 \n",
      "\n",
      "Change in average loss: 0.0015516508577157195\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.122077  [   64/60000]\n",
      "loss: 0.204180  [ 6464/60000]\n",
      "loss: 0.149885  [12864/60000]\n",
      "loss: 0.187658  [19264/60000]\n",
      "loss: 0.180089  [25664/60000]\n",
      "loss: 0.202148  [32064/60000]\n",
      "loss: 0.169046  [38464/60000]\n",
      "loss: 0.200714  [44864/60000]\n",
      "loss: 0.206097  [51264/60000]\n",
      "loss: 0.231100  [57664/60000]\n",
      "Average training loss: 0.17014899663229996\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.167638 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.351227 \n",
      "\n",
      "Change in average loss: 0.0016038620721366137\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.122368  [   64/60000]\n",
      "loss: 0.201286  [ 6464/60000]\n",
      "loss: 0.148236  [12864/60000]\n",
      "loss: 0.185403  [19264/60000]\n",
      "loss: 0.178027  [25664/60000]\n",
      "loss: 0.199433  [32064/60000]\n",
      "loss: 0.167395  [38464/60000]\n",
      "loss: 0.199223  [44864/60000]\n",
      "loss: 0.205660  [51264/60000]\n",
      "loss: 0.229374  [57664/60000]\n",
      "Average training loss: 0.16862188686312898\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.166367 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.351751 \n",
      "\n",
      "Change in average loss: 0.0015271097691709756\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.120112  [   64/60000]\n",
      "loss: 0.198603  [ 6464/60000]\n",
      "loss: 0.146817  [12864/60000]\n",
      "loss: 0.182765  [19264/60000]\n",
      "loss: 0.175788  [25664/60000]\n",
      "loss: 0.198394  [32064/60000]\n",
      "loss: 0.164871  [38464/60000]\n",
      "loss: 0.198466  [44864/60000]\n",
      "loss: 0.204451  [51264/60000]\n",
      "loss: 0.226279  [57664/60000]\n",
      "Average training loss: 0.16707256077322116\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.164962 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.352449 \n",
      "\n",
      "Change in average loss: 0.001549326089907821\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.119611  [   64/60000]\n",
      "loss: 0.199280  [ 6464/60000]\n",
      "loss: 0.146627  [12864/60000]\n",
      "loss: 0.179985  [19264/60000]\n",
      "loss: 0.174350  [25664/60000]\n",
      "loss: 0.196105  [32064/60000]\n",
      "loss: 0.162992  [38464/60000]\n",
      "loss: 0.197455  [44864/60000]\n",
      "loss: 0.202426  [51264/60000]\n",
      "loss: 0.224936  [57664/60000]\n",
      "Average training loss: 0.16559352931469234\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.163526 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.353048 \n",
      "\n",
      "Change in average loss: 0.0014790314585288178\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.118775  [   64/60000]\n",
      "loss: 0.195487  [ 6464/60000]\n",
      "loss: 0.146603  [12864/60000]\n",
      "loss: 0.178851  [19264/60000]\n",
      "loss: 0.172471  [25664/60000]\n",
      "loss: 0.196508  [32064/60000]\n",
      "loss: 0.163001  [38464/60000]\n",
      "loss: 0.195516  [44864/60000]\n",
      "loss: 0.201732  [51264/60000]\n",
      "loss: 0.223537  [57664/60000]\n",
      "Average training loss: 0.16409802912220137\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.162040 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.353613 \n",
      "\n",
      "Change in average loss: 0.0014955001924909783\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.117606  [   64/60000]\n",
      "loss: 0.193427  [ 6464/60000]\n",
      "loss: 0.146379  [12864/60000]\n",
      "loss: 0.178118  [19264/60000]\n",
      "loss: 0.170307  [25664/60000]\n",
      "loss: 0.194839  [32064/60000]\n",
      "loss: 0.160127  [38464/60000]\n",
      "loss: 0.194821  [44864/60000]\n",
      "loss: 0.200548  [51264/60000]\n",
      "loss: 0.220178  [57664/60000]\n",
      "Average training loss: 0.16260916468447079\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.160792 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.354537 \n",
      "\n",
      "Change in average loss: 0.0014888644377305804\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.115754  [   64/60000]\n",
      "loss: 0.191457  [ 6464/60000]\n",
      "loss: 0.144775  [12864/60000]\n",
      "loss: 0.176075  [19264/60000]\n",
      "loss: 0.168983  [25664/60000]\n",
      "loss: 0.193466  [32064/60000]\n",
      "loss: 0.158906  [38464/60000]\n",
      "loss: 0.192715  [44864/60000]\n",
      "loss: 0.199114  [51264/60000]\n",
      "loss: 0.217556  [57664/60000]\n",
      "Average training loss: 0.16119517239013206\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.159447 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.355253 \n",
      "\n",
      "Change in average loss: 0.0014139922943387229\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.114920  [   64/60000]\n",
      "loss: 0.188034  [ 6464/60000]\n",
      "loss: 0.143793  [12864/60000]\n",
      "loss: 0.174191  [19264/60000]\n",
      "loss: 0.166088  [25664/60000]\n",
      "loss: 0.193235  [32064/60000]\n",
      "loss: 0.157922  [38464/60000]\n",
      "loss: 0.190169  [44864/60000]\n",
      "loss: 0.197940  [51264/60000]\n",
      "loss: 0.215336  [57664/60000]\n",
      "Average training loss: 0.15975753488793557\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.158256 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.356325 \n",
      "\n",
      "Change in average loss: 0.0014376375021964982\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.114077  [   64/60000]\n",
      "loss: 0.184767  [ 6464/60000]\n",
      "loss: 0.143681  [12864/60000]\n",
      "loss: 0.172945  [19264/60000]\n",
      "loss: 0.164398  [25664/60000]\n",
      "loss: 0.192519  [32064/60000]\n",
      "loss: 0.156240  [38464/60000]\n",
      "loss: 0.188440  [44864/60000]\n",
      "loss: 0.195807  [51264/60000]\n",
      "loss: 0.213582  [57664/60000]\n",
      "Average training loss: 0.158325938662804\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.156808 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.356987 \n",
      "\n",
      "Change in average loss: 0.0014315962251315595\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.112310  [   64/60000]\n",
      "loss: 0.182894  [ 6464/60000]\n",
      "loss: 0.142768  [12864/60000]\n",
      "loss: 0.171384  [19264/60000]\n",
      "loss: 0.163539  [25664/60000]\n",
      "loss: 0.193205  [32064/60000]\n",
      "loss: 0.155068  [38464/60000]\n",
      "loss: 0.187550  [44864/60000]\n",
      "loss: 0.194426  [51264/60000]\n",
      "loss: 0.211287  [57664/60000]\n",
      "Average training loss: 0.15693549201813842\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.155556 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.357954 \n",
      "\n",
      "Change in average loss: 0.0013904466446655883\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.111347  [   64/60000]\n",
      "loss: 0.180631  [ 6464/60000]\n",
      "loss: 0.142182  [12864/60000]\n",
      "loss: 0.171110  [19264/60000]\n",
      "loss: 0.161347  [25664/60000]\n",
      "loss: 0.191376  [32064/60000]\n",
      "loss: 0.154341  [38464/60000]\n",
      "loss: 0.185488  [44864/60000]\n",
      "loss: 0.194709  [51264/60000]\n",
      "loss: 0.208435  [57664/60000]\n",
      "Average training loss: 0.1555087268273078\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.154383 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.359045 \n",
      "\n",
      "Change in average loss: 0.0014267651908306256\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.109989  [   64/60000]\n",
      "loss: 0.178259  [ 6464/60000]\n",
      "loss: 0.140167  [12864/60000]\n",
      "loss: 0.168565  [19264/60000]\n",
      "loss: 0.157440  [25664/60000]\n",
      "loss: 0.190519  [32064/60000]\n",
      "loss: 0.152659  [38464/60000]\n",
      "loss: 0.183492  [44864/60000]\n",
      "loss: 0.192282  [51264/60000]\n",
      "loss: 0.205625  [57664/60000]\n",
      "Average training loss: 0.1541605696661958\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.153077 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.359675 \n",
      "\n",
      "Change in average loss: 0.0013481571611120013\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.109074  [   64/60000]\n",
      "loss: 0.175610  [ 6464/60000]\n",
      "loss: 0.139394  [12864/60000]\n",
      "loss: 0.166863  [19264/60000]\n",
      "loss: 0.155709  [25664/60000]\n",
      "loss: 0.190862  [32064/60000]\n",
      "loss: 0.151293  [38464/60000]\n",
      "loss: 0.181575  [44864/60000]\n",
      "loss: 0.190811  [51264/60000]\n",
      "loss: 0.202868  [57664/60000]\n",
      "Average training loss: 0.1528076714575926\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.152024 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.360803 \n",
      "\n",
      "Change in average loss: 0.0013528982086031816\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.108230  [   64/60000]\n",
      "loss: 0.173400  [ 6464/60000]\n",
      "loss: 0.138045  [12864/60000]\n",
      "loss: 0.165611  [19264/60000]\n",
      "loss: 0.154624  [25664/60000]\n",
      "loss: 0.190460  [32064/60000]\n",
      "loss: 0.150859  [38464/60000]\n",
      "loss: 0.179208  [44864/60000]\n",
      "loss: 0.189225  [51264/60000]\n",
      "loss: 0.200152  [57664/60000]\n",
      "Average training loss: 0.15150658867713104\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.150838 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.361506 \n",
      "\n",
      "Change in average loss: 0.0013010827804615654\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.106608  [   64/60000]\n",
      "loss: 0.172993  [ 6464/60000]\n",
      "loss: 0.137204  [12864/60000]\n",
      "loss: 0.163361  [19264/60000]\n",
      "loss: 0.152605  [25664/60000]\n",
      "loss: 0.189228  [32064/60000]\n",
      "loss: 0.148122  [38464/60000]\n",
      "loss: 0.178073  [44864/60000]\n",
      "loss: 0.187451  [51264/60000]\n",
      "loss: 0.197743  [57664/60000]\n",
      "Average training loss: 0.15016706917347558\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.149676 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.362207 \n",
      "\n",
      "Change in average loss: 0.001339519503655462\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.105369  [   64/60000]\n",
      "loss: 0.168977  [ 6464/60000]\n",
      "loss: 0.137224  [12864/60000]\n",
      "loss: 0.162560  [19264/60000]\n",
      "loss: 0.150987  [25664/60000]\n",
      "loss: 0.188519  [32064/60000]\n",
      "loss: 0.147809  [38464/60000]\n",
      "loss: 0.177276  [44864/60000]\n",
      "loss: 0.187250  [51264/60000]\n",
      "loss: 0.196323  [57664/60000]\n",
      "Average training loss: 0.14887874786740046\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.148585 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.362982 \n",
      "\n",
      "Change in average loss: 0.0012883213060751186\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.104703  [   64/60000]\n",
      "loss: 0.167906  [ 6464/60000]\n",
      "loss: 0.136767  [12864/60000]\n",
      "loss: 0.162088  [19264/60000]\n",
      "loss: 0.150147  [25664/60000]\n",
      "loss: 0.188220  [32064/60000]\n",
      "loss: 0.145839  [38464/60000]\n",
      "loss: 0.175468  [44864/60000]\n",
      "loss: 0.185173  [51264/60000]\n",
      "loss: 0.193699  [57664/60000]\n",
      "Average training loss: 0.14759262093602976\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.147367 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.363591 \n",
      "\n",
      "Change in average loss: 0.0012861269313707013\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.102400  [   64/60000]\n",
      "loss: 0.165948  [ 6464/60000]\n",
      "loss: 0.134840  [12864/60000]\n",
      "loss: 0.160328  [19264/60000]\n",
      "loss: 0.146316  [25664/60000]\n",
      "loss: 0.188388  [32064/60000]\n",
      "loss: 0.144666  [38464/60000]\n",
      "loss: 0.174710  [44864/60000]\n",
      "loss: 0.184648  [51264/60000]\n",
      "loss: 0.191814  [57664/60000]\n",
      "Average training loss: 0.14632298522936638\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.146263 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.364585 \n",
      "\n",
      "Change in average loss: 0.0012696357066633857\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.101009  [   64/60000]\n",
      "loss: 0.164434  [ 6464/60000]\n",
      "loss: 0.133940  [12864/60000]\n",
      "loss: 0.159131  [19264/60000]\n",
      "loss: 0.145349  [25664/60000]\n",
      "loss: 0.188062  [32064/60000]\n",
      "loss: 0.144676  [38464/60000]\n",
      "loss: 0.171581  [44864/60000]\n",
      "loss: 0.183209  [51264/60000]\n",
      "loss: 0.189323  [57664/60000]\n",
      "Average training loss: 0.145110320331636\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.145039 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.365335 \n",
      "\n",
      "Change in average loss: 0.0012126648977303645\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.100965  [   64/60000]\n",
      "loss: 0.161851  [ 6464/60000]\n",
      "loss: 0.133976  [12864/60000]\n",
      "loss: 0.157697  [19264/60000]\n",
      "loss: 0.143768  [25664/60000]\n",
      "loss: 0.187729  [32064/60000]\n",
      "loss: 0.142472  [38464/60000]\n",
      "loss: 0.170638  [44864/60000]\n",
      "loss: 0.182514  [51264/60000]\n",
      "loss: 0.188892  [57664/60000]\n",
      "Average training loss: 0.14388249902281045\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.144113 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.366779 \n",
      "\n",
      "Change in average loss: 0.0012278213088255663\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.100393  [   64/60000]\n",
      "loss: 0.160954  [ 6464/60000]\n",
      "loss: 0.133216  [12864/60000]\n",
      "loss: 0.156674  [19264/60000]\n",
      "loss: 0.141289  [25664/60000]\n",
      "loss: 0.187029  [32064/60000]\n",
      "loss: 0.141662  [38464/60000]\n",
      "loss: 0.169456  [44864/60000]\n",
      "loss: 0.180751  [51264/60000]\n",
      "loss: 0.187926  [57664/60000]\n",
      "Average training loss: 0.1426700936681998\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.142999 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.367418 \n",
      "\n",
      "Change in average loss: 0.0012124053546106461\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.099439  [   64/60000]\n",
      "loss: 0.159162  [ 6464/60000]\n",
      "loss: 0.132687  [12864/60000]\n",
      "loss: 0.155930  [19264/60000]\n",
      "loss: 0.140503  [25664/60000]\n",
      "loss: 0.185901  [32064/60000]\n",
      "loss: 0.140267  [38464/60000]\n",
      "loss: 0.167684  [44864/60000]\n",
      "loss: 0.179022  [51264/60000]\n",
      "loss: 0.186011  [57664/60000]\n",
      "Average training loss: 0.1414521304544991\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.141843 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.368313 \n",
      "\n",
      "Change in average loss: 0.0012179632137007124\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.098600  [   64/60000]\n",
      "loss: 0.158940  [ 6464/60000]\n",
      "loss: 0.132575  [12864/60000]\n",
      "loss: 0.154453  [19264/60000]\n",
      "loss: 0.139288  [25664/60000]\n",
      "loss: 0.186041  [32064/60000]\n",
      "loss: 0.138756  [38464/60000]\n",
      "loss: 0.165176  [44864/60000]\n",
      "loss: 0.177710  [51264/60000]\n",
      "loss: 0.183926  [57664/60000]\n",
      "Average training loss: 0.14029434960343437\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.140775 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.369363 \n",
      "\n",
      "Change in average loss: 0.0011577808510647158\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.097762  [   64/60000]\n",
      "loss: 0.155526  [ 6464/60000]\n",
      "loss: 0.131791  [12864/60000]\n",
      "loss: 0.152772  [19264/60000]\n",
      "loss: 0.136833  [25664/60000]\n",
      "loss: 0.184491  [32064/60000]\n",
      "loss: 0.136285  [38464/60000]\n",
      "loss: 0.163671  [44864/60000]\n",
      "loss: 0.176331  [51264/60000]\n",
      "loss: 0.181825  [57664/60000]\n",
      "Average training loss: 0.1390629245011982\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.139967 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.370090 \n",
      "\n",
      "Change in average loss: 0.0012314251022361722\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.096094  [   64/60000]\n",
      "loss: 0.154685  [ 6464/60000]\n",
      "loss: 0.131304  [12864/60000]\n",
      "loss: 0.151981  [19264/60000]\n",
      "loss: 0.135196  [25664/60000]\n",
      "loss: 0.184231  [32064/60000]\n",
      "loss: 0.135083  [38464/60000]\n",
      "loss: 0.161985  [44864/60000]\n",
      "loss: 0.175490  [51264/60000]\n",
      "loss: 0.182033  [57664/60000]\n",
      "Average training loss: 0.13794036298148285\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.138737 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.371414 \n",
      "\n",
      "Change in average loss: 0.0011225615197153482\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.095556  [   64/60000]\n",
      "loss: 0.152490  [ 6464/60000]\n",
      "loss: 0.130702  [12864/60000]\n",
      "loss: 0.150863  [19264/60000]\n",
      "loss: 0.132643  [25664/60000]\n",
      "loss: 0.182197  [32064/60000]\n",
      "loss: 0.134870  [38464/60000]\n",
      "loss: 0.159227  [44864/60000]\n",
      "loss: 0.175308  [51264/60000]\n",
      "loss: 0.180808  [57664/60000]\n",
      "Average training loss: 0.13679945244114283\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.137617 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.371945 \n",
      "\n",
      "Change in average loss: 0.0011409105403400177\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.094615  [   64/60000]\n",
      "loss: 0.150400  [ 6464/60000]\n",
      "loss: 0.129146  [12864/60000]\n",
      "loss: 0.150424  [19264/60000]\n",
      "loss: 0.131422  [25664/60000]\n",
      "loss: 0.182839  [32064/60000]\n",
      "loss: 0.133348  [38464/60000]\n",
      "loss: 0.158555  [44864/60000]\n",
      "loss: 0.173029  [51264/60000]\n",
      "loss: 0.181095  [57664/60000]\n",
      "Average training loss: 0.13563517452096507\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.136915 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.373269 \n",
      "\n",
      "Change in average loss: 0.0011642779201777587\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.094415  [   64/60000]\n",
      "loss: 0.150913  [ 6464/60000]\n",
      "loss: 0.128033  [12864/60000]\n",
      "loss: 0.148376  [19264/60000]\n",
      "loss: 0.130737  [25664/60000]\n",
      "loss: 0.182339  [32064/60000]\n",
      "loss: 0.131481  [38464/60000]\n",
      "loss: 0.156248  [44864/60000]\n",
      "loss: 0.172404  [51264/60000]\n",
      "loss: 0.175473  [57664/60000]\n",
      "Average training loss: 0.13457243911413622\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.135976 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.374219 \n",
      "\n",
      "Change in average loss: 0.0010627354068288575\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.093297  [   64/60000]\n",
      "loss: 0.148450  [ 6464/60000]\n",
      "loss: 0.127222  [12864/60000]\n",
      "loss: 0.147346  [19264/60000]\n",
      "loss: 0.128669  [25664/60000]\n",
      "loss: 0.181986  [32064/60000]\n",
      "loss: 0.130928  [38464/60000]\n",
      "loss: 0.155126  [44864/60000]\n",
      "loss: 0.169968  [51264/60000]\n",
      "loss: 0.174653  [57664/60000]\n",
      "Average training loss: 0.13347567222329346\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.134858 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.374983 \n",
      "\n",
      "Change in average loss: 0.0010967668908427597\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.092109  [   64/60000]\n",
      "loss: 0.147713  [ 6464/60000]\n",
      "loss: 0.127333  [12864/60000]\n",
      "loss: 0.146334  [19264/60000]\n",
      "loss: 0.126874  [25664/60000]\n",
      "loss: 0.179880  [32064/60000]\n",
      "loss: 0.129417  [38464/60000]\n",
      "loss: 0.153446  [44864/60000]\n",
      "loss: 0.170758  [51264/60000]\n",
      "loss: 0.174305  [57664/60000]\n",
      "Average training loss: 0.1323610770780204\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.134115 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.376020 \n",
      "\n",
      "Change in average loss: 0.0011145951452730674\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.092772  [   64/60000]\n",
      "loss: 0.146240  [ 6464/60000]\n",
      "loss: 0.125667  [12864/60000]\n",
      "loss: 0.144555  [19264/60000]\n",
      "loss: 0.125437  [25664/60000]\n",
      "loss: 0.181810  [32064/60000]\n",
      "loss: 0.127776  [38464/60000]\n",
      "loss: 0.152327  [44864/60000]\n",
      "loss: 0.169537  [51264/60000]\n",
      "loss: 0.171812  [57664/60000]\n",
      "Average training loss: 0.1312914379417642\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.133210 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.377029 \n",
      "\n",
      "Change in average loss: 0.0010696391362561841\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.090959  [   64/60000]\n",
      "loss: 0.145041  [ 6464/60000]\n",
      "loss: 0.124658  [12864/60000]\n",
      "loss: 0.144665  [19264/60000]\n",
      "loss: 0.124913  [25664/60000]\n",
      "loss: 0.180983  [32064/60000]\n",
      "loss: 0.126137  [38464/60000]\n",
      "loss: 0.152103  [44864/60000]\n",
      "loss: 0.167402  [51264/60000]\n",
      "loss: 0.169931  [57664/60000]\n",
      "Average training loss: 0.13024371934693252\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.132138 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.378248 \n",
      "\n",
      "Change in average loss: 0.001047718594831687\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.091178  [   64/60000]\n",
      "loss: 0.143165  [ 6464/60000]\n",
      "loss: 0.124586  [12864/60000]\n",
      "loss: 0.143264  [19264/60000]\n",
      "loss: 0.123537  [25664/60000]\n",
      "loss: 0.181304  [32064/60000]\n",
      "loss: 0.124448  [38464/60000]\n",
      "loss: 0.149690  [44864/60000]\n",
      "loss: 0.166495  [51264/60000]\n",
      "loss: 0.169396  [57664/60000]\n",
      "Average training loss: 0.12915984442683934\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.131273 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.379078 \n",
      "\n",
      "Change in average loss: 0.0010838749200931808\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.090271  [   64/60000]\n",
      "loss: 0.142796  [ 6464/60000]\n",
      "loss: 0.122702  [12864/60000]\n",
      "loss: 0.142545  [19264/60000]\n",
      "loss: 0.121893  [25664/60000]\n",
      "loss: 0.181220  [32064/60000]\n",
      "loss: 0.121361  [38464/60000]\n",
      "loss: 0.147319  [44864/60000]\n",
      "loss: 0.165144  [51264/60000]\n",
      "loss: 0.166730  [57664/60000]\n",
      "Average training loss: 0.1281303676532339\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.130353 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.380195 \n",
      "\n",
      "Change in average loss: 0.001029476773605431\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.088585  [   64/60000]\n",
      "loss: 0.143675  [ 6464/60000]\n",
      "loss: 0.123585  [12864/60000]\n",
      "loss: 0.140491  [19264/60000]\n",
      "loss: 0.120460  [25664/60000]\n",
      "loss: 0.180670  [32064/60000]\n",
      "loss: 0.119131  [38464/60000]\n",
      "loss: 0.149398  [44864/60000]\n",
      "loss: 0.163953  [51264/60000]\n",
      "loss: 0.165407  [57664/60000]\n",
      "Average training loss: 0.12711448252582347\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.129495 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.381400 \n",
      "\n",
      "Change in average loss: 0.0010158851274104375\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.088197  [   64/60000]\n",
      "loss: 0.142092  [ 6464/60000]\n",
      "loss: 0.121266  [12864/60000]\n",
      "loss: 0.139226  [19264/60000]\n",
      "loss: 0.119103  [25664/60000]\n",
      "loss: 0.180223  [32064/60000]\n",
      "loss: 0.117951  [38464/60000]\n",
      "loss: 0.146649  [44864/60000]\n",
      "loss: 0.162863  [51264/60000]\n",
      "loss: 0.164256  [57664/60000]\n",
      "Average training loss: 0.1261025824264359\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.128802 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.382551 \n",
      "\n",
      "Change in average loss: 0.0010119000993875638\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.088638  [   64/60000]\n",
      "loss: 0.141816  [ 6464/60000]\n",
      "loss: 0.119472  [12864/60000]\n",
      "loss: 0.139283  [19264/60000]\n",
      "loss: 0.117931  [25664/60000]\n",
      "loss: 0.180309  [32064/60000]\n",
      "loss: 0.117601  [38464/60000]\n",
      "loss: 0.144709  [44864/60000]\n",
      "loss: 0.160937  [51264/60000]\n",
      "loss: 0.162482  [57664/60000]\n",
      "Average training loss: 0.1251192710427905\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.127808 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.383664 \n",
      "\n",
      "Change in average loss: 0.0009833113836454077\n",
      "TRAINING DONE!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_fn, optimizer, train_dataloader, dev_dataloader, end_training_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cd7ef-027b-48a1-9c2d-f29fdda4898b",
   "metadata": {},
   "source": [
    "### More regularization\n",
    "\n",
    "It seems like 1e-4 weight decay didn't really impact the overfitting.\n",
    "Let's try bigger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "263620f2-dc20-44eb-81ac-cb650c906244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(layers_dims, layers_activations).to(device)\n",
    "print(model)\n",
    "\n",
    "weight_decay = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fb77b7f-907a-41a6-a14c-0eb73379b220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.363905  [   64/60000]\n",
      "loss: 1.252863  [ 6464/60000]\n",
      "loss: 0.704967  [12864/60000]\n",
      "loss: 0.865815  [19264/60000]\n",
      "loss: 0.667265  [25664/60000]\n",
      "loss: 0.645295  [32064/60000]\n",
      "loss: 0.604094  [38464/60000]\n",
      "loss: 0.635680  [44864/60000]\n",
      "loss: 0.658490  [51264/60000]\n",
      "loss: 0.569495  [57664/60000]\n",
      "Average training loss: 0.7501920419397639\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.503954 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.523041 \n",
      "\n",
      "Change in average loss: inf\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.445585  [   64/60000]\n",
      "loss: 0.541711  [ 6464/60000]\n",
      "loss: 0.336069  [12864/60000]\n",
      "loss: 0.609999  [19264/60000]\n",
      "loss: 0.483774  [25664/60000]\n",
      "loss: 0.487954  [32064/60000]\n",
      "loss: 0.431556  [38464/60000]\n",
      "loss: 0.603316  [44864/60000]\n",
      "loss: 0.605457  [51264/60000]\n",
      "loss: 0.458329  [57664/60000]\n",
      "Average training loss: 0.46449921548620726\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.430985 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.459650 \n",
      "\n",
      "Change in average loss: 0.2856928264535566\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.348644  [   64/60000]\n",
      "loss: 0.461625  [ 6464/60000]\n",
      "loss: 0.272059  [12864/60000]\n",
      "loss: 0.547126  [19264/60000]\n",
      "loss: 0.421796  [25664/60000]\n",
      "loss: 0.428465  [32064/60000]\n",
      "loss: 0.381534  [38464/60000]\n",
      "loss: 0.572945  [44864/60000]\n",
      "loss: 0.562948  [51264/60000]\n",
      "loss: 0.420336  [57664/60000]\n",
      "Average training loss: 0.4174901487857802\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.399373 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.433943 \n",
      "\n",
      "Change in average loss: 0.04700906670042704\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.303432  [   64/60000]\n",
      "loss: 0.421736  [ 6464/60000]\n",
      "loss: 0.240769  [12864/60000]\n",
      "loss: 0.504354  [19264/60000]\n",
      "loss: 0.388151  [25664/60000]\n",
      "loss: 0.400158  [32064/60000]\n",
      "loss: 0.357951  [38464/60000]\n",
      "loss: 0.544513  [44864/60000]\n",
      "loss: 0.535046  [51264/60000]\n",
      "loss: 0.407411  [57664/60000]\n",
      "Average training loss: 0.39239144947991444\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.379255 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.417944 \n",
      "\n",
      "Change in average loss: 0.02509869930586578\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.276580  [   64/60000]\n",
      "loss: 0.393509  [ 6464/60000]\n",
      "loss: 0.221426  [12864/60000]\n",
      "loss: 0.470868  [19264/60000]\n",
      "loss: 0.364567  [25664/60000]\n",
      "loss: 0.382638  [32064/60000]\n",
      "loss: 0.342370  [38464/60000]\n",
      "loss: 0.518269  [44864/60000]\n",
      "loss: 0.518132  [51264/60000]\n",
      "loss: 0.398505  [57664/60000]\n",
      "Average training loss: 0.37516735834099335\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.364417 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.406386 \n",
      "\n",
      "Change in average loss: 0.017224091138921094\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.258962  [   64/60000]\n",
      "loss: 0.373105  [ 6464/60000]\n",
      "loss: 0.209091  [12864/60000]\n",
      "loss: 0.442207  [19264/60000]\n",
      "loss: 0.345582  [25664/60000]\n",
      "loss: 0.368249  [32064/60000]\n",
      "loss: 0.333621  [38464/60000]\n",
      "loss: 0.493399  [44864/60000]\n",
      "loss: 0.503506  [51264/60000]\n",
      "loss: 0.392334  [57664/60000]\n",
      "Average training loss: 0.36202775151617744\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.352649 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.397965 \n",
      "\n",
      "Change in average loss: 0.013139606824815908\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.246752  [   64/60000]\n",
      "loss: 0.356990  [ 6464/60000]\n",
      "loss: 0.202031  [12864/60000]\n",
      "loss: 0.416971  [19264/60000]\n",
      "loss: 0.331075  [25664/60000]\n",
      "loss: 0.357333  [32064/60000]\n",
      "loss: 0.326459  [38464/60000]\n",
      "loss: 0.474466  [44864/60000]\n",
      "loss: 0.493272  [51264/60000]\n",
      "loss: 0.390952  [57664/60000]\n",
      "Average training loss: 0.35135714925015404\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.342890 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.391209 \n",
      "\n",
      "Change in average loss: 0.010670602266023399\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.235494  [   64/60000]\n",
      "loss: 0.343868  [ 6464/60000]\n",
      "loss: 0.196291  [12864/60000]\n",
      "loss: 0.400280  [19264/60000]\n",
      "loss: 0.318168  [25664/60000]\n",
      "loss: 0.349909  [32064/60000]\n",
      "loss: 0.318923  [38464/60000]\n",
      "loss: 0.459620  [44864/60000]\n",
      "loss: 0.483594  [51264/60000]\n",
      "loss: 0.388718  [57664/60000]\n",
      "Average training loss: 0.3423819216345546\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.334508 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.385710 \n",
      "\n",
      "Change in average loss: 0.008975227615599435\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.228639  [   64/60000]\n",
      "loss: 0.334135  [ 6464/60000]\n",
      "loss: 0.193741  [12864/60000]\n",
      "loss: 0.384059  [19264/60000]\n",
      "loss: 0.307972  [25664/60000]\n",
      "loss: 0.340673  [32064/60000]\n",
      "loss: 0.314655  [38464/60000]\n",
      "loss: 0.445978  [44864/60000]\n",
      "loss: 0.475128  [51264/60000]\n",
      "loss: 0.386023  [57664/60000]\n",
      "Average training loss: 0.3345726958708341\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.327140 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.381075 \n",
      "\n",
      "Change in average loss: 0.0078092257637205065\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.222829  [   64/60000]\n",
      "loss: 0.326557  [ 6464/60000]\n",
      "loss: 0.191890  [12864/60000]\n",
      "loss: 0.371012  [19264/60000]\n",
      "loss: 0.299763  [25664/60000]\n",
      "loss: 0.334509  [32064/60000]\n",
      "loss: 0.309535  [38464/60000]\n",
      "loss: 0.436096  [44864/60000]\n",
      "loss: 0.467017  [51264/60000]\n",
      "loss: 0.386515  [57664/60000]\n",
      "Average training loss: 0.3276703099467988\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.320584 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.377086 \n",
      "\n",
      "Change in average loss: 0.006902385924035304\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.217387  [   64/60000]\n",
      "loss: 0.320095  [ 6464/60000]\n",
      "loss: 0.190364  [12864/60000]\n",
      "loss: 0.359329  [19264/60000]\n",
      "loss: 0.291844  [25664/60000]\n",
      "loss: 0.327594  [32064/60000]\n",
      "loss: 0.304958  [38464/60000]\n",
      "loss: 0.425164  [44864/60000]\n",
      "loss: 0.459486  [51264/60000]\n",
      "loss: 0.387736  [57664/60000]\n",
      "Average training loss: 0.3214717342544085\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.314678 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.373643 \n",
      "\n",
      "Change in average loss: 0.006198575692390296\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.212552  [   64/60000]\n",
      "loss: 0.313149  [ 6464/60000]\n",
      "loss: 0.188616  [12864/60000]\n",
      "loss: 0.348814  [19264/60000]\n",
      "loss: 0.285072  [25664/60000]\n",
      "loss: 0.320491  [32064/60000]\n",
      "loss: 0.301030  [38464/60000]\n",
      "loss: 0.414121  [44864/60000]\n",
      "loss: 0.451109  [51264/60000]\n",
      "loss: 0.387404  [57664/60000]\n",
      "Average training loss: 0.31587259406283463\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.309313 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.370496 \n",
      "\n",
      "Change in average loss: 0.00559914019157387\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.209114  [   64/60000]\n",
      "loss: 0.307819  [ 6464/60000]\n",
      "loss: 0.187632  [12864/60000]\n",
      "loss: 0.340394  [19264/60000]\n",
      "loss: 0.279988  [25664/60000]\n",
      "loss: 0.315474  [32064/60000]\n",
      "loss: 0.296821  [38464/60000]\n",
      "loss: 0.405897  [44864/60000]\n",
      "loss: 0.444047  [51264/60000]\n",
      "loss: 0.388780  [57664/60000]\n",
      "Average training loss: 0.31071433949190924\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.304331 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.367571 \n",
      "\n",
      "Change in average loss: 0.005158254570925391\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.205186  [   64/60000]\n",
      "loss: 0.302019  [ 6464/60000]\n",
      "loss: 0.186774  [12864/60000]\n",
      "loss: 0.331751  [19264/60000]\n",
      "loss: 0.275532  [25664/60000]\n",
      "loss: 0.310544  [32064/60000]\n",
      "loss: 0.292801  [38464/60000]\n",
      "loss: 0.396022  [44864/60000]\n",
      "loss: 0.436466  [51264/60000]\n",
      "loss: 0.388259  [57664/60000]\n",
      "Average training loss: 0.3058731360539699\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.299568 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.364716 \n",
      "\n",
      "Change in average loss: 0.004841203437939368\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.202570  [   64/60000]\n",
      "loss: 0.297077  [ 6464/60000]\n",
      "loss: 0.184944  [12864/60000]\n",
      "loss: 0.325317  [19264/60000]\n",
      "loss: 0.272701  [25664/60000]\n",
      "loss: 0.306773  [32064/60000]\n",
      "loss: 0.289619  [38464/60000]\n",
      "loss: 0.387119  [44864/60000]\n",
      "loss: 0.429735  [51264/60000]\n",
      "loss: 0.388703  [57664/60000]\n",
      "Average training loss: 0.30130574276357064\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.295126 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.362122 \n",
      "\n",
      "Change in average loss: 0.004567393290399235\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.200274  [   64/60000]\n",
      "loss: 0.292540  [ 6464/60000]\n",
      "loss: 0.183351  [12864/60000]\n",
      "loss: 0.318015  [19264/60000]\n",
      "loss: 0.270397  [25664/60000]\n",
      "loss: 0.302019  [32064/60000]\n",
      "loss: 0.286673  [38464/60000]\n",
      "loss: 0.379697  [44864/60000]\n",
      "loss: 0.422654  [51264/60000]\n",
      "loss: 0.386851  [57664/60000]\n",
      "Average training loss: 0.29699475338845366\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.290924 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.359754 \n",
      "\n",
      "Change in average loss: 0.00431098937511698\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.198029  [   64/60000]\n",
      "loss: 0.287599  [ 6464/60000]\n",
      "loss: 0.183748  [12864/60000]\n",
      "loss: 0.311581  [19264/60000]\n",
      "loss: 0.268076  [25664/60000]\n",
      "loss: 0.298101  [32064/60000]\n",
      "loss: 0.283690  [38464/60000]\n",
      "loss: 0.371230  [44864/60000]\n",
      "loss: 0.415412  [51264/60000]\n",
      "loss: 0.387179  [57664/60000]\n",
      "Average training loss: 0.29292107991445293\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.286914 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.357516 \n",
      "\n",
      "Change in average loss: 0.0040736734740007274\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.195619  [   64/60000]\n",
      "loss: 0.283560  [ 6464/60000]\n",
      "loss: 0.183409  [12864/60000]\n",
      "loss: 0.304447  [19264/60000]\n",
      "loss: 0.266610  [25664/60000]\n",
      "loss: 0.293798  [32064/60000]\n",
      "loss: 0.281090  [38464/60000]\n",
      "loss: 0.363066  [44864/60000]\n",
      "loss: 0.407409  [51264/60000]\n",
      "loss: 0.385815  [57664/60000]\n",
      "Average training loss: 0.28897439300823313\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.283135 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.355358 \n",
      "\n",
      "Change in average loss: 0.0039466869062197985\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.193209  [   64/60000]\n",
      "loss: 0.279692  [ 6464/60000]\n",
      "loss: 0.183575  [12864/60000]\n",
      "loss: 0.298298  [19264/60000]\n",
      "loss: 0.264678  [25664/60000]\n",
      "loss: 0.290463  [32064/60000]\n",
      "loss: 0.277438  [38464/60000]\n",
      "loss: 0.354800  [44864/60000]\n",
      "loss: 0.400514  [51264/60000]\n",
      "loss: 0.383288  [57664/60000]\n",
      "Average training loss: 0.28519315179635973\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.279496 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.353503 \n",
      "\n",
      "Change in average loss: 0.0037812412118733985\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.191208  [   64/60000]\n",
      "loss: 0.275262  [ 6464/60000]\n",
      "loss: 0.182467  [12864/60000]\n",
      "loss: 0.292698  [19264/60000]\n",
      "loss: 0.262199  [25664/60000]\n",
      "loss: 0.287534  [32064/60000]\n",
      "loss: 0.274737  [38464/60000]\n",
      "loss: 0.348087  [44864/60000]\n",
      "loss: 0.393742  [51264/60000]\n",
      "loss: 0.381779  [57664/60000]\n",
      "Average training loss: 0.2815209382505559\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.275903 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.351633 \n",
      "\n",
      "Change in average loss: 0.0036722135458038263\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.188719  [   64/60000]\n",
      "loss: 0.273128  [ 6464/60000]\n",
      "loss: 0.181466  [12864/60000]\n",
      "loss: 0.288128  [19264/60000]\n",
      "loss: 0.259757  [25664/60000]\n",
      "loss: 0.283195  [32064/60000]\n",
      "loss: 0.273269  [38464/60000]\n",
      "loss: 0.340083  [44864/60000]\n",
      "loss: 0.387656  [51264/60000]\n",
      "loss: 0.380177  [57664/60000]\n",
      "Average training loss: 0.2780540482814251\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.272515 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.350016 \n",
      "\n",
      "Change in average loss: 0.003466889969130804\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.186120  [   64/60000]\n",
      "loss: 0.269394  [ 6464/60000]\n",
      "loss: 0.180165  [12864/60000]\n",
      "loss: 0.283672  [19264/60000]\n",
      "loss: 0.256865  [25664/60000]\n",
      "loss: 0.279636  [32064/60000]\n",
      "loss: 0.271327  [38464/60000]\n",
      "loss: 0.334666  [44864/60000]\n",
      "loss: 0.381900  [51264/60000]\n",
      "loss: 0.378362  [57664/60000]\n",
      "Average training loss: 0.2747035828560019\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.269270 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.348429 \n",
      "\n",
      "Change in average loss: 0.0033504654254232213\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.184080  [   64/60000]\n",
      "loss: 0.265461  [ 6464/60000]\n",
      "loss: 0.178926  [12864/60000]\n",
      "loss: 0.279811  [19264/60000]\n",
      "loss: 0.254695  [25664/60000]\n",
      "loss: 0.275008  [32064/60000]\n",
      "loss: 0.268503  [38464/60000]\n",
      "loss: 0.329729  [44864/60000]\n",
      "loss: 0.375253  [51264/60000]\n",
      "loss: 0.375666  [57664/60000]\n",
      "Average training loss: 0.2714789063532724\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.266092 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.346952 \n",
      "\n",
      "Change in average loss: 0.0032246765027295066\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.182015  [   64/60000]\n",
      "loss: 0.263422  [ 6464/60000]\n",
      "loss: 0.178511  [12864/60000]\n",
      "loss: 0.274789  [19264/60000]\n",
      "loss: 0.252247  [25664/60000]\n",
      "loss: 0.272602  [32064/60000]\n",
      "loss: 0.267000  [38464/60000]\n",
      "loss: 0.327212  [44864/60000]\n",
      "loss: 0.370177  [51264/60000]\n",
      "loss: 0.374327  [57664/60000]\n",
      "Average training loss: 0.26834700090576336\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.263107 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.345551 \n",
      "\n",
      "Change in average loss: 0.0031319054475090202\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.180195  [   64/60000]\n",
      "loss: 0.261505  [ 6464/60000]\n",
      "loss: 0.177748  [12864/60000]\n",
      "loss: 0.270162  [19264/60000]\n",
      "loss: 0.250296  [25664/60000]\n",
      "loss: 0.267955  [32064/60000]\n",
      "loss: 0.264747  [38464/60000]\n",
      "loss: 0.323399  [44864/60000]\n",
      "loss: 0.364975  [51264/60000]\n",
      "loss: 0.371310  [57664/60000]\n",
      "Average training loss: 0.2653375074886945\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.260145 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.344127 \n",
      "\n",
      "Change in average loss: 0.0030094934170688536\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.178199  [   64/60000]\n",
      "loss: 0.259571  [ 6464/60000]\n",
      "loss: 0.178066  [12864/60000]\n",
      "loss: 0.266495  [19264/60000]\n",
      "loss: 0.248666  [25664/60000]\n",
      "loss: 0.265140  [32064/60000]\n",
      "loss: 0.263551  [38464/60000]\n",
      "loss: 0.320678  [44864/60000]\n",
      "loss: 0.360138  [51264/60000]\n",
      "loss: 0.369746  [57664/60000]\n",
      "Average training loss: 0.26241088276510555\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.257275 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.342933 \n",
      "\n",
      "Change in average loss: 0.0029266247235889553\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.175678  [   64/60000]\n",
      "loss: 0.257406  [ 6464/60000]\n",
      "loss: 0.176881  [12864/60000]\n",
      "loss: 0.262952  [19264/60000]\n",
      "loss: 0.246986  [25664/60000]\n",
      "loss: 0.262440  [32064/60000]\n",
      "loss: 0.262284  [38464/60000]\n",
      "loss: 0.316474  [44864/60000]\n",
      "loss: 0.356734  [51264/60000]\n",
      "loss: 0.366486  [57664/60000]\n",
      "Average training loss: 0.259564375373791\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.254453 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.341727 \n",
      "\n",
      "Change in average loss: 0.0028465073913145345\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.174522  [   64/60000]\n",
      "loss: 0.254303  [ 6464/60000]\n",
      "loss: 0.176368  [12864/60000]\n",
      "loss: 0.259163  [19264/60000]\n",
      "loss: 0.245850  [25664/60000]\n",
      "loss: 0.259703  [32064/60000]\n",
      "loss: 0.260956  [38464/60000]\n",
      "loss: 0.312859  [44864/60000]\n",
      "loss: 0.352265  [51264/60000]\n",
      "loss: 0.364601  [57664/60000]\n",
      "Average training loss: 0.25680859258243527\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.251765 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.340403 \n",
      "\n",
      "Change in average loss: 0.002755782791355743\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.172930  [   64/60000]\n",
      "loss: 0.251494  [ 6464/60000]\n",
      "loss: 0.175711  [12864/60000]\n",
      "loss: 0.256396  [19264/60000]\n",
      "loss: 0.244464  [25664/60000]\n",
      "loss: 0.257421  [32064/60000]\n",
      "loss: 0.260463  [38464/60000]\n",
      "loss: 0.308863  [44864/60000]\n",
      "loss: 0.346996  [51264/60000]\n",
      "loss: 0.359321  [57664/60000]\n",
      "Average training loss: 0.25410050009168794\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.249112 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.339042 \n",
      "\n",
      "Change in average loss: 0.002708092490747327\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.172773  [   64/60000]\n",
      "loss: 0.249484  [ 6464/60000]\n",
      "loss: 0.175611  [12864/60000]\n",
      "loss: 0.254466  [19264/60000]\n",
      "loss: 0.243606  [25664/60000]\n",
      "loss: 0.255396  [32064/60000]\n",
      "loss: 0.258148  [38464/60000]\n",
      "loss: 0.305329  [44864/60000]\n",
      "loss: 0.343613  [51264/60000]\n",
      "loss: 0.355732  [57664/60000]\n",
      "Average training loss: 0.25150103396825446\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.246506 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.337768 \n",
      "\n",
      "Change in average loss: 0.0025994661234334804\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.170273  [   64/60000]\n",
      "loss: 0.246958  [ 6464/60000]\n",
      "loss: 0.174534  [12864/60000]\n",
      "loss: 0.252143  [19264/60000]\n",
      "loss: 0.243371  [25664/60000]\n",
      "loss: 0.253408  [32064/60000]\n",
      "loss: 0.255650  [38464/60000]\n",
      "loss: 0.303557  [44864/60000]\n",
      "loss: 0.339941  [51264/60000]\n",
      "loss: 0.353806  [57664/60000]\n",
      "Average training loss: 0.24895408831393795\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.243992 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.336565 \n",
      "\n",
      "Change in average loss: 0.0025469456543165125\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.168423  [   64/60000]\n",
      "loss: 0.245858  [ 6464/60000]\n",
      "loss: 0.173352  [12864/60000]\n",
      "loss: 0.250449  [19264/60000]\n",
      "loss: 0.242423  [25664/60000]\n",
      "loss: 0.252143  [32064/60000]\n",
      "loss: 0.254371  [38464/60000]\n",
      "loss: 0.299840  [44864/60000]\n",
      "loss: 0.336638  [51264/60000]\n",
      "loss: 0.348899  [57664/60000]\n",
      "Average training loss: 0.24645939985635693\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.241611 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.335577 \n",
      "\n",
      "Change in average loss: 0.0024946884575810235\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.167089  [   64/60000]\n",
      "loss: 0.244082  [ 6464/60000]\n",
      "loss: 0.172884  [12864/60000]\n",
      "loss: 0.249612  [19264/60000]\n",
      "loss: 0.241198  [25664/60000]\n",
      "loss: 0.248961  [32064/60000]\n",
      "loss: 0.251948  [38464/60000]\n",
      "loss: 0.298099  [44864/60000]\n",
      "loss: 0.332811  [51264/60000]\n",
      "loss: 0.343540  [57664/60000]\n",
      "Average training loss: 0.24404448689396446\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.239184 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.334573 \n",
      "\n",
      "Change in average loss: 0.0024149129623924626\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.167103  [   64/60000]\n",
      "loss: 0.242448  [ 6464/60000]\n",
      "loss: 0.172433  [12864/60000]\n",
      "loss: 0.249170  [19264/60000]\n",
      "loss: 0.240176  [25664/60000]\n",
      "loss: 0.246206  [32064/60000]\n",
      "loss: 0.249532  [38464/60000]\n",
      "loss: 0.295308  [44864/60000]\n",
      "loss: 0.329574  [51264/60000]\n",
      "loss: 0.337495  [57664/60000]\n",
      "Average training loss: 0.24166931373986608\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.236850 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.333576 \n",
      "\n",
      "Change in average loss: 0.002375173154098381\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.165743  [   64/60000]\n",
      "loss: 0.240000  [ 6464/60000]\n",
      "loss: 0.172405  [12864/60000]\n",
      "loss: 0.247169  [19264/60000]\n",
      "loss: 0.238869  [25664/60000]\n",
      "loss: 0.244649  [32064/60000]\n",
      "loss: 0.247028  [38464/60000]\n",
      "loss: 0.292588  [44864/60000]\n",
      "loss: 0.326643  [51264/60000]\n",
      "loss: 0.333703  [57664/60000]\n",
      "Average training loss: 0.2393564093929491\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.234548 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.332960 \n",
      "\n",
      "Change in average loss: 0.002312904346916972\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.164148  [   64/60000]\n",
      "loss: 0.237508  [ 6464/60000]\n",
      "loss: 0.171562  [12864/60000]\n",
      "loss: 0.248074  [19264/60000]\n",
      "loss: 0.237808  [25664/60000]\n",
      "loss: 0.243646  [32064/60000]\n",
      "loss: 0.246539  [38464/60000]\n",
      "loss: 0.289098  [44864/60000]\n",
      "loss: 0.323317  [51264/60000]\n",
      "loss: 0.329293  [57664/60000]\n",
      "Average training loss: 0.23708580215094185\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.232308 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.331991 \n",
      "\n",
      "Change in average loss: 0.0022706072420072554\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.161790  [   64/60000]\n",
      "loss: 0.236520  [ 6464/60000]\n",
      "loss: 0.170460  [12864/60000]\n",
      "loss: 0.246397  [19264/60000]\n",
      "loss: 0.237234  [25664/60000]\n",
      "loss: 0.242318  [32064/60000]\n",
      "loss: 0.244826  [38464/60000]\n",
      "loss: 0.285002  [44864/60000]\n",
      "loss: 0.321054  [51264/60000]\n",
      "loss: 0.327149  [57664/60000]\n",
      "Average training loss: 0.23493087510151395\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.230289 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.331429 \n",
      "\n",
      "Change in average loss: 0.002154927049427907\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.160605  [   64/60000]\n",
      "loss: 0.235123  [ 6464/60000]\n",
      "loss: 0.168940  [12864/60000]\n",
      "loss: 0.246042  [19264/60000]\n",
      "loss: 0.235603  [25664/60000]\n",
      "loss: 0.240587  [32064/60000]\n",
      "loss: 0.243155  [38464/60000]\n",
      "loss: 0.282857  [44864/60000]\n",
      "loss: 0.316903  [51264/60000]\n",
      "loss: 0.321415  [57664/60000]\n",
      "Average training loss: 0.23280894309123443\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.228266 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.330934 \n",
      "\n",
      "Change in average loss: 0.0021219320102795203\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.158588  [   64/60000]\n",
      "loss: 0.233665  [ 6464/60000]\n",
      "loss: 0.169008  [12864/60000]\n",
      "loss: 0.245266  [19264/60000]\n",
      "loss: 0.234460  [25664/60000]\n",
      "loss: 0.238201  [32064/60000]\n",
      "loss: 0.241623  [38464/60000]\n",
      "loss: 0.280340  [44864/60000]\n",
      "loss: 0.313171  [51264/60000]\n",
      "loss: 0.317999  [57664/60000]\n",
      "Average training loss: 0.2307396892354941\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.226266 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.330209 \n",
      "\n",
      "Change in average loss: 0.002069253855740333\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.157897  [   64/60000]\n",
      "loss: 0.232447  [ 6464/60000]\n",
      "loss: 0.167828  [12864/60000]\n",
      "loss: 0.244360  [19264/60000]\n",
      "loss: 0.233650  [25664/60000]\n",
      "loss: 0.236894  [32064/60000]\n",
      "loss: 0.239565  [38464/60000]\n",
      "loss: 0.276705  [44864/60000]\n",
      "loss: 0.309795  [51264/60000]\n",
      "loss: 0.314822  [57664/60000]\n",
      "Average training loss: 0.22870153652579545\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.224452 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.330038 \n",
      "\n",
      "Change in average loss: 0.0020381527096986485\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.154930  [   64/60000]\n",
      "loss: 0.231104  [ 6464/60000]\n",
      "loss: 0.165919  [12864/60000]\n",
      "loss: 0.243805  [19264/60000]\n",
      "loss: 0.231789  [25664/60000]\n",
      "loss: 0.235420  [32064/60000]\n",
      "loss: 0.238387  [38464/60000]\n",
      "loss: 0.275338  [44864/60000]\n",
      "loss: 0.306629  [51264/60000]\n",
      "loss: 0.310596  [57664/60000]\n",
      "Average training loss: 0.22674408238103141\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.222546 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329509 \n",
      "\n",
      "Change in average loss: 0.0019574541447640315\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.153815  [   64/60000]\n",
      "loss: 0.229954  [ 6464/60000]\n",
      "loss: 0.164431  [12864/60000]\n",
      "loss: 0.242204  [19264/60000]\n",
      "loss: 0.230410  [25664/60000]\n",
      "loss: 0.233903  [32064/60000]\n",
      "loss: 0.236779  [38464/60000]\n",
      "loss: 0.272031  [44864/60000]\n",
      "loss: 0.303046  [51264/60000]\n",
      "loss: 0.308975  [57664/60000]\n",
      "Average training loss: 0.22480716866089592\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.220619 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.329240 \n",
      "\n",
      "Change in average loss: 0.0019369137201354913\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.152609  [   64/60000]\n",
      "loss: 0.227012  [ 6464/60000]\n",
      "loss: 0.163193  [12864/60000]\n",
      "loss: 0.241138  [19264/60000]\n",
      "loss: 0.228989  [25664/60000]\n",
      "loss: 0.232293  [32064/60000]\n",
      "loss: 0.235190  [38464/60000]\n",
      "loss: 0.270363  [44864/60000]\n",
      "loss: 0.298890  [51264/60000]\n",
      "loss: 0.306409  [57664/60000]\n",
      "Average training loss: 0.22295257077415362\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.218639 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.328791 \n",
      "\n",
      "Change in average loss: 0.001854597886742304\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.150051  [   64/60000]\n",
      "loss: 0.226205  [ 6464/60000]\n",
      "loss: 0.162852  [12864/60000]\n",
      "loss: 0.240522  [19264/60000]\n",
      "loss: 0.227887  [25664/60000]\n",
      "loss: 0.229938  [32064/60000]\n",
      "loss: 0.233613  [38464/60000]\n",
      "loss: 0.267991  [44864/60000]\n",
      "loss: 0.295654  [51264/60000]\n",
      "loss: 0.302219  [57664/60000]\n",
      "Average training loss: 0.22106894351907377\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.216985 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.328644 \n",
      "\n",
      "Change in average loss: 0.001883627255079845\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.148386  [   64/60000]\n",
      "loss: 0.223792  [ 6464/60000]\n",
      "loss: 0.161511  [12864/60000]\n",
      "loss: 0.240314  [19264/60000]\n",
      "loss: 0.225928  [25664/60000]\n",
      "loss: 0.228782  [32064/60000]\n",
      "loss: 0.231866  [38464/60000]\n",
      "loss: 0.265296  [44864/60000]\n",
      "loss: 0.292182  [51264/60000]\n",
      "loss: 0.299448  [57664/60000]\n",
      "Average training loss: 0.21931716575742022\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.215353 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.328505 \n",
      "\n",
      "Change in average loss: 0.0017517777616535557\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.147352  [   64/60000]\n",
      "loss: 0.222716  [ 6464/60000]\n",
      "loss: 0.160877  [12864/60000]\n",
      "loss: 0.239374  [19264/60000]\n",
      "loss: 0.225752  [25664/60000]\n",
      "loss: 0.226434  [32064/60000]\n",
      "loss: 0.230161  [38464/60000]\n",
      "loss: 0.263076  [44864/60000]\n",
      "loss: 0.288124  [51264/60000]\n",
      "loss: 0.296578  [57664/60000]\n",
      "Average training loss: 0.2175681356078526\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.213602 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.328403 \n",
      "\n",
      "Change in average loss: 0.0017490301495676208\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.146157  [   64/60000]\n",
      "loss: 0.221456  [ 6464/60000]\n",
      "loss: 0.159755  [12864/60000]\n",
      "loss: 0.237716  [19264/60000]\n",
      "loss: 0.225071  [25664/60000]\n",
      "loss: 0.225112  [32064/60000]\n",
      "loss: 0.230131  [38464/60000]\n",
      "loss: 0.258964  [44864/60000]\n",
      "loss: 0.285966  [51264/60000]\n",
      "loss: 0.292365  [57664/60000]\n",
      "Average training loss: 0.21586579445805124\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.212071 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328267 \n",
      "\n",
      "Change in average loss: 0.0017023411498013619\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.144463  [   64/60000]\n",
      "loss: 0.221024  [ 6464/60000]\n",
      "loss: 0.157881  [12864/60000]\n",
      "loss: 0.236974  [19264/60000]\n",
      "loss: 0.223681  [25664/60000]\n",
      "loss: 0.223792  [32064/60000]\n",
      "loss: 0.228227  [38464/60000]\n",
      "loss: 0.256253  [44864/60000]\n",
      "loss: 0.283241  [51264/60000]\n",
      "loss: 0.290935  [57664/60000]\n",
      "Average training loss: 0.21420001511825426\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.210534 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328271 \n",
      "\n",
      "Change in average loss: 0.0016657793397969745\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.143325  [   64/60000]\n",
      "loss: 0.218300  [ 6464/60000]\n",
      "loss: 0.156157  [12864/60000]\n",
      "loss: 0.236182  [19264/60000]\n",
      "loss: 0.223450  [25664/60000]\n",
      "loss: 0.222558  [32064/60000]\n",
      "loss: 0.227455  [38464/60000]\n",
      "loss: 0.253021  [44864/60000]\n",
      "loss: 0.280859  [51264/60000]\n",
      "loss: 0.287550  [57664/60000]\n",
      "Average training loss: 0.2125895490317838\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.208849 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328224 \n",
      "\n",
      "Change in average loss: 0.0016104660864704745\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.142021  [   64/60000]\n",
      "loss: 0.217215  [ 6464/60000]\n",
      "loss: 0.155217  [12864/60000]\n",
      "loss: 0.236063  [19264/60000]\n",
      "loss: 0.221915  [25664/60000]\n",
      "loss: 0.221326  [32064/60000]\n",
      "loss: 0.225901  [38464/60000]\n",
      "loss: 0.250262  [44864/60000]\n",
      "loss: 0.278416  [51264/60000]\n",
      "loss: 0.284068  [57664/60000]\n",
      "Average training loss: 0.21099297313897342\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.207241 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328077 \n",
      "\n",
      "Change in average loss: 0.0015965758928103702\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.140358  [   64/60000]\n",
      "loss: 0.215781  [ 6464/60000]\n",
      "loss: 0.154262  [12864/60000]\n",
      "loss: 0.234520  [19264/60000]\n",
      "loss: 0.220990  [25664/60000]\n",
      "loss: 0.220028  [32064/60000]\n",
      "loss: 0.223745  [38464/60000]\n",
      "loss: 0.247059  [44864/60000]\n",
      "loss: 0.275190  [51264/60000]\n",
      "loss: 0.281592  [57664/60000]\n",
      "Average training loss: 0.20941341954317175\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.205811 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.328057 \n",
      "\n",
      "Change in average loss: 0.0015795535958016693\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.139845  [   64/60000]\n",
      "loss: 0.214746  [ 6464/60000]\n",
      "loss: 0.152582  [12864/60000]\n",
      "loss: 0.234642  [19264/60000]\n",
      "loss: 0.219744  [25664/60000]\n",
      "loss: 0.218892  [32064/60000]\n",
      "loss: 0.222821  [38464/60000]\n",
      "loss: 0.245290  [44864/60000]\n",
      "loss: 0.274120  [51264/60000]\n",
      "loss: 0.279613  [57664/60000]\n",
      "Average training loss: 0.2079104188122729\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.204563 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328159 \n",
      "\n",
      "Change in average loss: 0.0015030007308988458\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.137782  [   64/60000]\n",
      "loss: 0.212438  [ 6464/60000]\n",
      "loss: 0.151285  [12864/60000]\n",
      "loss: 0.233525  [19264/60000]\n",
      "loss: 0.217781  [25664/60000]\n",
      "loss: 0.217113  [32064/60000]\n",
      "loss: 0.221900  [38464/60000]\n",
      "loss: 0.242751  [44864/60000]\n",
      "loss: 0.270227  [51264/60000]\n",
      "loss: 0.278829  [57664/60000]\n",
      "Average training loss: 0.20645915736744144\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.203036 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.327987 \n",
      "\n",
      "Change in average loss: 0.0014512614448314587\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.137055  [   64/60000]\n",
      "loss: 0.212213  [ 6464/60000]\n",
      "loss: 0.149639  [12864/60000]\n",
      "loss: 0.233127  [19264/60000]\n",
      "loss: 0.217109  [25664/60000]\n",
      "loss: 0.216958  [32064/60000]\n",
      "loss: 0.219591  [38464/60000]\n",
      "loss: 0.241046  [44864/60000]\n",
      "loss: 0.269708  [51264/60000]\n",
      "loss: 0.275470  [57664/60000]\n",
      "Average training loss: 0.20500997578633873\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.201552 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.327878 \n",
      "\n",
      "Change in average loss: 0.0014491815811027153\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.135745  [   64/60000]\n",
      "loss: 0.209669  [ 6464/60000]\n",
      "loss: 0.148751  [12864/60000]\n",
      "loss: 0.232510  [19264/60000]\n",
      "loss: 0.214491  [25664/60000]\n",
      "loss: 0.214077  [32064/60000]\n",
      "loss: 0.218700  [38464/60000]\n",
      "loss: 0.238792  [44864/60000]\n",
      "loss: 0.268368  [51264/60000]\n",
      "loss: 0.273691  [57664/60000]\n",
      "Average training loss: 0.20358182435064937\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.200332 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.327989 \n",
      "\n",
      "Change in average loss: 0.001428151435689362\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.135279  [   64/60000]\n",
      "loss: 0.207342  [ 6464/60000]\n",
      "loss: 0.147961  [12864/60000]\n",
      "loss: 0.231325  [19264/60000]\n",
      "loss: 0.213719  [25664/60000]\n",
      "loss: 0.213848  [32064/60000]\n",
      "loss: 0.218060  [38464/60000]\n",
      "loss: 0.236099  [44864/60000]\n",
      "loss: 0.266170  [51264/60000]\n",
      "loss: 0.269813  [57664/60000]\n",
      "Average training loss: 0.20217727548055558\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.198973 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.328162 \n",
      "\n",
      "Change in average loss: 0.0014045488700937803\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.133795  [   64/60000]\n",
      "loss: 0.206540  [ 6464/60000]\n",
      "loss: 0.146350  [12864/60000]\n",
      "loss: 0.229785  [19264/60000]\n",
      "loss: 0.213087  [25664/60000]\n",
      "loss: 0.213170  [32064/60000]\n",
      "loss: 0.216283  [38464/60000]\n",
      "loss: 0.235279  [44864/60000]\n",
      "loss: 0.264693  [51264/60000]\n",
      "loss: 0.267812  [57664/60000]\n",
      "Average training loss: 0.2008281980893378\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.197648 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.328195 \n",
      "\n",
      "Change in average loss: 0.0013490773912177845\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.132760  [   64/60000]\n",
      "loss: 0.205602  [ 6464/60000]\n",
      "loss: 0.145049  [12864/60000]\n",
      "loss: 0.229866  [19264/60000]\n",
      "loss: 0.211395  [25664/60000]\n",
      "loss: 0.212590  [32064/60000]\n",
      "loss: 0.215954  [38464/60000]\n",
      "loss: 0.233616  [44864/60000]\n",
      "loss: 0.262385  [51264/60000]\n",
      "loss: 0.265229  [57664/60000]\n",
      "Average training loss: 0.19946063364714955\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.196458 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328364 \n",
      "\n",
      "Change in average loss: 0.001367564442188246\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.131353  [   64/60000]\n",
      "loss: 0.203573  [ 6464/60000]\n",
      "loss: 0.143966  [12864/60000]\n",
      "loss: 0.228727  [19264/60000]\n",
      "loss: 0.210405  [25664/60000]\n",
      "loss: 0.211293  [32064/60000]\n",
      "loss: 0.213308  [38464/60000]\n",
      "loss: 0.230613  [44864/60000]\n",
      "loss: 0.261376  [51264/60000]\n",
      "loss: 0.263051  [57664/60000]\n",
      "Average training loss: 0.1981609945517105\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.195162 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328516 \n",
      "\n",
      "Change in average loss: 0.0012996390954390646\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.131035  [   64/60000]\n",
      "loss: 0.201088  [ 6464/60000]\n",
      "loss: 0.142850  [12864/60000]\n",
      "loss: 0.227603  [19264/60000]\n",
      "loss: 0.209047  [25664/60000]\n",
      "loss: 0.210288  [32064/60000]\n",
      "loss: 0.212432  [38464/60000]\n",
      "loss: 0.230011  [44864/60000]\n",
      "loss: 0.259153  [51264/60000]\n",
      "loss: 0.260868  [57664/60000]\n",
      "Average training loss: 0.1968777303391301\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.193939 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328343 \n",
      "\n",
      "Change in average loss: 0.0012832642125803762\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.130086  [   64/60000]\n",
      "loss: 0.200330  [ 6464/60000]\n",
      "loss: 0.142080  [12864/60000]\n",
      "loss: 0.227590  [19264/60000]\n",
      "loss: 0.207758  [25664/60000]\n",
      "loss: 0.210096  [32064/60000]\n",
      "loss: 0.211718  [38464/60000]\n",
      "loss: 0.228088  [44864/60000]\n",
      "loss: 0.257660  [51264/60000]\n",
      "loss: 0.259577  [57664/60000]\n",
      "Average training loss: 0.1955995234980512\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.192756 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328701 \n",
      "\n",
      "Change in average loss: 0.001278206841078905\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.128958  [   64/60000]\n",
      "loss: 0.199168  [ 6464/60000]\n",
      "loss: 0.142137  [12864/60000]\n",
      "loss: 0.226810  [19264/60000]\n",
      "loss: 0.206873  [25664/60000]\n",
      "loss: 0.208098  [32064/60000]\n",
      "loss: 0.210859  [38464/60000]\n",
      "loss: 0.224695  [44864/60000]\n",
      "loss: 0.254955  [51264/60000]\n",
      "loss: 0.258758  [57664/60000]\n",
      "Average training loss: 0.19432237158134294\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.191636 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.328788 \n",
      "\n",
      "Change in average loss: 0.001277151916708269\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.128928  [   64/60000]\n",
      "loss: 0.197554  [ 6464/60000]\n",
      "loss: 0.140844  [12864/60000]\n",
      "loss: 0.226359  [19264/60000]\n",
      "loss: 0.205148  [25664/60000]\n",
      "loss: 0.206848  [32064/60000]\n",
      "loss: 0.209648  [38464/60000]\n",
      "loss: 0.224355  [44864/60000]\n",
      "loss: 0.255453  [51264/60000]\n",
      "loss: 0.255285  [57664/60000]\n",
      "Average training loss: 0.19312835255188981\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.190552 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.329020 \n",
      "\n",
      "Change in average loss: 0.001194019029453125\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.127535  [   64/60000]\n",
      "loss: 0.197073  [ 6464/60000]\n",
      "loss: 0.140413  [12864/60000]\n",
      "loss: 0.225588  [19264/60000]\n",
      "loss: 0.204215  [25664/60000]\n",
      "loss: 0.206266  [32064/60000]\n",
      "loss: 0.208451  [38464/60000]\n",
      "loss: 0.221743  [44864/60000]\n",
      "loss: 0.253925  [51264/60000]\n",
      "loss: 0.251380  [57664/60000]\n",
      "Average training loss: 0.1919524788157518\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.189328 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329179 \n",
      "\n",
      "Change in average loss: 0.0011758737361380056\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.126806  [   64/60000]\n",
      "loss: 0.196649  [ 6464/60000]\n",
      "loss: 0.140180  [12864/60000]\n",
      "loss: 0.224943  [19264/60000]\n",
      "loss: 0.202517  [25664/60000]\n",
      "loss: 0.206106  [32064/60000]\n",
      "loss: 0.207796  [38464/60000]\n",
      "loss: 0.218484  [44864/60000]\n",
      "loss: 0.251931  [51264/60000]\n",
      "loss: 0.249548  [57664/60000]\n",
      "Average training loss: 0.19076736805154315\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.188221 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329208 \n",
      "\n",
      "Change in average loss: 0.0011851107642086578\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.126569  [   64/60000]\n",
      "loss: 0.195369  [ 6464/60000]\n",
      "loss: 0.138218  [12864/60000]\n",
      "loss: 0.224831  [19264/60000]\n",
      "loss: 0.202257  [25664/60000]\n",
      "loss: 0.203877  [32064/60000]\n",
      "loss: 0.207584  [38464/60000]\n",
      "loss: 0.215993  [44864/60000]\n",
      "loss: 0.251100  [51264/60000]\n",
      "loss: 0.247281  [57664/60000]\n",
      "Average training loss: 0.18961082240824761\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.187135 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329203 \n",
      "\n",
      "Change in average loss: 0.0011565456432955368\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.124920  [   64/60000]\n",
      "loss: 0.194508  [ 6464/60000]\n",
      "loss: 0.138982  [12864/60000]\n",
      "loss: 0.223460  [19264/60000]\n",
      "loss: 0.200105  [25664/60000]\n",
      "loss: 0.202875  [32064/60000]\n",
      "loss: 0.206943  [38464/60000]\n",
      "loss: 0.215279  [44864/60000]\n",
      "loss: 0.249398  [51264/60000]\n",
      "loss: 0.245021  [57664/60000]\n",
      "Average training loss: 0.18847925742980895\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.186112 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329243 \n",
      "\n",
      "Change in average loss: 0.001131564978438665\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.125593  [   64/60000]\n",
      "loss: 0.193761  [ 6464/60000]\n",
      "loss: 0.135982  [12864/60000]\n",
      "loss: 0.224027  [19264/60000]\n",
      "loss: 0.199826  [25664/60000]\n",
      "loss: 0.202141  [32064/60000]\n",
      "loss: 0.205129  [38464/60000]\n",
      "loss: 0.212970  [44864/60000]\n",
      "loss: 0.248723  [51264/60000]\n",
      "loss: 0.242545  [57664/60000]\n",
      "Average training loss: 0.18737313388856744\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.185205 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329730 \n",
      "\n",
      "Change in average loss: 0.0011061235412415105\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.125411  [   64/60000]\n",
      "loss: 0.192684  [ 6464/60000]\n",
      "loss: 0.136325  [12864/60000]\n",
      "loss: 0.223142  [19264/60000]\n",
      "loss: 0.198055  [25664/60000]\n",
      "loss: 0.201317  [32064/60000]\n",
      "loss: 0.206464  [38464/60000]\n",
      "loss: 0.211463  [44864/60000]\n",
      "loss: 0.246637  [51264/60000]\n",
      "loss: 0.241320  [57664/60000]\n",
      "Average training loss: 0.18633233031023666\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.183999 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329727 \n",
      "\n",
      "Change in average loss: 0.001040803578330779\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.125187  [   64/60000]\n",
      "loss: 0.191153  [ 6464/60000]\n",
      "loss: 0.136269  [12864/60000]\n",
      "loss: 0.221688  [19264/60000]\n",
      "loss: 0.197197  [25664/60000]\n",
      "loss: 0.200695  [32064/60000]\n",
      "loss: 0.204892  [38464/60000]\n",
      "loss: 0.208394  [44864/60000]\n",
      "loss: 0.246232  [51264/60000]\n",
      "loss: 0.240342  [57664/60000]\n",
      "Average training loss: 0.1851947582813341\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.183004 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329536 \n",
      "\n",
      "Change in average loss: 0.0011375720289025504\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.124025  [   64/60000]\n",
      "loss: 0.189119  [ 6464/60000]\n",
      "loss: 0.135885  [12864/60000]\n",
      "loss: 0.221131  [19264/60000]\n",
      "loss: 0.195570  [25664/60000]\n",
      "loss: 0.198885  [32064/60000]\n",
      "loss: 0.203512  [38464/60000]\n",
      "loss: 0.206058  [44864/60000]\n",
      "loss: 0.242409  [51264/60000]\n",
      "loss: 0.238467  [57664/60000]\n",
      "Average training loss: 0.18414677533783766\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.181873 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.329901 \n",
      "\n",
      "Change in average loss: 0.0010479829434964505\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.123494  [   64/60000]\n",
      "loss: 0.188641  [ 6464/60000]\n",
      "loss: 0.135206  [12864/60000]\n",
      "loss: 0.219624  [19264/60000]\n",
      "loss: 0.194466  [25664/60000]\n",
      "loss: 0.197338  [32064/60000]\n",
      "loss: 0.203103  [38464/60000]\n",
      "loss: 0.205088  [44864/60000]\n",
      "loss: 0.241779  [51264/60000]\n",
      "loss: 0.237217  [57664/60000]\n",
      "Average training loss: 0.18307775226054288\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.180937 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.330124 \n",
      "\n",
      "Change in average loss: 0.001069023077294784\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.122521  [   64/60000]\n",
      "loss: 0.187360  [ 6464/60000]\n",
      "loss: 0.133454  [12864/60000]\n",
      "loss: 0.219435  [19264/60000]\n",
      "loss: 0.193952  [25664/60000]\n",
      "loss: 0.195611  [32064/60000]\n",
      "loss: 0.202969  [38464/60000]\n",
      "loss: 0.202923  [44864/60000]\n",
      "loss: 0.241250  [51264/60000]\n",
      "loss: 0.235572  [57664/60000]\n",
      "Average training loss: 0.1820494717102188\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.180043 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.330363 \n",
      "\n",
      "Change in average loss: 0.0010282805503240844\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.122263  [   64/60000]\n",
      "loss: 0.184923  [ 6464/60000]\n",
      "loss: 0.133711  [12864/60000]\n",
      "loss: 0.218425  [19264/60000]\n",
      "loss: 0.192144  [25664/60000]\n",
      "loss: 0.195428  [32064/60000]\n",
      "loss: 0.202027  [38464/60000]\n",
      "loss: 0.201265  [44864/60000]\n",
      "loss: 0.240122  [51264/60000]\n",
      "loss: 0.234377  [57664/60000]\n",
      "Average training loss: 0.18100590497922542\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.178993 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.330295 \n",
      "\n",
      "Change in average loss: 0.0010435667309933727\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.121592  [   64/60000]\n",
      "loss: 0.184947  [ 6464/60000]\n",
      "loss: 0.133193  [12864/60000]\n",
      "loss: 0.217799  [19264/60000]\n",
      "loss: 0.191949  [25664/60000]\n",
      "loss: 0.195170  [32064/60000]\n",
      "loss: 0.201262  [38464/60000]\n",
      "loss: 0.199036  [44864/60000]\n",
      "loss: 0.239997  [51264/60000]\n",
      "loss: 0.231805  [57664/60000]\n",
      "Average training loss: 0.1800825609319182\n",
      "Training performance for current epoch\n",
      "Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.178187 \n",
      "\n",
      "Dev performance for current epoch\n",
      "Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.330928 \n",
      "\n",
      "Change in average loss: 0.0009233440473072063\n",
      "TRAINING DONE!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_fn, optimizer, train_dataloader, dev_dataloader, end_training_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e12dd3-3d8d-4141-b16b-370cb6e55192",
   "metadata": {},
   "source": [
    "### Increasing regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "862a2d70-738c-4eea-ae15-c0c2f4ca67f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(layers_dims, layers_activations).to(device)\n",
    "print(model)\n",
    "\n",
    "weight_decay = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
