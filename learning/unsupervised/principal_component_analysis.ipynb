{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb699431-d11e-4504-9d54-ada00b0ab95c",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique widely used in data analysis and machine learning. It transforms a dataset with possibly correlated features into a new set of **uncorrelated features**, called **principal components**, ordered by the amount of **variance** they capture in the data.\n",
    "\n",
    "PCA is a method for **finding the directions in which the data varies the most**, and then **re-expressing** the data along those directions. These directions are called **principal components**.\n",
    "\n",
    "- Think of your dataset as a cloud of points in a high-dimensional space.\n",
    "- PCA finds **new axes** (directions) such that:\n",
    "  - The first axis explains as much **variance** in the data as possible.\n",
    "  - The second axis is orthogonal (perpendicular) to the first and explains the next most variance, and so on.\n",
    "\n",
    "\n",
    "### **Why use PCA?**\n",
    "\n",
    "- Reduce the number of features while retaining most of the information (variance).\n",
    "- Remove noise or redundant features.\n",
    "- Visualize high-dimensional data in 2D or 3D.\n",
    "- Improve performance of machine learning models.\n",
    "\n",
    "\n",
    "Let’s say we have a dataset $ X \\in \\mathbb{R}^{n \\times d} $, where:\n",
    "- $ n $ is the number of samples,\n",
    "- $ d $ is the number of features (dimensions).\n",
    "\n",
    "Each row $ \\mathbf{x}^{(i)} \\in \\mathbb{R}^d $ is one data point.\n",
    "\n",
    "\n",
    "## 📍 Step 1: Z-score normalization\n",
    "\n",
    "**Z-score normalization**, also known as **standardization**, is a technique used to **rescale features** so that they have:\n",
    "\n",
    "- a **mean of 0**, and  \n",
    "- a **standard deviation of 1**.\n",
    "\n",
    "Given a feature $ x_i $ the **Z-score normalized** value of this feature is:\n",
    "\n",
    "$$\n",
    "z_i = \\frac{x_i - \\mu_i}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu_i $ is the **mean** of the feature $ x_i $,\n",
    "- $ \\sigma_i $ is the **standard deviation** of the feature $ x_i $,\n",
    "- $ z_i $ is the **standardized** value.\n",
    "\n",
    "Why use it for PCA?  \n",
    "- PCA looks for directions of **maximum variance**.\n",
    "- Mean shifts don't change variance — but **we want the variance to be about the origin**, so all our axes (principal components) go through the center of the data cloud.\n",
    "- It simplifies math because the mean becomes 0, which simplifies the covariance formula.\n",
    "\n",
    "\n",
    "### Step 2: **Compute the Covariance Matrix**\n",
    "\n",
    "Covariance tells us how much two variables **change together**.\n",
    "\n",
    "For centered data $ X $, we compute:\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n} X^T X\n",
    "$$\n",
    "This is a $ d \\times d $ matrix where each entry $ \\Sigma_{ij} $ measures how features $ i $ and $ j $ vary together\n",
    "\n",
    "- The covariance matrix captures the **structure** of the data.\n",
    "- If feature \\( i \\) increases when feature \\( j \\) increases, \\( \\Sigma_{ij} > 0 \\).\n",
    "- We're looking for the directions (linear combinations of features) where **data varies the most**.\n",
    "\n",
    "\n",
    "### Step 3: **Eigen Decomposition**\n",
    "\n",
    "Now we compute the **eigenvectors** and **eigenvalues** of the covariance matrix \\( \\Sigma \\):\n",
    "$$\n",
    "\\Sigma \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{v} \\in \\mathbb{R}^d $ is an **eigenvector**,\n",
    "- $ \\lambda $ is the corresponding **eigenvalue**.\n",
    "\n",
    "We get $ d $ eigenvectors and eigenvalues.\n",
    "\n",
    "- Each eigenvector gives a **direction** in the feature space.\n",
    "- The corresponding eigenvalue tells us **how much variance** the data has along that direction.\n",
    "- So we sort eigenvectors by decreasing eigenvalues → biggest variance first.\n",
    "\n",
    "\n",
    "### Step 4: **Select Top $ k $ Components**\n",
    "\n",
    "Choose the top $ k $ eigenvectors (those with the largest eigenvalues) to form a matrix:\n",
    "$$\n",
    "W_k = [\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k] \\in \\mathbb{R}^{d \\times k}\n",
    "$$\n",
    "\n",
    "This will be our new **basis** for the data.\n",
    "\n",
    "- We are **rotating** the data into a new coordinate system aligned with directions of greatest variance.\n",
    "- We also **reduce dimensionality** by ignoring the directions with little variance (often interpreted as noise).\n",
    "\n",
    "\n",
    "### Step 5: **Project the Data**\n",
    "\n",
    "We now project the centered data into the new space:\n",
    "$$\n",
    "Z = X_{\\text{centered}} W_k \\in \\mathbb{R}^{n \\times k}\n",
    "$$\n",
    "\n",
    "Each row in $ Z $ is a **low-dimensional representation** of the original data point.\n",
    "\n",
    "- We re-express each data point using the new axes that capture the most variation.\n",
    "- This gives us a compressed, denoised version of the data.\n",
    "\n",
    "\n",
    "### Geometric Interpretation\n",
    "\n",
    "Imagine a 3D point cloud where most of the spread is along a plane. PCA:\n",
    "- Finds that plane.\n",
    "- Projects the data onto that plane.\n",
    "- Discards the small component orthogonal to the plane (less important).\n",
    "\n",
    "This is why PCA can drastically **reduce dimensionality** without losing much **information**.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Implementation in Python (with `scikit-learn`)**\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load your data (example)\n",
    "df = pd.read_csv('your_data.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "\n",
    "# Step 2: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA(n_components=2)  # choose number of components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 4: Check explained variance\n",
    "print(pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "\n",
    "### 3. **Choosing the Number of Components**\n",
    "Use the **explained variance ratio** to decide how many principal components to keep:\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA().fit(X_scaled)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "         pca.explained_variance_ratio_.cumsum())\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
